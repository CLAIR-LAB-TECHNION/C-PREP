{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3d340b-8dda-47dc-a21e-ec962bd5f762",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88c1d86-3880-4510-a0eb-d78e5383cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from rmrl.nn.pretraining import RMGraphGenerator, HighestValueNet, RMGNNTrainer\n",
    "\n",
    "\n",
    "NUM_PROPS = 3\n",
    "MAX_NODES = 6\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "\n",
    "from pathlib import Path\n",
    "save_dir = Path(f'grpt_model/{NUM_PROPS}_props/')\n",
    "save_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6a6382-3d3d-42d9-a7e5-a6a901c0a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "rm_gen = RMGraphGenerator(NUM_PROPS, MAX_NODES, batch_size=BATCH_SIZE, seed=SEED)\n",
    "model = HighestValueNet(input_size=rm_gen.graph_space.nf_space.n)\n",
    "trainer = RMGNNTrainer(rm_gen, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd41598-0fcd-4f43-b49a-de69e178aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2134ad847b404de69630cba7f0915ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   100\n",
      "train loss: 0.37749698758125305\n",
      "eval loss:  0.3464449942111969\n",
      "==================================================\n",
      "\n",
      "iteration   200\n",
      "train loss: 0.19348308444023132\n",
      "eval loss:  0.1980610191822052\n",
      "==================================================\n",
      "\n",
      "iteration   300\n",
      "train loss: 0.15281707048416138\n",
      "eval loss:  0.07796155661344528\n",
      "==================================================\n",
      "\n",
      "iteration   400\n",
      "train loss: 0.12701015174388885\n",
      "eval loss:  0.029531465843319893\n",
      "==================================================\n",
      "\n",
      "iteration   500\n",
      "train loss: 0.015078277327120304\n",
      "eval loss:  0.027365194633603096\n",
      "==================================================\n",
      "\n",
      "iteration   600\n",
      "train loss: 0.02746163308620453\n",
      "eval loss:  0.03170926123857498\n",
      "==================================================\n",
      "\n",
      "iteration   700\n",
      "train loss: 0.015056964941322803\n",
      "eval loss:  0.01249528769403696\n",
      "==================================================\n",
      "\n",
      "iteration   800\n",
      "train loss: 0.011968957260251045\n",
      "eval loss:  0.0036509172059595585\n",
      "==================================================\n",
      "\n",
      "iteration   900\n",
      "train loss: 0.007988596335053444\n",
      "eval loss:  0.003121941350400448\n",
      "==================================================\n",
      "\n",
      "iteration   1000\n",
      "train loss: 0.007884564809501171\n",
      "eval loss:  0.008318617008626461\n",
      "==================================================\n",
      "\n",
      "iteration   1100\n",
      "train loss: 0.004806059878319502\n",
      "eval loss:  0.006662908475846052\n",
      "==================================================\n",
      "\n",
      "iteration   1200\n",
      "train loss: 0.004149107728153467\n",
      "eval loss:  0.0031164675019681454\n",
      "==================================================\n",
      "\n",
      "iteration   1300\n",
      "train loss: 0.003115179017186165\n",
      "eval loss:  0.0018937460845336318\n",
      "==================================================\n",
      "\n",
      "iteration   1400\n",
      "train loss: 0.003997513558715582\n",
      "eval loss:  0.001603586133569479\n",
      "==================================================\n",
      "\n",
      "iteration   1500\n",
      "train loss: 0.002272292971611023\n",
      "eval loss:  0.0016821485478430986\n",
      "==================================================\n",
      "\n",
      "iteration   1600\n",
      "train loss: 0.002156350528821349\n",
      "eval loss:  0.002125050174072385\n",
      "==================================================\n",
      "\n",
      "iteration   1700\n",
      "train loss: 0.001625001896172762\n",
      "eval loss:  0.0018988305237144232\n",
      "==================================================\n",
      "\n",
      "iteration   1800\n",
      "train loss: 0.0019074630690738559\n",
      "eval loss:  0.0006489796796813607\n",
      "==================================================\n",
      "\n",
      "iteration   1900\n",
      "train loss: 0.0017132387729361653\n",
      "eval loss:  0.0014975146623328328\n",
      "==================================================\n",
      "\n",
      "iteration   2000\n",
      "train loss: 0.0010199638782069087\n",
      "eval loss:  0.0015270119765773416\n",
      "==================================================\n",
      "\n",
      "iteration   2100\n",
      "train loss: 0.0010587230790406466\n",
      "eval loss:  0.0010268582263961434\n",
      "==================================================\n",
      "\n",
      "iteration   2200\n",
      "train loss: 0.0010460150660946965\n",
      "eval loss:  0.0011421311646699905\n",
      "==================================================\n",
      "\n",
      "iteration   2300\n",
      "train loss: 0.0011323712533339858\n",
      "eval loss:  0.0005933744832873344\n",
      "==================================================\n",
      "\n",
      "iteration   2400\n",
      "train loss: 0.0015643929364159703\n",
      "eval loss:  0.0007784658228047192\n",
      "==================================================\n",
      "\n",
      "iteration   2500\n",
      "train loss: 0.0006964080384932458\n",
      "eval loss:  0.00045834388583898544\n",
      "==================================================\n",
      "\n",
      "iteration   2600\n",
      "train loss: 0.0009293743059970438\n",
      "eval loss:  0.0009185668895952404\n",
      "==================================================\n",
      "\n",
      "iteration   2700\n",
      "train loss: 0.0008415617630816996\n",
      "eval loss:  0.0005183274624869227\n",
      "==================================================\n",
      "\n",
      "iteration   2800\n",
      "train loss: 0.0005206450587138534\n",
      "eval loss:  0.0007196902297437191\n",
      "==================================================\n",
      "\n",
      "iteration   2900\n",
      "train loss: 0.00020278044394217432\n",
      "eval loss:  0.0002888126182369888\n",
      "==================================================\n",
      "\n",
      "iteration   3000\n",
      "train loss: 0.0003589157131500542\n",
      "eval loss:  0.0006546634831465781\n",
      "==================================================\n",
      "\n",
      "iteration   3100\n",
      "train loss: 0.0002069671027129516\n",
      "eval loss:  0.00036980255390517414\n",
      "==================================================\n",
      "\n",
      "iteration   3200\n",
      "train loss: 0.0003896178677678108\n",
      "eval loss:  0.0003592025605030358\n",
      "==================================================\n",
      "\n",
      "iteration   3300\n",
      "train loss: 0.00048757393960841\n",
      "eval loss:  0.0005186446942389011\n",
      "==================================================\n",
      "\n",
      "iteration   3400\n",
      "train loss: 0.000402122619561851\n",
      "eval loss:  0.00036707386607304215\n",
      "==================================================\n",
      "\n",
      "iteration   3500\n",
      "train loss: 0.0002985545143019408\n",
      "eval loss:  0.00021088786888867617\n",
      "==================================================\n",
      "\n",
      "iteration   3600\n",
      "train loss: 0.0004747526836581528\n",
      "eval loss:  0.00032894950709305704\n",
      "==================================================\n",
      "\n",
      "iteration   3700\n",
      "train loss: 0.0003202807856723666\n",
      "eval loss:  0.0003309705061838031\n",
      "==================================================\n",
      "\n",
      "iteration   3800\n",
      "train loss: 0.00038715629489161074\n",
      "eval loss:  0.0004572332836687565\n",
      "==================================================\n",
      "\n",
      "iteration   3900\n",
      "train loss: 0.0003173649893142283\n",
      "eval loss:  0.0003351792984176427\n",
      "==================================================\n",
      "\n",
      "iteration   4000\n",
      "train loss: 0.0003237319178879261\n",
      "eval loss:  0.00030150925158523023\n",
      "==================================================\n",
      "\n",
      "iteration   4100\n",
      "train loss: 0.0003444851899985224\n",
      "eval loss:  0.00023960803810041398\n",
      "==================================================\n",
      "\n",
      "iteration   4200\n",
      "train loss: 0.0001787355577107519\n",
      "eval loss:  0.00020911080355290323\n",
      "==================================================\n",
      "\n",
      "iteration   4300\n",
      "train loss: 0.0001026105965138413\n",
      "eval loss:  0.0002909996546804905\n",
      "==================================================\n",
      "\n",
      "iteration   4400\n",
      "train loss: 0.00030831919866614044\n",
      "eval loss:  0.0001460528001189232\n",
      "==================================================\n",
      "\n",
      "iteration   4500\n",
      "train loss: 0.00025230462779290974\n",
      "eval loss:  0.00019289077317807823\n",
      "==================================================\n",
      "\n",
      "iteration   4600\n",
      "train loss: 0.00022902493947185576\n",
      "eval loss:  0.00022044139041099697\n",
      "==================================================\n",
      "\n",
      "iteration   4700\n",
      "train loss: 0.00011750099656637758\n",
      "eval loss:  0.0001342545438092202\n",
      "==================================================\n",
      "\n",
      "iteration   4800\n",
      "train loss: 0.0002220701426267624\n",
      "eval loss:  0.0001393074489897117\n",
      "==================================================\n",
      "\n",
      "iteration   4900\n",
      "train loss: 0.00015246921975631267\n",
      "eval loss:  0.0001376358704874292\n",
      "==================================================\n",
      "\n",
      "iteration   5000\n",
      "train loss: 0.00017731383559294045\n",
      "eval loss:  7.957022899063304e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5100\n",
      "train loss: 9.196360770147294e-05\n",
      "eval loss:  5.51007324247621e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5200\n",
      "train loss: 9.457807027501985e-05\n",
      "eval loss:  0.00011283062485745177\n",
      "==================================================\n",
      "\n",
      "iteration   5300\n",
      "train loss: 9.34593117563054e-05\n",
      "eval loss:  7.290940993698314e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5400\n",
      "train loss: 0.00011326548701617867\n",
      "eval loss:  6.313698395388201e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5500\n",
      "train loss: 8.026881550904363e-05\n",
      "eval loss:  9.469613723922521e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5600\n",
      "train loss: 9.16535354917869e-05\n",
      "eval loss:  0.0001135827333200723\n",
      "==================================================\n",
      "\n",
      "iteration   5700\n",
      "train loss: 8.276677544927225e-05\n",
      "eval loss:  9.067830251296982e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5800\n",
      "train loss: 5.5437431001337245e-05\n",
      "eval loss:  5.089725527795963e-05\n",
      "==================================================\n",
      "\n",
      "iteration   5900\n",
      "train loss: 9.840445272857323e-05\n",
      "eval loss:  7.590365567011759e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6000\n",
      "train loss: 7.632584311068058e-05\n",
      "eval loss:  0.0001260043791262433\n",
      "==================================================\n",
      "\n",
      "iteration   6100\n",
      "train loss: 8.845757110975683e-05\n",
      "eval loss:  7.302539597731084e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6200\n",
      "train loss: 8.698459714651108e-05\n",
      "eval loss:  9.729393786983564e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6300\n",
      "train loss: 6.396482785930857e-05\n",
      "eval loss:  6.134912837296724e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6400\n",
      "train loss: 9.663146192906424e-05\n",
      "eval loss:  5.208246875554323e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6500\n",
      "train loss: 8.121549763018265e-05\n",
      "eval loss:  6.856821710243821e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6600\n",
      "train loss: 3.315386493341066e-05\n",
      "eval loss:  4.133741822442971e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6700\n",
      "train loss: 5.9934194723609835e-05\n",
      "eval loss:  3.7358411645982414e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6800\n",
      "train loss: 4.311844895710237e-05\n",
      "eval loss:  8.488023013342172e-05\n",
      "==================================================\n",
      "\n",
      "iteration   6900\n",
      "train loss: 3.2097574148792773e-05\n",
      "eval loss:  2.893599958042614e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7000\n",
      "train loss: 3.74992705474142e-05\n",
      "eval loss:  3.791139170061797e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7100\n",
      "train loss: 3.896818816429004e-05\n",
      "eval loss:  4.3633892346406356e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7200\n",
      "train loss: 3.897757414961234e-05\n",
      "eval loss:  2.5596937121008523e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7300\n",
      "train loss: 4.23233796027489e-05\n",
      "eval loss:  5.2465602493612096e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7400\n",
      "train loss: 2.86434715235373e-05\n",
      "eval loss:  4.893150617135689e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7500\n",
      "train loss: 2.5961448045563884e-05\n",
      "eval loss:  2.604580731713213e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7600\n",
      "train loss: 2.3938488084240817e-05\n",
      "eval loss:  4.7823872591834515e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7700\n",
      "train loss: 3.548465247149579e-05\n",
      "eval loss:  2.9413657102850266e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7800\n",
      "train loss: 5.2005823818035424e-05\n",
      "eval loss:  3.0645889637526125e-05\n",
      "==================================================\n",
      "\n",
      "iteration   7900\n",
      "train loss: 2.6159999833907932e-05\n",
      "eval loss:  2.1105439373059198e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8000\n",
      "train loss: 1.6031730410759337e-05\n",
      "eval loss:  2.6754289137898013e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8100\n",
      "train loss: 2.8956625101272948e-05\n",
      "eval loss:  1.677689579082653e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8200\n",
      "train loss: 2.7735562980524264e-05\n",
      "eval loss:  1.802119550120551e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8300\n",
      "train loss: 2.0852005036431365e-05\n",
      "eval loss:  2.3857044652686454e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8400\n",
      "train loss: 1.911345862026792e-05\n",
      "eval loss:  2.399950062681455e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8500\n",
      "train loss: 1.6389165466534905e-05\n",
      "eval loss:  2.088979454129003e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8600\n",
      "train loss: 2.4355849745916203e-05\n",
      "eval loss:  2.3569107725052163e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8700\n",
      "train loss: 2.1803785784868523e-05\n",
      "eval loss:  1.6247773601207882e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8800\n",
      "train loss: 2.0478375517996028e-05\n",
      "eval loss:  1.2398082617437467e-05\n",
      "==================================================\n",
      "\n",
      "iteration   8900\n",
      "train loss: 2.1636416931869462e-05\n",
      "eval loss:  1.9354381947778165e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9000\n",
      "train loss: 1.4373323210747913e-05\n",
      "eval loss:  2.4861434212652966e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9100\n",
      "train loss: 2.2894528228789568e-05\n",
      "eval loss:  9.101549949264154e-06\n",
      "==================================================\n",
      "\n",
      "iteration   9200\n",
      "train loss: 1.835152943385765e-05\n",
      "eval loss:  1.4717253179696854e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9300\n",
      "train loss: 1.1715537766576745e-05\n",
      "eval loss:  1.0580773960100487e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9400\n",
      "train loss: 1.0403154192317743e-05\n",
      "eval loss:  7.696994543948676e-06\n",
      "==================================================\n",
      "\n",
      "iteration   9500\n",
      "train loss: 1.0635564649419393e-05\n",
      "eval loss:  1.1813134733529296e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9600\n",
      "train loss: 1.2831177627958823e-05\n",
      "eval loss:  8.524583790858742e-06\n",
      "==================================================\n",
      "\n",
      "iteration   9700\n",
      "train loss: 1.880212948890403e-05\n",
      "eval loss:  1.5041067854326684e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9800\n",
      "train loss: 1.2230775610078126e-05\n",
      "eval loss:  1.2759694982378278e-05\n",
      "==================================================\n",
      "\n",
      "iteration   9900\n",
      "train loss: 9.203127774526365e-06\n",
      "eval loss:  5.965849140920909e-06\n",
      "==================================================\n",
      "\n",
      "iteration   10000\n",
      "train loss: 1.2766403415298555e-05\n",
      "eval loss:  9.313014743383974e-06\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(10_000, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432c8418-0b13-4d4e-a294-dcbc63d3f152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13c3bcdc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['iteration'], history['training_loss'], label='training')\n",
    "plt.plot(history['iteration'], history['validation_loss'], label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71087567",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(trainer.model.state_dict(), save_dir / 'full')\n",
    "torch.save(trainer.model.gnn.state_dict(), save_dir / 'gnn')\n",
    "torch.save(trainer.model.rnn.state_dict(), save_dir / 'rnn')\n",
    "with open(save_dir / 'history.json', 'w') as f:\n",
    "    json.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c0fc3-34ff-4ac5-831e-ab100fd35e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
