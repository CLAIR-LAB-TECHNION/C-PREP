{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35b66da-eb3c-448b-8457-d63b64d633e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9200224d-b33b-46fd-a587-93ae1df8ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from rmrl.experiments.configurations import *\n",
    "from rmrl.experiments.with_transfer import WithTransferExperiment\n",
    "from rmrl.experiments.runner import ExperimentsRunner\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73c1259-a154-4032-b9c7-aa770442928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_exp(seed=SEED, mods=None):\n",
    "    return WithTransferExperiment(\n",
    "        cfg=ExperimentConfiguration(\n",
    "            env=SupportedEnvironments.SMALL,\n",
    "            cspace=ContextSpaces.FIXED_ENTITIES,\n",
    "            alg=Algos.PPO,\n",
    "            mods=mods or [],\n",
    "            rm_kwargs={\n",
    "                'grid_resolution': (2, 2)\n",
    "            },\n",
    "            model_kwargs=dict(\n",
    "                gnn_hidden_dims=[32, 32],\n",
    "            ),\n",
    "            alg_kwargs={\n",
    "                # 'learning_starts': 0,\n",
    "                # 'exploration_fraction': 0.5,\n",
    "                'n_steps': 1024\n",
    "            },\n",
    "            seed=seed\n",
    "        ),\n",
    "        total_timesteps=1e5,\n",
    "        dump_dir='single_experiment_dumps',\n",
    "        verbose=1,\n",
    "        log_interval=1,\n",
    "        eval_freq=500,\n",
    "        min_evals=10,\n",
    "        n_eval_episodes=10,\n",
    "    )\n",
    "\n",
    "no_geco_exp = get_simple_exp(mods=[Mods.AS, Mods.RS])\n",
    "with_geco_exp = get_simple_exp(mods=[Mods.AS, Mods.RS, Mods.GECO])\n",
    "geco_upt_exp = get_simple_exp(mods=[Mods.AS, Mods.RS, Mods.GECOUPT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc89849a-f54e-431b-a46e-23c95935bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pairs = ExperimentsRunner.load_or_sample_contexts(exp=no_geco_exp,\n",
    "                                                          num_samples=3,\n",
    "                                                          sample_seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d403af-aaed-4e43-9a15-8cce14b8b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "for seed in [SEED * i for i in range(1, 4)]:\n",
    "    rs_only = get_simple_exp(seed, mods=[Mods.RS])\n",
    "    rm_as = get_simple_exp(seed, mods=[Mods.AS, Mods.RS])\n",
    "    rm_as_geco = get_simple_exp(seed, mods=[Mods.AS, Mods.RS, Mods.GECO])\n",
    "    rm_as_gecoupt = get_simple_exp(seed, mods=[Mods.AS, Mods.RS, Mods.GECOUPT])\n",
    "    experiments.extend([rs_only, rm_as, rm_as_geco, rm_as_gecoupt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7c6837-b0fb-46ec-b2f6-fda55aad14fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12707e9d90ed4cc7a1047aff8a3301b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4e711c920f4bd9ba1f8b499bfafa70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -9.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 211      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016901547 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.561      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0207      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.0766      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -2.52    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007578823 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0315     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00761    |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 5.58     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010267925 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0143      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 8.58     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.50 +/- 61.50\n",
      "Episode length: 13.50 +/- 11.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.5        |\n",
      "|    mean_reward          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009592578 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.00241     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0758      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | 24.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | -1.59    |\n",
      "|    ep_true_rew_mean | 17       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.7         |\n",
      "|    mean_reward          | 36.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106364135 |\n",
      "|    clip_fraction        | 0.115        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.0548       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.157        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.018       |\n",
      "|    value_loss           | 0.261        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 32.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.90 +/- 56.24\n",
      "Episode length: 9.10 +/- 10.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.1         |\n",
      "|    mean_reward          | 60.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014355194 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.15    |\n",
      "|    ep_true_rew_mean | 43.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013111228 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 56.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017002415 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0386      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | 59.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.97     |\n",
      "|    ep_rew_mean      | -0.64    |\n",
      "|    ep_true_rew_mean | 75       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.9        |\n",
      "|    mean_reward          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01925154 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0436     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.00 +/- 48.51\n",
      "Episode length: 8.00 +/- 8.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 72       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.17     |\n",
      "|    ep_rew_mean      | -0.469   |\n",
      "|    ep_true_rew_mean | 85.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016141746 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0013      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.90 +/- 0.83\n",
      "Episode length: 3.10 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.29     |\n",
      "|    ep_rew_mean      | -0.386   |\n",
      "|    ep_true_rew_mean | 91.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01541737 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.39       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 0.08       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.63     |\n",
      "|    ep_rew_mean      | -0.25    |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024001025 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.758      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.0346      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.64     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029445471 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0742     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18     |\n",
      "|    ep_rew_mean      | -0.123   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=97.00 +/- 0.77\n",
      "Episode length: 3.00 +/- 0.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040322766 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.00487     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.10 +/- 1.22\n",
      "Episode length: 2.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013553147 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.56\n",
      "Episode length: 3.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87     |\n",
      "|    ep_rew_mean      | -0.104   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02251571 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.218     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    value_loss           | 0.00373    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0813  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074846037 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000531    |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0209      |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=97.90 +/- 1.04\n",
      "Episode length: 2.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.1      |\n",
      "|    mean_reward     | 97.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.67     |\n",
      "|    ep_rew_mean      | -0.0586  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002041668 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.1        |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00197    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.000746    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.46     |\n",
      "|    ep_rew_mean      | -0.0673  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.1          |\n",
      "|    mean_reward          | 96.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015933607 |\n",
      "|    clip_fraction        | 0.0242       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0976      |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0337      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00957     |\n",
      "|    value_loss           | 0.000473     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.45     |\n",
      "|    ep_rew_mean      | -0.0699  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042482736 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0981      |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0257      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00758     |\n",
      "|    value_loss           | 0.000267     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.52     |\n",
      "|    ep_rew_mean      | -0.0702  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041810153 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0491      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.000526     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.48     |\n",
      "|    ep_rew_mean      | -0.0692  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=84.70 +/- 36.58\n",
      "Episode length: 5.30 +/- 6.63\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5.3       |\n",
      "|    mean_reward          | 84.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 23000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0574432 |\n",
      "|    clip_fraction        | 0.0738    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.902     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0443   |\n",
      "|    n_updates            | 220       |\n",
      "|    policy_gradient_loss | -0.0258   |\n",
      "|    value_loss           | 0.000298  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d90dcf823ee4e58ac70d217bd81ed69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -7.51    |\n",
      "| time/               |          |\n",
      "|    fps              | 217      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=59.50 +/- 55.33\n",
      "Episode length: 10.50 +/- 9.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.5         |\n",
      "|    mean_reward          | 59.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0154441055 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.64        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00768      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    value_loss           | 0.08         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=47.70 +/- 59.37\n",
      "Episode length: 12.30 +/- 10.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.3     |\n",
      "|    mean_reward     | 47.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | -1.42    |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011259161 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.0426     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 12.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011687048 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.038       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 23.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.4       |\n",
      "|    mean_reward          | 11.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01326676 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    value_loss           | 0.222      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 38.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015810292 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0733      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=60.20 +/- 55.79\n",
      "Episode length: 9.80 +/- 10.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | -0.994   |\n",
      "|    ep_true_rew_mean | 61.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=95.70 +/- 0.64\n",
      "Episode length: 4.30 +/- 0.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02426048 |\n",
      "|    clip_fraction        | 0.299      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0408     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=59.70 +/- 55.46\n",
      "Episode length: 10.30 +/- 9.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.3     |\n",
      "|    mean_reward     | 59.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | -0.623   |\n",
      "|    ep_true_rew_mean | 81.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020554677 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00324     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=96.60 +/- 1.62\n",
      "Episode length: 3.40 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.21     |\n",
      "|    ep_rew_mean      | -0.449   |\n",
      "|    ep_true_rew_mean | 86.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.60 +/- 0.66\n",
      "Episode length: 3.40 +/- 0.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021522101 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.18     |\n",
      "|    ep_rew_mean      | -0.277   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02664128 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.853     |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0616    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0588    |\n",
      "|    value_loss           | 0.0365     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.34     |\n",
      "|    ep_rew_mean      | -0.198   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.74\n",
      "Episode length: 3.60 +/- 1.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04475213 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.622     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0637    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.0132     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.116   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.80 +/- 1.08\n",
      "Episode length: 4.20 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014329677 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.118   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014842164 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.294      |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0279     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.03     |\n",
      "|    ep_rew_mean      | -0.0796  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071365247 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.601        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0402      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0248      |\n",
      "|    value_loss           | 0.00214      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.70 +/- 1.62\n",
      "Episode length: 3.30 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0752  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002872207 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00618    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05     |\n",
      "|    ep_rew_mean      | -0.0793  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003786264 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00875    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.000574    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.00 +/- 1.00\n",
      "Episode length: 4.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0725  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008231998 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00396     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.000556    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0687  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014427253 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00796     |\n",
      "|    value_loss           | 0.000509     |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78b88710d07420eb04c707d56a17d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -9.42    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016501455 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | -0.116      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0288      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 0.0969      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -5.85    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009613648 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.0771      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -6.01    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005529697 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.623      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0785      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -7.15    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010463708 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -13.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007755637 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -1.95    |\n",
      "|    ep_true_rew_mean | -21.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006999978 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.0956      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000382   |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -16.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063263723 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.112        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 0.224        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -10.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.3         |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077671967 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.125        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.225        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -11.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008879388 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | -5.55    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008168834 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00934    |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.9     |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | -0.81    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010110348 |\n",
      "|    clip_fraction        | 0.086       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 6.51     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=36.00 +/- 61.01\n",
      "Episode length: 14.00 +/- 11.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077810492 |\n",
      "|    clip_fraction        | 0.0776       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.569       |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00661     |\n",
      "|    value_loss           | 0.243        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.25    |\n",
      "|    ep_true_rew_mean | 22.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.3         |\n",
      "|    mean_reward          | 11.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040677926 |\n",
      "|    clip_fraction        | 0.0846       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.573       |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00889     |\n",
      "|    value_loss           | 0.253        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=23.50 +/- 59.41\n",
      "Episode length: 16.50 +/- 10.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.5     |\n",
      "|    mean_reward     | 23.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17       |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 21       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007522161 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00811    |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 30.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008101384 |\n",
      "|    clip_fraction        | 0.093       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0898      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.983   |\n",
      "|    ep_true_rew_mean | 41.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006902659 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.5     |\n",
      "|    ep_rew_mean      | -0.96    |\n",
      "|    ep_true_rew_mean | 41.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=60.80 +/- 56.18\n",
      "Episode length: 9.20 +/- 10.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.2          |\n",
      "|    mean_reward          | 60.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091677215 |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.339        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0827       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00495     |\n",
      "|    value_loss           | 0.226        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=60.90 +/- 56.24\n",
      "Episode length: 9.10 +/- 10.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.1      |\n",
      "|    mean_reward     | 60.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 39.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006628576 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=47.50 +/- 59.20\n",
      "Episode length: 12.50 +/- 10.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.5     |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 39.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03249061 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.576     |\n",
      "|    explained_variance   | 0.313      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.107      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=48.10 +/- 59.70\n",
      "Episode length: 11.90 +/- 10.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.61     |\n",
      "|    ep_rew_mean      | -0.551   |\n",
      "|    ep_true_rew_mean | 80.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=84.70 +/- 36.58\n",
      "Episode length: 5.30 +/- 6.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015135262 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0771      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.37     |\n",
      "|    ep_rew_mean      | -0.371   |\n",
      "|    ep_true_rew_mean | 87.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.1       |\n",
      "|    mean_reward          | 47.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01558279 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.471     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0217     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.14     |\n",
      "|    ep_rew_mean      | -0.25    |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012574663 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0155      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.0698      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23500, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.29     |\n",
      "|    ep_rew_mean      | -0.187   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.20 +/- 0.75\n",
      "Episode length: 3.80 +/- 0.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008209933 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.0291      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.025       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=97.00 +/- 1.00\n",
      "Episode length: 3.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19     |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024457324 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0569     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.90 +/- 1.70\n",
      "Episode length: 3.10 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18     |\n",
      "|    ep_rew_mean      | -0.0946  |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16745551 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.19       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0556    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 0.00318    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78     |\n",
      "|    ep_rew_mean      | -0.271   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027926465 |\n",
      "|    clip_fraction        | 0.423       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.00737     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 0.0452      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | -0.175   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017732196 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.0849      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0656     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0626     |\n",
      "|    value_loss           | 0.0176      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.55     |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 156      |\n",
      "|    total_timesteps  | 28672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10692942 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.197      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.00895    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=95.40 +/- 1.02\n",
      "Episode length: 4.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0823  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 29       |\n",
      "|    time_elapsed     | 161      |\n",
      "|    total_timesteps  | 29696    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059771456 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0862     |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0143     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n",
      "execution time: 374.5288116931915; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9c6092b8604718bd8d627fd366cc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -0.457   |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009337883 |\n",
      "|    clip_fraction        | 0.0504      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.622      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    value_loss           | 0.0817      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -0.376   |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082434565 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | -0.0187      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0692       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00749     |\n",
      "|    value_loss           | 0.159        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 3.45     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009215037 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0369      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 14.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091177765 |\n",
      "|    clip_fraction        | 0.0787       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | -0.0306      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0924       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.236        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.51    |\n",
      "|    ep_true_rew_mean | 18.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008602338 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0377      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 26.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016374234 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=36.40 +/- 61.41\n",
      "Episode length: 13.60 +/- 11.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.09    |\n",
      "|    ep_true_rew_mean | 52.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015747286 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | -0.933   |\n",
      "|    ep_true_rew_mean | 60.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.5       |\n",
      "|    mean_reward          | 11.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01671009 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0883     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=61.00 +/- 56.31\n",
      "Episode length: 9.00 +/- 10.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 61       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | -0.658   |\n",
      "|    ep_true_rew_mean | 75.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.30 +/- 36.44\n",
      "Episode length: 5.70 +/- 6.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022703893 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00584    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=60.70 +/- 56.11\n",
      "Episode length: 9.30 +/- 10.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.7      |\n",
      "|    ep_rew_mean      | -0.499   |\n",
      "|    ep_true_rew_mean | 83.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019925242 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.18     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029089652 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0522     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 0.0569      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.43     |\n",
      "|    ep_rew_mean      | -0.217   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023118135 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 0.0204      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026911333 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0873     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.0105      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.13     |\n",
      "|    ep_rew_mean      | -0.122   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015330861 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0589     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.80 +/- 1.60\n",
      "Episode length: 3.20 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016887335 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.10 +/- 1.22\n",
      "Episode length: 2.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.52     |\n",
      "|    ep_rew_mean      | -0.0763  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.70 +/- 1.10\n",
      "Episode length: 3.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010664155 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.00156     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.48     |\n",
      "|    ep_rew_mean      | -0.0806  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004273032 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00957    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0729  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029502173 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000154     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.00064      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.47     |\n",
      "|    ep_rew_mean      | -0.0682  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.90 +/- 0.94\n",
      "Episode length: 3.10 +/- 0.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.1          |\n",
      "|    mean_reward          | 96.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013441957 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00159     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00835     |\n",
      "|    value_loss           | 0.000386     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=97.00 +/- 0.89\n",
      "Episode length: 3.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | -0.066   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=97.10 +/- 1.70\n",
      "Episode length: 2.90 +/- 1.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.9          |\n",
      "|    mean_reward          | 97.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019161412 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0924      |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0335      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.000461     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=97.30 +/- 1.10\n",
      "Episode length: 2.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | -0.0717  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005960406 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0849     |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000161    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.000736    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.52     |\n",
      "|    ep_rew_mean      | -0.0668  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030168935 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0724      |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0154      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00751     |\n",
      "|    value_loss           | 0.000212     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64     |\n",
      "|    ep_rew_mean      | -0.0731  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 96           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020539248 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0714      |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00535     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00714     |\n",
      "|    value_loss           | 0.000114     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | -0.0694  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=97.10 +/- 0.94\n",
      "Episode length: 2.90 +/- 0.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.9          |\n",
      "|    mean_reward          | 97.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019289271 |\n",
      "|    clip_fraction        | 0.00967      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0698      |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00544     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    value_loss           | 9.16e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.29     |\n",
      "|    ep_rew_mean      | -0.0615  |\n",
      "|    ep_true_rew_mean | 96.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063110754 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0032     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.000131    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10168512 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.139     |\n",
      "|    explained_variance   | 0.121      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 0.015      |\n",
      "----------------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4667f31e35b7484984c007420b90f8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -12.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 222      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011239063 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.751      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000667    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    value_loss           | 0.0829      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -5.99    |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.2       |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01040192 |\n",
      "|    clip_fraction        | 0.0362     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.58      |\n",
      "|    explained_variance   | -0.0475    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0618     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00667   |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 9.11     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010961913 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0543      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0927      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 13.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011780491 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0741      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 19.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.5         |\n",
      "|    mean_reward          | -0.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127174845 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0177      |\n",
      "|    value_loss           | 0.211        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 45.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=83.60 +/- 36.21\n",
      "Episode length: 6.40 +/- 6.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 83.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015191924 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=84.00 +/- 36.34\n",
      "Episode length: 6.00 +/- 6.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 84       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 57.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=84.00 +/- 36.35\n",
      "Episode length: 6.00 +/- 6.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016622351 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0348      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.80 +/- 48.91\n",
      "Episode length: 7.20 +/- 8.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.735   |\n",
      "|    ep_true_rew_mean | 78.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.10 +/- 48.56\n",
      "Episode length: 7.90 +/- 8.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.9         |\n",
      "|    mean_reward          | 72.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022054888 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00956     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=84.00 +/- 36.34\n",
      "Episode length: 6.00 +/- 6.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 84       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.73     |\n",
      "|    ep_rew_mean      | -0.476   |\n",
      "|    ep_true_rew_mean | 87.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033680327 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0063     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    value_loss           | 0.0978      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.15     |\n",
      "|    ep_rew_mean      | -0.351   |\n",
      "|    ep_true_rew_mean | 91.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.90 +/- 1.64\n",
      "Episode length: 3.10 +/- 1.64\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.1       |\n",
      "|    mean_reward          | 96.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 10500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0169386 |\n",
      "|    clip_fraction        | 0.294     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.94     |\n",
      "|    explained_variance   | 0.296     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.0501   |\n",
      "|    value_loss           | 0.0523    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 0.83\n",
      "Episode length: 3.90 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.62     |\n",
      "|    ep_rew_mean      | -0.219   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023231633 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0563     |\n",
      "|    value_loss           | 0.032       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031454284 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.061      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.26     |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.60 +/- 1.62\n",
      "Episode length: 3.40 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020282898 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.02     |\n",
      "|    ep_rew_mean      | -0.0869  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004726477 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.0851  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037019376 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.677        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0347      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0225      |\n",
      "|    value_loss           | 0.00135      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0476c2bf45ff4c85a4402e625afc0b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -4.64    |\n",
      "| time/               |          |\n",
      "|    fps              | 219      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021625005 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.0552      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00829    |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 4.93     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006111186 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00824    |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | -1.57    |\n",
      "|    ep_true_rew_mean | 15.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006821923 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0743      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 20.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010290222 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 26.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012384835 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 39.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.8         |\n",
      "|    mean_reward          | 48.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058575273 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0955       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00543     |\n",
      "|    value_loss           | 0.186        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.907   |\n",
      "|    ep_true_rew_mean | 46.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.1         |\n",
      "|    mean_reward          | 59.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098246215 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.381        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0966       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 0.2          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=11.40 +/- 55.60\n",
      "Episode length: 18.60 +/- 9.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.6     |\n",
      "|    mean_reward     | 11.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.939   |\n",
      "|    ep_true_rew_mean | 48.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.1       |\n",
      "|    mean_reward          | 23.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05833323 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.375     |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0995     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    value_loss           | 0.208      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=61.20 +/- 56.44\n",
      "Episode length: 8.80 +/- 10.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 61.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.53     |\n",
      "|    ep_rew_mean      | -0.476   |\n",
      "|    ep_true_rew_mean | 85.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.1        |\n",
      "|    mean_reward          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024511635 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.25     |\n",
      "|    ep_rew_mean      | -0.46    |\n",
      "|    ep_true_rew_mean | 85.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=35.80 +/- 60.81\n",
      "Episode length: 14.20 +/- 10.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.2        |\n",
      "|    mean_reward          | 35.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012201397 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.0317      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.58     |\n",
      "|    ep_rew_mean      | -0.303   |\n",
      "|    ep_true_rew_mean | 92.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030802354 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 0.0737      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.16     |\n",
      "|    ep_rew_mean      | -0.189   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090619646 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0419     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    value_loss           | 0.0335      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129874125 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.086       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0462      |\n",
      "|    value_loss           | 0.00706      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.90 +/- 1.30\n",
      "Episode length: 3.10 +/- 1.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10426848 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.16      |\n",
      "|    explained_variance   | 0.431      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0507    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.00364    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030225797 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    value_loss           | 0.0109      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.0795  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107178785 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0938     |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.70 +/- 0.64\n",
      "Episode length: 4.30 +/- 0.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0678  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015555782 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0777     |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.000958    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0687  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.50 +/- 0.81\n",
      "Episode length: 3.50 +/- 0.81\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07981286 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    value_loss           | 0.00068    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.05     |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.60 +/- 1.74\n",
      "Episode length: 3.40 +/- 1.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | 96.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20252386 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.211     |\n",
      "|    explained_variance   | 0.0353     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0708    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0609    |\n",
      "|    value_loss           | 0.0255     |\n",
      "----------------------------------------\n",
      "execution time: 325.55203580856323; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9809b9d081f04e05bffd2547de53708d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    ep_true_rew_mean | -4.82    |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013858017 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.844      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00288     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0793      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -3.64    |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006857089 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0196      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | 1.39     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012047035 |\n",
      "|    clip_fraction        | 0.0596      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.00567    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0842      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00946    |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 11.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.40 +/- 55.92\n",
      "Episode length: 9.60 +/- 10.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013977 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0248      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.269       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 22.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010936977 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0415      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 37.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014448756 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0749      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 51.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.9         |\n",
      "|    mean_reward          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151114715 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.262        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    value_loss           | 0.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=61.00 +/- 56.31\n",
      "Episode length: 9.00 +/- 10.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 61       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | -0.788   |\n",
      "|    ep_true_rew_mean | 73.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.30 +/- 48.66\n",
      "Episode length: 7.70 +/- 8.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.7        |\n",
      "|    mean_reward          | 72.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01847563 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0169     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.29     |\n",
      "|    ep_rew_mean      | -0.549   |\n",
      "|    ep_true_rew_mean | 83.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.40 +/- 55.92\n",
      "Episode length: 9.60 +/- 10.16\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.6        |\n",
      "|    mean_reward          | 60.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02032122 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0084     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0493    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.19     |\n",
      "|    ep_rew_mean      | -0.395   |\n",
      "|    ep_true_rew_mean | 91.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.10 +/- 55.73\n",
      "Episode length: 9.90 +/- 9.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.9         |\n",
      "|    mean_reward          | 60.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025255319 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.95       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 0.0817      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=72.00 +/- 48.51\n",
      "Episode length: 8.00 +/- 8.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 72       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96     |\n",
      "|    ep_rew_mean      | -0.269   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02786867 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.759     |\n",
      "|    explained_variance   | 0.358      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00118    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0522    |\n",
      "|    value_loss           | 0.0349     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=97.10 +/- 1.14\n",
      "Episode length: 2.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.46     |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02395637 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.538     |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0625    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 0.0129     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.40 +/- 1.20\n",
      "Episode length: 2.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.6        |\n",
      "|    mean_reward          | 97.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01531293 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.394     |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0637    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.00654    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.104   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028866183 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0544     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.00 +/- 0.89\n",
      "Episode length: 3.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77     |\n",
      "|    ep_rew_mean      | -0.0909  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005663221 |\n",
      "|    clip_fraction        | 0.041       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0736  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038597346 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0324      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 0.00164      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.83     |\n",
      "|    ep_rew_mean      | -0.0858  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00434996 |\n",
      "|    clip_fraction        | 0.0416     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.124     |\n",
      "|    explained_variance   | 0.666      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0665  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020535497 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.1         |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0212      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 0.000561     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=97.30 +/- 1.62\n",
      "Episode length: 2.70 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338c82f2c8c4b60a9620164716c63de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -0.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149819115 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.78        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.022        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.0844       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | -1.57    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006944784 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0179     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 6.39     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012462673 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0983      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 27.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.90 +/- 36.30\n",
      "Episode length: 22.90 +/- 6.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.9        |\n",
      "|    mean_reward          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011058739 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.0889      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.095       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    ep_true_rew_mean | 37       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.90 +/- 36.30\n",
      "Episode length: 22.90 +/- 6.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.9        |\n",
      "|    mean_reward          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011883287 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0756      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | -1.18    |\n",
      "|    ep_true_rew_mean | 44.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014947079 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0812      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 54.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.00 +/- 55.66\n",
      "Episode length: 10.00 +/- 9.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016256858 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=84.90 +/- 36.66\n",
      "Episode length: 5.10 +/- 6.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 84.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.4     |\n",
      "|    ep_rew_mean      | -0.731   |\n",
      "|    ep_true_rew_mean | 70.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.30 +/- 36.46\n",
      "Episode length: 5.70 +/- 6.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018708818 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.19     |\n",
      "|    ep_rew_mean      | -0.525   |\n",
      "|    ep_true_rew_mean | 85.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.60 +/- 36.55\n",
      "Episode length: 5.40 +/- 6.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017304186 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000191   |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.09     |\n",
      "|    ep_rew_mean      | -0.333   |\n",
      "|    ep_true_rew_mean | 90.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=84.60 +/- 36.55\n",
      "Episode length: 5.40 +/- 6.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.4        |\n",
      "|    mean_reward          | 84.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01518452 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.973     |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.0814     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.87     |\n",
      "|    ep_rew_mean      | -0.236   |\n",
      "|    ep_true_rew_mean | 93.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=72.00 +/- 48.51\n",
      "Episode length: 8.00 +/- 8.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | 72          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023843179 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.812      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.0677      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=72.50 +/- 48.77\n",
      "Episode length: 7.50 +/- 8.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.28     |\n",
      "|    ep_rew_mean      | -0.192   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 72.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020712178 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 0.0168      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=84.00 +/- 36.37\n",
      "Episode length: 6.00 +/- 6.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6        |\n",
      "|    mean_reward     | 84       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.38     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012350083 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.00765     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97     |\n",
      "|    ep_rew_mean      | -0.0942  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008457707 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.00293     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.90 +/- 1.70\n",
      "Episode length: 4.10 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 96           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057719517 |\n",
      "|    clip_fraction        | 0.0903       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.595        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0597      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0333      |\n",
      "|    value_loss           | 0.00255      |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9786d6c3774511bb4d7e35001be1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -1.96    |\n",
      "|    ep_true_rew_mean | -19.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077669853 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.705       |\n",
      "|    explained_variance   | -0.054       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0283       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000514     |\n",
      "|    value_loss           | 0.0952       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -10.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00522629 |\n",
      "|    clip_fraction        | 0.0969     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.703     |\n",
      "|    explained_variance   | 0.0562     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0942     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00121   |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -2.51    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006738211 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.146       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 1.49     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937195 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0888      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 25.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=72.30 +/- 48.66\n",
      "Episode length: 7.70 +/- 8.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.7         |\n",
      "|    mean_reward          | 72.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012270738 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.234       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=60.80 +/- 56.18\n",
      "Episode length: 9.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 60.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 29.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.50 +/- 55.99\n",
      "Episode length: 9.50 +/- 10.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.5          |\n",
      "|    mean_reward          | 60.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058426936 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.678       |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0988       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    value_loss           | 0.225        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=23.90 +/- 59.90\n",
      "Episode length: 16.10 +/- 10.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | -1.15    |\n",
      "|    ep_true_rew_mean | 31.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=72.70 +/- 48.85\n",
      "Episode length: 7.30 +/- 8.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.3          |\n",
      "|    mean_reward          | 72.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112709645 |\n",
      "|    clip_fraction        | 0.133        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.709       |\n",
      "|    explained_variance   | 0.256        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00983     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | -1.13    |\n",
      "|    ep_true_rew_mean | 34.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 18.2      |\n",
      "|    mean_reward          | 11.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0118342 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.745    |\n",
      "|    explained_variance   | 0.308     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.105     |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.00709  |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | -0.744   |\n",
      "|    ep_true_rew_mean | 61.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010746084 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.72       |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0803      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.40 +/- 59.28\n",
      "Episode length: 16.60 +/- 10.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | 23.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.916   |\n",
      "|    ep_true_rew_mean | 52       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.50 +/- 55.99\n",
      "Episode length: 9.50 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010253129 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.735      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | -0.627   |\n",
      "|    ep_true_rew_mean | 72.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.7       |\n",
      "|    mean_reward          | 48.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02171643 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0953     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.6      |\n",
      "|    ep_rew_mean      | -0.307   |\n",
      "|    ep_true_rew_mean | 90.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.5         |\n",
      "|    mean_reward          | 72.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016226852 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "execution time: 250.2879400253296; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e58fcc4e644fddbd971419e7e979ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -2.69    |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016511295 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.663      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.0826      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 1.61     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108457375 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | 0.00144      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 0.169        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 11.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011177022 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0192      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0736      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 20.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.60 +/- 61.60\n",
      "Episode length: 13.40 +/- 11.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | 36.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012454385 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.009       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.09        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.44    |\n",
      "|    ep_true_rew_mean | 23.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.7       |\n",
      "|    mean_reward          | 48.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00987608 |\n",
      "|    clip_fraction        | 0.0956     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.12       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0771     |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.9     |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    ep_true_rew_mean | 32.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011976867 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | -1.15    |\n",
      "|    ep_true_rew_mean | 48.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014243878 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -0.901   |\n",
      "|    ep_true_rew_mean | 61.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=71.90 +/- 48.48\n",
      "Episode length: 8.10 +/- 8.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.1        |\n",
      "|    mean_reward          | 71.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02145813 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.305      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0795     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=84.10 +/- 36.42\n",
      "Episode length: 5.90 +/- 6.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.36     |\n",
      "|    ep_rew_mean      | -0.49    |\n",
      "|    ep_true_rew_mean | 82.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019100301 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0353      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.76     |\n",
      "|    ep_rew_mean      | -0.4     |\n",
      "|    ep_true_rew_mean | 90.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.56\n",
      "Episode length: 3.60 +/- 1.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018618032 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0545     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.0716      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89     |\n",
      "|    ep_rew_mean      | -0.191   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 3.1      |\n",
      "|    mean_reward          | 96.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 11500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.026798 |\n",
      "|    clip_fraction        | 0.221    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.773   |\n",
      "|    explained_variance   | 0.138    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0423  |\n",
      "|    n_updates            | 110      |\n",
      "|    policy_gradient_loss | -0.0436  |\n",
      "|    value_loss           | 0.0259   |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023043588 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.0128      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77     |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.20 +/- 0.98\n",
      "Episode length: 2.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022709979 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.00489     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.12     |\n",
      "|    ep_rew_mean      | -0.111   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.90 +/- 1.64\n",
      "Episode length: 4.10 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019887906 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0536     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.00231     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.0839  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006209897 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.186      |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.00206     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.07     |\n",
      "|    ep_rew_mean      | -0.102   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029394315 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.628        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00161     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0184      |\n",
      "|    value_loss           | 0.00159      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.00 +/- 0.77\n",
      "Episode length: 4.00 +/- 0.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0863  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.34\n",
      "Episode length: 3.00 +/- 1.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3            |\n",
      "|    mean_reward          | 97           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026642163 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00166      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.013       |\n",
      "|    value_loss           | 0.00107      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0668  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.70 +/- 0.78\n",
      "Episode length: 4.30 +/- 0.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023577781 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0151      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.000604     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=97.10 +/- 1.70\n",
      "Episode length: 2.90 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1052cb94fda24578a17597476054a3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -2.67    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010243799 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.5        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.0833      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | -0.255   |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006130391 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0583     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | 3.08     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009960912 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0358      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0894      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.59    |\n",
      "|    ep_true_rew_mean | 9.49     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011809417 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0655      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 32.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.5        |\n",
      "|    mean_reward          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017808005 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 52.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=59.70 +/- 55.46\n",
      "Episode length: 10.30 +/- 9.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.3         |\n",
      "|    mean_reward          | 59.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142716225 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.227        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0734       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    value_loss           | 0.216        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.888   |\n",
      "|    ep_true_rew_mean | 61.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.9         |\n",
      "|    mean_reward          | 60.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016898893 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | -0.758   |\n",
      "|    ep_true_rew_mean | 70.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.70 +/- 36.58\n",
      "Episode length: 5.30 +/- 6.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021307405 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.87     |\n",
      "|    ep_rew_mean      | -0.558   |\n",
      "|    ep_true_rew_mean | 85.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02137819 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00399   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0522    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.04     |\n",
      "|    ep_rew_mean      | -0.263   |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258448 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.885     |\n",
      "|    explained_variance   | 0.188      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0317    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    value_loss           | 0.0404     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.19     |\n",
      "|    ep_rew_mean      | -0.191   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.4       |\n",
      "|    mean_reward          | 95.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 11500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0278587 |\n",
      "|    clip_fraction        | 0.251     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.685    |\n",
      "|    explained_variance   | 0.355     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0279   |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -0.0546   |\n",
      "|    value_loss           | 0.0214    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022050653 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.00697     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=95.90 +/- 0.70\n",
      "Episode length: 4.10 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.117   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021733804 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0605     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.30 +/- 1.19\n",
      "Episode length: 4.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.0924  |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006840457 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0106     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.00206     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.60 +/- 1.56\n",
      "Episode length: 3.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0779  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038152991 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.608        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0237      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0191      |\n",
      "|    value_loss           | 0.00157      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.9      |\n",
      "|    ep_rew_mean      | -0.082   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015169555 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0275     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.000907    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87     |\n",
      "|    ep_rew_mean      | -0.074   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048763957 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0236      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 0.000976     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77     |\n",
      "|    ep_rew_mean      | -0.0728  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033708846 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.3e-05     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00461     |\n",
      "|    value_loss           | 0.000422     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.0713  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006118443 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00106    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.000431    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.0765  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033925648 |\n",
      "|    clip_fraction        | 0.0601       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0345      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0108      |\n",
      "|    value_loss           | 0.000515     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=95.80 +/- 1.08\n",
      "Episode length: 4.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0714  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053096814 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0249      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0108      |\n",
      "|    value_loss           | 0.000342     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0685  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.80 +/- 0.60\n",
      "Episode length: 4.20 +/- 0.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012957507 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0952      |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00676     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0065      |\n",
      "|    value_loss           | 0.000223     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.30 +/- 0.78\n",
      "Episode length: 4.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0653  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=97.00 +/- 1.61\n",
      "Episode length: 3.00 +/- 1.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3            |\n",
      "|    mean_reward          | 97           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034020199 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.084       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00815     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 0.000377     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0655  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014141435 |\n",
      "|    clip_fraction        | 0.0467      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.085      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.047      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.00019     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f54d6c933de472da08f4ed6b67565bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | -0.255   |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009454932 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | -0.0328     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    value_loss           | 0.0996      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | -2.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038146346 |\n",
      "|    clip_fraction        | 0.0763       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.0641       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0882       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 0.173        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -9.34    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063549927 |\n",
      "|    clip_fraction        | 0.0995       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.6         |\n",
      "|    explained_variance   | 0.0701       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    value_loss           | 0.208        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -9.38    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019953987 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 1.24     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005923507 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.26    |\n",
      "|    ep_true_rew_mean | 26.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.7         |\n",
      "|    mean_reward          | 60.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009570795 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.13    |\n",
      "|    ep_true_rew_mean | 31.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008325258 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | -0.998   |\n",
      "|    ep_true_rew_mean | 39.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.7         |\n",
      "|    mean_reward          | 24.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101185255 |\n",
      "|    clip_fraction        | 0.0716       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.518       |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0867       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00464     |\n",
      "|    value_loss           | 0.209        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=35.80 +/- 60.81\n",
      "Episode length: 14.20 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 35.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.5        |\n",
      "|    mean_reward          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003262207 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 26.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.3         |\n",
      "|    mean_reward          | 48.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036632558 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.259        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00736     |\n",
      "|    value_loss           | 0.239        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | -0.991   |\n",
      "|    ep_true_rew_mean | 44       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.6          |\n",
      "|    mean_reward          | 60.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057414547 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.408       |\n",
      "|    explained_variance   | 0.374        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00788     |\n",
      "|    value_loss           | 0.214        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.6     |\n",
      "|    ep_rew_mean      | -0.871   |\n",
      "|    ep_true_rew_mean | 53.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16           |\n",
      "|    mean_reward          | 24           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039506955 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.422       |\n",
      "|    explained_variance   | 0.36         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0655       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    value_loss           | 0.205        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.808   |\n",
      "|    ep_true_rew_mean | 60.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=11.30 +/- 55.45\n",
      "Episode length: 18.70 +/- 9.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.7        |\n",
      "|    mean_reward          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043795247 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0719      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.1      |\n",
      "|    ep_rew_mean      | -0.43    |\n",
      "|    ep_true_rew_mean | 86.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016519476 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.446      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.28     |\n",
      "|    ep_rew_mean      | -0.201   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010456933 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.0493      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=95.40 +/- 1.02\n",
      "Episode length: 4.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.59     |\n",
      "|    ep_rew_mean      | -0.218   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010895772 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.0344      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050485253 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0695     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 0.00979     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.24     |\n",
      "|    ep_rew_mean      | -0.197   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22223839 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.0775     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0668    |\n",
      "|    value_loss           | 0.0126     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.90 +/- 1.37\n",
      "Episode length: 3.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0826  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08821404 |\n",
      "|    clip_fraction        | 0.0607     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    value_loss           | 0.00402    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0624  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002027642 |\n",
      "|    clip_fraction        | 0.0118      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0736     |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.70 +/- 1.35\n",
      "Episode length: 4.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.0945  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | 96.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05721551 |\n",
      "|    clip_fraction        | 0.0311     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0613    |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00116   |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 0.00442    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.9      |\n",
      "|    ep_rew_mean      | -0.0698  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=97.30 +/- 1.49\n",
      "Episode length: 2.70 +/- 1.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.7          |\n",
      "|    mean_reward          | 97.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071679456 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0589      |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00255     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00836     |\n",
      "|    value_loss           | 0.000434     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23500, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.075   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014995879 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0549      |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0135       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    value_loss           | 0.000225     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0762  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=95.60 +/- 1.02\n",
      "Episode length: 4.40 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029268959 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.056       |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 0.000168     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75     |\n",
      "|    ep_rew_mean      | -0.0645  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 3.9           |\n",
      "|    mean_reward          | 96.1          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 26000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00077842036 |\n",
      "|    clip_fraction        | 0.0102        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0564       |\n",
      "|    explained_variance   | 0.99          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00572       |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    value_loss           | 5e-05         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.00 +/- 1.79\n",
      "Episode length: 4.00 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0737  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044425367 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0552      |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0251      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    value_loss           | 0.000803     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=95.90 +/- 0.83\n",
      "Episode length: 4.10 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.03     |\n",
      "|    ep_rew_mean      | -0.0792  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028767255 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0475      |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00238     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00672     |\n",
      "|    value_loss           | 0.00035      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "execution time: 374.0869381427765; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d871a9f75b4347b3dd8c7647fef746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -7.16    |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014135069 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0556     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0242      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.092       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -0.404   |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.1       |\n",
      "|    mean_reward          | 23.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00865512 |\n",
      "|    clip_fraction        | 0.0411     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.58      |\n",
      "|    explained_variance   | 0.0198     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0779     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00906   |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -1.87    |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009307428 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0319      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | 24.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 11.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010244828 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0186      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.086       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.38    |\n",
      "|    ep_true_rew_mean | 28.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010789758 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0744      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 27.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016475223 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0922      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | -1.11    |\n",
      "|    ep_true_rew_mean | 49.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=84.00 +/- 36.34\n",
      "Episode length: 6.00 +/- 6.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019448554 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0756      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | -0.759   |\n",
      "|    ep_true_rew_mean | 72.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.60 +/- 36.55\n",
      "Episode length: 5.40 +/- 6.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018580515 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | -0.651   |\n",
      "|    ep_true_rew_mean | 77.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.50 +/- 0.92\n",
      "Episode length: 3.50 +/- 0.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0153530445 |\n",
      "|    clip_fraction        | 0.267        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.363        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0325       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0425      |\n",
      "|    value_loss           | 0.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=84.30 +/- 36.46\n",
      "Episode length: 5.70 +/- 6.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | 84.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.7      |\n",
      "|    ep_rew_mean      | -0.334   |\n",
      "|    ep_true_rew_mean | 91.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=85.00 +/- 36.67\n",
      "Episode length: 5.00 +/- 6.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 85          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016959209 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00326     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0557     |\n",
      "|    value_loss           | 0.0665      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=97.10 +/- 1.14\n",
      "Episode length: 2.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.64     |\n",
      "|    ep_rew_mean      | -0.26    |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026125206 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.771      |\n",
      "|    explained_variance   | 0.082       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.51     |\n",
      "|    ep_rew_mean      | -0.152   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.20 +/- 0.75\n",
      "Episode length: 2.80 +/- 0.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | 97.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03473048 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.546     |\n",
      "|    explained_variance   | 0.363      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 0.014      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.123   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.10 +/- 1.51\n",
      "Episode length: 3.90 +/- 1.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018988375 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.064      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.086   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032170482 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0493     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.00 +/- 1.55\n",
      "Episode length: 4.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | -0.0656  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006706354 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.000885    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.65     |\n",
      "|    ep_rew_mean      | -0.085   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040373635 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0322      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0159      |\n",
      "|    value_loss           | 0.000975     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.52     |\n",
      "|    ep_rew_mean      | -0.0703  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028164503 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000454     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0129      |\n",
      "|    value_loss           | 0.000551     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1442df35574cdeb6529e85b26bba40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 3.69     |\n",
      "| time/               |          |\n",
      "|    fps              | 218      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=24.20 +/- 60.27\n",
      "Episode length: 15.80 +/- 11.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013733361 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.462      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=23.80 +/- 59.78\n",
      "Episode length: 16.20 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 4.71     |\n",
      "| time/               |          |\n",
      "|    fps              | 201      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16.3         |\n",
      "|    mean_reward          | 23.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117554115 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | -0.00968     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0831       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.206        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 13.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=35.10 +/- 60.11\n",
      "Episode length: 14.90 +/- 10.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.9        |\n",
      "|    mean_reward          | 35.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011168891 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0803      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.217       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 26.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013577092 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0851      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.39    |\n",
      "|    ep_true_rew_mean | 29.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014561215 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | -0.99    |\n",
      "|    ep_true_rew_mean | 52.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02055023 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.315      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0303     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=97.10 +/- 1.22\n",
      "Episode length: 2.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | -0.624   |\n",
      "|    ep_true_rew_mean | 80.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022264082 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00877    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.28     |\n",
      "|    ep_rew_mean      | -0.522   |\n",
      "|    ep_true_rew_mean | 84.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.90 +/- 0.83\n",
      "Episode length: 4.10 +/- 0.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030592913 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00342     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.86     |\n",
      "|    ep_rew_mean      | -0.324   |\n",
      "|    ep_true_rew_mean | 90.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022510074 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.079       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.97     |\n",
      "|    ep_rew_mean      | -0.161   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025184078 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | -0.164   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020822223 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00947    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.0113      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.50 +/- 1.36\n",
      "Episode length: 4.50 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01604364 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.33      |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    value_loss           | 0.00431    |\n",
      "----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4608ed023225408bb4afe554da0444e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -2.05    |\n",
      "|    ep_true_rew_mean | -11.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013311477 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | -0.0339     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0916      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00599    |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -7.02    |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006107338 |\n",
      "|    clip_fraction        | 0.0822      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.0228      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0967      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 2.09     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005995898 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.00362     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0084     |\n",
      "|    value_loss           | 0.276       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 10.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030296491 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.0255       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.14         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00518     |\n",
      "|    value_loss           | 0.281        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | 18.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002701645 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.0425      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00782    |\n",
      "|    value_loss           | 0.269       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.53    |\n",
      "|    ep_true_rew_mean | 22.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077400655 |\n",
      "|    clip_fraction        | 0.0804       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.0773       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0809       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.239        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.37    |\n",
      "|    ep_true_rew_mean | 32.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013464365 |\n",
      "|    clip_fraction        | 0.0848      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0789      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.27    |\n",
      "|    ep_true_rew_mean | 34.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007985828 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00963    |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 40.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009092451 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0763      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.09    |\n",
      "|    ep_true_rew_mean | 42.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16.2         |\n",
      "|    mean_reward          | 23.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077009606 |\n",
      "|    clip_fraction        | 0.0968       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.389        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0899       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 0.185        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | -0.904   |\n",
      "|    ep_true_rew_mean | 52.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.29\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.8       |\n",
      "|    mean_reward          | 24.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04357963 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.35      |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0693     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    value_loss           | 0.187      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.39     |\n",
      "|    ep_rew_mean      | -0.564   |\n",
      "|    ep_true_rew_mean | 77.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023823602 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.61     |\n",
      "|    ep_rew_mean      | -0.31    |\n",
      "|    ep_true_rew_mean | 90.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=23.90 +/- 59.90\n",
      "Episode length: 16.10 +/- 10.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014286753 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00181    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.9     |\n",
      "|    mean_reward     | 12.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78     |\n",
      "|    ep_rew_mean      | -0.237   |\n",
      "|    ep_true_rew_mean | 93.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013177457 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.0527      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.00 +/- 1.00\n",
      "Episode length: 4.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.126   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017681539 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.0192      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.00 +/- 1.48\n",
      "Episode length: 3.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.41     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015404584 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0499     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.00736     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.0972  |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.8       |\n",
      "|    mean_reward          | 96.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0420316 |\n",
      "|    clip_fraction        | 0.0892    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.222    |\n",
      "|    explained_variance   | 0.481     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0224   |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0374   |\n",
      "|    value_loss           | 0.00349   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.00 +/- 0.63\n",
      "Episode length: 5.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.071   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.80 +/- 0.75\n",
      "Episode length: 4.20 +/- 0.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039925766 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.587        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0435      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 0.00136      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94     |\n",
      "|    ep_rew_mean      | -0.085   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011256846 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.90 +/- 0.54\n",
      "Episode length: 4.10 +/- 0.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.68     |\n",
      "|    ep_rew_mean      | -0.0648  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18578562 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.000175   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "execution time: 272.6640019416809; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470f587461d543d7ae281fd70859e861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 6.17     |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012104446 |\n",
      "|    clip_fraction        | 0.086       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.0998      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=35.50 +/- 60.50\n",
      "Episode length: 14.50 +/- 10.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.5     |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 12       |\n",
      "| time/               |          |\n",
      "|    fps              | 207      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012799003 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0465      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 23.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011565452 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0702      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.236       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 31.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012098154 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=72.50 +/- 48.75\n",
      "Episode length: 7.50 +/- 8.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 56.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=97.10 +/- 1.04\n",
      "Episode length: 2.90 +/- 1.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.9          |\n",
      "|    mean_reward          | 97.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0140619995 |\n",
      "|    clip_fraction        | 0.198        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    value_loss           | 0.186        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=84.40 +/- 36.49\n",
      "Episode length: 5.60 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.824   |\n",
      "|    ep_true_rew_mean | 66.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.00 +/- 48.52\n",
      "Episode length: 8.00 +/- 8.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | 72          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018244933 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.46     |\n",
      "|    ep_rew_mean      | -0.577   |\n",
      "|    ep_true_rew_mean | 85.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024642978 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000691   |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=96.00 +/- 1.67\n",
      "Episode length: 4.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.37     |\n",
      "|    ep_rew_mean      | -0.319   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02764144 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.971     |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.061     |\n",
      "|    value_loss           | 0.0544     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8      |\n",
      "|    ep_rew_mean      | -0.258   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040029578 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.732      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0628     |\n",
      "|    value_loss           | 0.027       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02782253 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.522     |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0532    |\n",
      "|    value_loss           | 0.0097     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa724a7d83274616bf1f7ef8e244444e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | -0.478   |\n",
      "| time/               |          |\n",
      "|    fps              | 222      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010107434 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.132      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | -1.51    |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008881746 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.014       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 2.91     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012496717 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0311      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0912      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 13.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011879181 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0831      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 22.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015632007 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0777      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 44.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.2        |\n",
      "|    mean_reward          | 60.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01242399 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.107      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 58.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.90 +/- 48.20\n",
      "Episode length: 20.90 +/- 8.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.9       |\n",
      "|    mean_reward          | -0.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01687138 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0333     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.031     |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | -0.642   |\n",
      "|    ep_true_rew_mean | 82.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.00 +/- 36.35\n",
      "Episode length: 6.00 +/- 6.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021270055 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0052      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.96     |\n",
      "|    ep_rew_mean      | -0.501   |\n",
      "|    ep_true_rew_mean | 86       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.80 +/- 1.60\n",
      "Episode length: 3.20 +/- 1.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02409453 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.38       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00572    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.057     |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.04     |\n",
      "|    ep_rew_mean      | -0.261   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024599757 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.86       |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.053      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0509     |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.21     |\n",
      "|    ep_rew_mean      | -0.192   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034554303 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.0166      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.53     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.80 +/- 1.72\n",
      "Episode length: 3.20 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024771236 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.113   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01089522 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.32      |\n",
      "|    explained_variance   | 0.517      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0643    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    value_loss           | 0.00337    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=97.20 +/- 1.17\n",
      "Episode length: 2.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013070598 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.85     |\n",
      "|    ep_rew_mean      | -0.0752  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051258425 |\n",
      "|    clip_fraction        | 0.0465       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.015       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0198      |\n",
      "|    value_loss           | 0.000861     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.0732  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014494892 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.157      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00125    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.000742    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.074   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.41\n",
      "Episode length: 3.00 +/- 1.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3            |\n",
      "|    mean_reward          | 97           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024468852 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000198    |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 0.000684     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.072   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=97.30 +/- 1.00\n",
      "Episode length: 2.70 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.7          |\n",
      "|    mean_reward          | 97.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026399624 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 0.000636     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | -0.0687  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005564181 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.091      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00564    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.000443    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0696  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004886086 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.071      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00631    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.00031     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0648  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030899462 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0556      |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0403      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00793     |\n",
      "|    value_loss           | 0.000466     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.50 +/- 0.50\n",
      "Episode length: 4.50 +/- 0.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0674  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005661078 |\n",
      "|    clip_fraction        | 0.00576      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0529      |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000968    |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00659     |\n",
      "|    value_loss           | 0.000277     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0702  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012521478 |\n",
      "|    clip_fraction        | 0.00898      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0491      |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000372    |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00801     |\n",
      "|    value_loss           | 0.000234     |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7617840a10c74a05ad6a730b37fa27bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 13.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008122021 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.84       |\n",
      "|    explained_variance   | -0.201      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | -1.39    |\n",
      "|    ep_true_rew_mean | 26       |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008707361 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.899      |\n",
      "|    explained_variance   | 0.0947      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.096       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17       |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 35       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010340214 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.954      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.1     |\n",
      "|    ep_rew_mean      | -1.2     |\n",
      "|    ep_true_rew_mean | 34.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013457183 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | -0.813   |\n",
      "|    ep_true_rew_mean | 60.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011326959 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.945      |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | -0.676   |\n",
      "|    ep_true_rew_mean | 74.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022354264 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.81     |\n",
      "|    ep_rew_mean      | -0.427   |\n",
      "|    ep_true_rew_mean | 86.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018522203 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.826      |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=23.40 +/- 59.28\n",
      "Episode length: 16.60 +/- 10.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | 23.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.36     |\n",
      "|    ep_rew_mean      | -0.295   |\n",
      "|    ep_true_rew_mean | 91.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013846206 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.728      |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00162     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.0766      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.60 +/- 1.02\n",
      "Episode length: 4.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.13     |\n",
      "|    ep_rew_mean      | -0.264   |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015042894 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0612     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 0.0358      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.70 +/- 1.10\n",
      "Episode length: 3.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.63     |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024681553 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.125   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025463605 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0387     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.00624     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2      |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993071 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82     |\n",
      "|    ep_rew_mean      | -0.0736  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003085916 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01     |\n",
      "|    ep_rew_mean      | -0.0871  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023026837 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0101      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 0.00134      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.0741  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009334292 |\n",
      "|    clip_fraction        | 0.0213      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00848    |\n",
      "|    value_loss           | 0.000326    |\n",
      "-----------------------------------------\n",
      "execution time: 261.6913092136383; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c888f13c14e84708b66060c19092d098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 4.35     |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011001246 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0939     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.0997      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 1.72     |\n",
      "| time/               |          |\n",
      "|    fps              | 202      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011658948 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0261      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 4.6      |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010917016 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0392      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 13.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | 60.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013289951 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.238       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.44    |\n",
      "|    ep_true_rew_mean | 25.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016146049 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0859      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 44.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013183203 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0312      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | -1.04    |\n",
      "|    ep_true_rew_mean | 46.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.5        |\n",
      "|    mean_reward          | 60.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01438879 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.347      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0613     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | 48       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | -0.838   |\n",
      "|    ep_true_rew_mean | 60.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.5          |\n",
      "|    mean_reward          | 72.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092515685 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.357        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0727       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.023       |\n",
      "|    value_loss           | 0.174        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | -0.698   |\n",
      "|    ep_true_rew_mean | 69.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=85.10 +/- 36.72\n",
      "Episode length: 4.90 +/- 6.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 85.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013044741 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.085       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.69     |\n",
      "|    ep_rew_mean      | -0.408   |\n",
      "|    ep_true_rew_mean | 82.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.9        |\n",
      "|    mean_reward          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01517871 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0184    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=85.10 +/- 36.72\n",
      "Episode length: 4.90 +/- 6.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.34     |\n",
      "|    ep_rew_mean      | -0.381   |\n",
      "|    ep_true_rew_mean | 88.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 72.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014954865 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.997      |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00528    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 0.0816      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.52     |\n",
      "|    ep_rew_mean      | -0.235   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.30 +/- 1.42\n",
      "Episode length: 2.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.7         |\n",
      "|    mean_reward          | 97.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011753382 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.791      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=84.80 +/- 36.62\n",
      "Episode length: 5.20 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.43     |\n",
      "|    ep_rew_mean      | -0.205   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010570261 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 0.028       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01894566 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.482     |\n",
      "|    explained_variance   | 0.492      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0719    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.00867    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05     |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022707433 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.00 +/- 1.00\n",
      "Episode length: 3.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19     |\n",
      "|    ep_rew_mean      | -0.0984  |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.90 +/- 0.70\n",
      "Episode length: 3.10 +/- 0.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009991335 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.0019      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | -0.0791  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009840397 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.000914    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e70d4ec1ff4cc2a0577b961afa8a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    ep_true_rew_mean | -14.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 222      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011476775 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0888     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -8.37    |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071013086 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.0199      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00708     |\n",
      "|    value_loss           | 0.183        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -1.39    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009882459 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0717      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0763      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 11.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.4       |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01141005 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.57      |\n",
      "|    explained_variance   | 0.0873     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0756     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 21.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009376886 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0811      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 25.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.90 +/- 59.90\n",
      "Episode length: 16.10 +/- 10.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008162906 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.19    |\n",
      "|    ep_true_rew_mean | 40.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=72.90 +/- 48.96\n",
      "Episode length: 7.10 +/- 8.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014456548 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0732      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=35.70 +/- 60.70\n",
      "Episode length: 14.30 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.3     |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | -0.931   |\n",
      "|    ep_true_rew_mean | 61.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 72.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019085843 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=47.70 +/- 59.36\n",
      "Episode length: 12.30 +/- 10.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.3     |\n",
      "|    mean_reward     | 47.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.703   |\n",
      "|    ep_true_rew_mean | 71.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.20 +/- 36.42\n",
      "Episode length: 5.80 +/- 6.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 84.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018770244 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=60.70 +/- 56.11\n",
      "Episode length: 9.30 +/- 10.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.35     |\n",
      "|    ep_rew_mean      | -0.444   |\n",
      "|    ep_true_rew_mean | 83.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=72.90 +/- 48.96\n",
      "Episode length: 7.10 +/- 9.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018608136 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0398     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.52     |\n",
      "|    ep_rew_mean      | -0.294   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.6        |\n",
      "|    mean_reward          | 84.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02214384 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.954     |\n",
      "|    explained_variance   | 0.479      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 0.0651     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=84.10 +/- 36.39\n",
      "Episode length: 5.90 +/- 6.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.07     |\n",
      "|    ep_rew_mean      | -0.242   |\n",
      "|    ep_true_rew_mean | 92.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=84.10 +/- 36.39\n",
      "Episode length: 5.90 +/- 6.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.9         |\n",
      "|    mean_reward          | 84.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014618671 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00447    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.0329      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.98     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.10 +/- 0.83\n",
      "Episode length: 3.90 +/- 0.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017607942 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0318     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.0154      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.55     |\n",
      "|    ep_rew_mean      | -0.124   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009149492 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 0.00752     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.06     |\n",
      "|    ep_rew_mean      | -0.0989  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008664793 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97     |\n",
      "|    ep_rew_mean      | -0.0814  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019466303 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    value_loss           | 0.0018      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.60 +/- 1.11\n",
      "Episode length: 4.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0673  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010979846 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01     |\n",
      "|    ep_rew_mean      | -0.079   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.50 +/- 1.75\n",
      "Episode length: 3.50 +/- 1.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009957466 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.157      |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0804  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043837563 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0189      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    value_loss           | 0.000542     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.0713  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024143796 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.000379    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.90 +/- 1.37\n",
      "Episode length: 3.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21500, episode_reward=95.90 +/- 0.83\n",
      "Episode length: 4.10 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17754036 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.144     |\n",
      "|    explained_variance   | 0.0564     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0469    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0506    |\n",
      "|    value_loss           | 0.0193     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.0779  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024666652 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0943     |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0759  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=95.80 +/- 1.08\n",
      "Episode length: 4.20 +/- 1.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00817694 |\n",
      "|    clip_fraction        | 0.0587     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0984    |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00841    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00963   |\n",
      "|    value_loss           | 0.000292   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0677  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004546494 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0819     |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    value_loss           | 0.000233    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.0647  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048842537 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0771      |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00441      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00773     |\n",
      "|    value_loss           | 0.000458     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f557948929c4264a0b4d22f5813b460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -16.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017759737 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | -0.0125     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | -2.42    |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00742586 |\n",
      "|    clip_fraction        | 0.0964     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.585     |\n",
      "|    explained_variance   | -0.109     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0965     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00691   |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 4.48     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006571236 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0856      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 2.21     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008274079 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 7.55     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=73.10 +/- 49.06\n",
      "Episode length: 6.90 +/- 9.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.9         |\n",
      "|    mean_reward          | 73.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005245479 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0927      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | -1.44    |\n",
      "|    ep_true_rew_mean | 10       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.8         |\n",
      "|    mean_reward          | 48.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041591916 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00401     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=23.40 +/- 59.28\n",
      "Episode length: 16.60 +/- 10.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | 23.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.3     |\n",
      "|    ep_true_rew_mean | 18.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.7         |\n",
      "|    mean_reward          | 36.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055195596 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0797       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00398     |\n",
      "|    value_loss           | 0.241        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 33.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.3        |\n",
      "|    mean_reward          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007927901 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | -1.13    |\n",
      "|    ep_true_rew_mean | 28.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.3        |\n",
      "|    mean_reward          | 48.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004072556 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | -1.23    |\n",
      "|    ep_true_rew_mean | 22.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16.4         |\n",
      "|    mean_reward          | 23.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042517553 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.513       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    value_loss           | 0.209        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "execution time: 288.57901310920715; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e7e5a4a1ec4772987970191bb7165b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 8.23     |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010645363 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0704     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.0971      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 4.89     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011802296 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0574      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | 4.22     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=85.60 +/- 36.87\n",
      "Episode length: 4.40 +/- 6.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 85.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125513505 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | -0.0146      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0187      |\n",
      "|    value_loss           | 0.227        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 13.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.30 +/- 61.31\n",
      "Episode length: 13.70 +/- 11.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014079426 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0274      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0673      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 24.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.9        |\n",
      "|    mean_reward          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010201745 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0738      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 33.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009151051 |\n",
      "|    clip_fraction        | 0.0853      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 42.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006982635 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0742      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.956   |\n",
      "|    ep_true_rew_mean | 50.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012198462 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0596      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 43       |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010011585 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0721      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=85.00 +/- 36.68\n",
      "Episode length: 5.00 +/- 6.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | -0.893   |\n",
      "|    ep_true_rew_mean | 50.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=72.90 +/- 48.95\n",
      "Episode length: 7.10 +/- 8.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011738291 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0807      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c43535c4df4c87b1bb31d1df2f3ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 221      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011955271 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.104      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00256     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | -4.82    |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008355981 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0264      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 0.62     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078742895 |\n",
      "|    clip_fraction        | 0.0965       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | 0.0795       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0765       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 0.214        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 1.06     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010044707 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0982      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 13.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010853069 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.052       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | -1.4     |\n",
      "|    ep_true_rew_mean | 25.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.4         |\n",
      "|    mean_reward          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091358125 |\n",
      "|    clip_fraction        | 0.0925       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | 0.0996       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.125        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.228        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 40.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015578209 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0886      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.2     |\n",
      "|    ep_true_rew_mean | 36.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014716878 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=47.90 +/- 59.54\n",
      "Episode length: 12.10 +/- 10.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | -0.931   |\n",
      "|    ep_true_rew_mean | 54.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=11.30 +/- 55.45\n",
      "Episode length: 18.70 +/- 9.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.7        |\n",
      "|    mean_reward          | 11.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017960731 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.50 +/- 59.40\n",
      "Episode length: 16.50 +/- 10.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.5     |\n",
      "|    mean_reward     | 23.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.6     |\n",
      "|    ep_rew_mean      | -0.645   |\n",
      "|    ep_true_rew_mean | 77.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=83.80 +/- 36.28\n",
      "Episode length: 6.20 +/- 6.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | 83.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016713832 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00967     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=71.80 +/- 48.41\n",
      "Episode length: 8.20 +/- 8.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 71.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.01     |\n",
      "|    ep_rew_mean      | -0.417   |\n",
      "|    ep_true_rew_mean | 87       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=71.90 +/- 48.46\n",
      "Episode length: 8.10 +/- 8.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.1         |\n",
      "|    mean_reward          | 71.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020508766 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0275     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 0.0925      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.81     |\n",
      "|    ep_rew_mean      | -0.317   |\n",
      "|    ep_true_rew_mean | 92.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019431252 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0532     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.0533      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=84.30 +/- 36.44\n",
      "Episode length: 5.70 +/- 6.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | 84.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.98     |\n",
      "|    ep_rew_mean      | -0.238   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=83.90 +/- 36.31\n",
      "Episode length: 6.10 +/- 6.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.1         |\n",
      "|    mean_reward          | 83.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028293742 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.734      |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0692     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 0.0222      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020479636 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.519      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0618     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 0.00754     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.60 +/- 0.80\n",
      "Episode length: 3.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011899609 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.00561     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.93     |\n",
      "|    ep_rew_mean      | -0.0812  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008353118 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17     |\n",
      "|    ep_rew_mean      | -0.0882  |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008061542 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.236      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.00155     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0776  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007047743 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.00162     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.83     |\n",
      "|    ep_rew_mean      | -0.0694  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006894985 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.000712    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=95.40 +/- 1.36\n",
      "Episode length: 4.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72     |\n",
      "|    ep_rew_mean      | -0.0707  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18313733 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.192     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    value_loss           | 0.000416   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.5      |\n",
      "|    ep_rew_mean      | -0.609   |\n",
      "|    ep_true_rew_mean | 86.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02657425 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.391     |\n",
      "|    explained_variance   | -0.00469   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0486    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.47     |\n",
      "|    ep_rew_mean      | -0.326   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01517284 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.371     |\n",
      "|    explained_variance   | -0.031     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00843   |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 0.0939     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.81     |\n",
      "|    ep_rew_mean      | -0.277   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036475375 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | -0.000612   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000619    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0566     |\n",
      "|    value_loss           | 0.0648      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=95.50 +/- 1.20\n",
      "Episode length: 4.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.53     |\n",
      "|    ep_rew_mean      | -0.139   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123115055 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0414     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf732ddb62e9491e9dc507b7de79620b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 16.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010489869 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.214      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 14.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012913106 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.0278      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | -1.27    |\n",
      "|    ep_true_rew_mean | 28.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011976875 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | -1.09    |\n",
      "|    ep_true_rew_mean | 41.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.1         |\n",
      "|    mean_reward          | 11.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120325545 |\n",
      "|    clip_fraction        | 0.161        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.2          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0968       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    value_loss           | 0.213        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.913   |\n",
      "|    ep_true_rew_mean | 56       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=47.70 +/- 59.36\n",
      "Episode length: 12.30 +/- 10.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.3        |\n",
      "|    mean_reward          | 47.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013964427 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0788      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.851   |\n",
      "|    ep_true_rew_mean | 62.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | 60.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010836824 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | -0.67    |\n",
      "|    ep_true_rew_mean | 74.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016537545 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.93     |\n",
      "|    ep_rew_mean      | -0.498   |\n",
      "|    ep_true_rew_mean | 83.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.20 +/- 36.41\n",
      "Episode length: 5.80 +/- 6.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.8        |\n",
      "|    mean_reward          | 84.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01920882 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.361      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00717    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.35     |\n",
      "|    ep_rew_mean      | -0.366   |\n",
      "|    ep_true_rew_mean | 88.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01999934 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.944     |\n",
      "|    explained_variance   | 0.309      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.0979     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.03     |\n",
      "|    ep_rew_mean      | -0.25    |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021367084 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.755      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0614     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 0.043       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.03     |\n",
      "|    ep_rew_mean      | -0.166   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.60 +/- 0.80\n",
      "Episode length: 3.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029674929 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.0131      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.125   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030021168 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0395     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43     |\n",
      "|    ep_rew_mean      | -0.107   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.9       |\n",
      "|    mean_reward          | 96.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 13500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0190406 |\n",
      "|    clip_fraction        | 0.0729    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.253    |\n",
      "|    explained_variance   | 0.486     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0291   |\n",
      "|    n_updates            | 130       |\n",
      "|    policy_gradient_loss | -0.0277   |\n",
      "|    value_loss           | 0.00351   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.80 +/- 0.87\n",
      "Episode length: 4.20 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97     |\n",
      "|    ep_rew_mean      | -0.0825  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.70 +/- 1.79\n",
      "Episode length: 4.30 +/- 1.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00356719 |\n",
      "|    clip_fraction        | 0.0463     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.191     |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00722   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.068   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032650423 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.782        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000947     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 0.000619     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | -0.0707  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004555629 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.137      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.000886    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "execution time: 278.2576446533203; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224d480c2b5244609a625b8339aadb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -9.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011489244 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.161      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -4.88    |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010590335 |\n",
      "|    clip_fraction        | 0.0467      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0428     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 4.97     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008622365 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.00228     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 10.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010368349 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0206      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.271       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.67    |\n",
      "|    ep_true_rew_mean | 9.24     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008339496 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0502      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0921      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.265       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | 11.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012089472 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | -0.0257     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.35    |\n",
      "|    ep_true_rew_mean | 32.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012431602 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0853      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | -1.21    |\n",
      "|    ep_true_rew_mean | 41.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010341737 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 48.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012824733 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0838      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | -0.789   |\n",
      "|    ep_true_rew_mean | 66.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015008753 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.03     |\n",
      "|    ep_rew_mean      | -0.549   |\n",
      "|    ep_true_rew_mean | 82       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019600157 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=36.70 +/- 61.70\n",
      "Episode length: 13.30 +/- 11.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.3     |\n",
      "|    mean_reward     | 36.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.85     |\n",
      "|    ep_rew_mean      | -0.431   |\n",
      "|    ep_true_rew_mean | 89.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.30 +/- 1.10\n",
      "Episode length: 2.70 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.7         |\n",
      "|    mean_reward          | 97.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013406648 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=84.80 +/- 36.63\n",
      "Episode length: 5.20 +/- 6.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.44     |\n",
      "|    ep_rew_mean      | -0.305   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018870529 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    value_loss           | 0.0559      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.42     |\n",
      "|    ep_rew_mean      | -0.225   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019660246 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.747      |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    value_loss           | 0.0176      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | -0.163   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033375815 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0584     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.26     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032300763 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.00716     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.0973  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016090527 |\n",
      "|    clip_fraction        | 0.0884      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0602     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4804ed572d0d4f04aded8e0b61277a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 2.06     |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011045813 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.231      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=36.80 +/- 61.80\n",
      "Episode length: 13.20 +/- 11.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | 36.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 1.82     |\n",
      "| time/               |          |\n",
      "|    fps              | 204      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011155909 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0124     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0691      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 6.63     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=36.30 +/- 61.31\n",
      "Episode length: 13.70 +/- 11.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013602929 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0511      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0983      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 19.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013816253 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.00728     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0895      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.3     |\n",
      "|    ep_true_rew_mean | 36.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.1        |\n",
      "|    mean_reward          | 59.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015201159 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | -1.12    |\n",
      "|    ep_true_rew_mean | 49.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017392632 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=97.10 +/- 1.30\n",
      "Episode length: 2.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | -0.667   |\n",
      "|    ep_true_rew_mean | 78.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020536713 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.42     |\n",
      "|    ep_rew_mean      | -0.461   |\n",
      "|    ep_true_rew_mean | 85.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025721308 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.10 +/- 0.83\n",
      "Episode length: 3.90 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.95     |\n",
      "|    ep_rew_mean      | -0.321   |\n",
      "|    ep_true_rew_mean | 92       |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=97.30 +/- 1.79\n",
      "Episode length: 2.70 +/- 1.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.7        |\n",
      "|    mean_reward          | 97.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02311475 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.901     |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    value_loss           | 0.0779     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75     |\n",
      "|    ep_rew_mean      | -0.218   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020735953 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0411     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.0219      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.87     |\n",
      "|    ep_rew_mean      | -0.164   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040129807 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0694     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0539     |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.111   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.30 +/- 1.55\n",
      "Episode length: 2.70 +/- 1.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.7         |\n",
      "|    mean_reward          | 97.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012290401 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0263     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.12     |\n",
      "|    ep_rew_mean      | -0.103   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008804335 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00615    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.40 +/- 0.80\n",
      "Episode length: 4.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.83     |\n",
      "|    ep_rew_mean      | -0.0774  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031461585 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00514     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.00125      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.50 +/- 0.92\n",
      "Episode length: 4.50 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e974ae8b98594d87b8e69c588df3ec1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 18.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010973525 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | -0.138      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.17    |\n",
      "|    ep_true_rew_mean | 40.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008549495 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.1     |\n",
      "|    ep_true_rew_mean | 44.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018840933 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.819      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0801      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.777   |\n",
      "|    ep_true_rew_mean | 67.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025817849 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=47.80 +/- 59.45\n",
      "Episode length: 12.20 +/- 10.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.18     |\n",
      "|    ep_rew_mean      | -0.446   |\n",
      "|    ep_true_rew_mean | 86.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017425917 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.31     |\n",
      "|    ep_rew_mean      | -0.293   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | 96.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01704419 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.646     |\n",
      "|    explained_variance   | 0.219      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    value_loss           | 0.073      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=96.30 +/- 0.78\n",
      "Episode length: 3.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.56     |\n",
      "|    ep_rew_mean      | -0.221   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027450044 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 0.0355      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=96.00 +/- 1.61\n",
      "Episode length: 4.00 +/- 1.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.67     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017968556 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | -0.00353    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0562     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    value_loss           | 0.00983     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=95.90 +/- 0.70\n",
      "Episode length: 4.10 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.90 +/- 0.94\n",
      "Episode length: 4.10 +/- 0.94\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.1       |\n",
      "|    mean_reward          | 95.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0885503 |\n",
      "|    clip_fraction        | 0.281     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.25     |\n",
      "|    explained_variance   | 0.299     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0751   |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0526   |\n",
      "|    value_loss           | 0.00871   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.0913  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033327207 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.161      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0244     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.00 +/- 0.77\n",
      "Episode length: 4.00 +/- 0.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0643  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028127136 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00712     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.00121      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.30 +/- 0.78\n",
      "Episode length: 4.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "execution time: 232.44206500053406; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c8eead90f54224b7e846d9ddeee521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -9.59    |\n",
      "| time/               |          |\n",
      "|    fps              | 233      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120399725 |\n",
      "|    clip_fraction        | 0.0858       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.189       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 0.168        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -7.17    |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011202659 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.00558     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -2.38    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00930886 |\n",
      "|    clip_fraction        | 0.0723     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | -0.0142    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.074      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 0.236      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 0.96     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.7       |\n",
      "|    mean_reward          | -12.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01263509 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.53      |\n",
      "|    explained_variance   | 0.0389     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.1        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    value_loss           | 0.255      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.63    |\n",
      "|    ep_true_rew_mean | 9.22     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 17.8         |\n",
      "|    mean_reward          | 12.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077121058 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.0219       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0748       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 0.236        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 18.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.7        |\n",
      "|    mean_reward          | 24.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013955451 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0717      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.7     |\n",
      "|    mean_reward     | 48.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 35.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.50 +/- 61.50\n",
      "Episode length: 13.50 +/- 11.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.5        |\n",
      "|    mean_reward          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011262671 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0954      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 46.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012693511 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.3     |\n",
      "|    ep_rew_mean      | -0.829   |\n",
      "|    ep_true_rew_mean | 61.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016520314 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.729   |\n",
      "|    ep_true_rew_mean | 73.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.90 +/- 56.24\n",
      "Episode length: 9.10 +/- 10.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.1         |\n",
      "|    mean_reward          | 60.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017937921 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0252      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.83     |\n",
      "|    ep_rew_mean      | -0.508   |\n",
      "|    ep_true_rew_mean | 84.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.8       |\n",
      "|    mean_reward          | 48.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02089471 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.404      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00387    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.1      |\n",
      "|    ep_rew_mean      | -0.353   |\n",
      "|    ep_true_rew_mean | 90.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=84.80 +/- 36.61\n",
      "Episode length: 5.20 +/- 6.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 84.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026847601 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0227      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 0.0565      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.08     |\n",
      "|    ep_rew_mean      | -0.275   |\n",
      "|    ep_true_rew_mean | 92.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 84.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016309991 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0541     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    value_loss           | 0.0396      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.19     |\n",
      "|    ep_rew_mean      | -0.187   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128516145 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.27         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0278       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0399      |\n",
      "|    value_loss           | 0.017        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.03     |\n",
      "|    ep_rew_mean      | -0.125   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013138527 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 0.00698     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.30 +/- 1.35\n",
      "Episode length: 2.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.06     |\n",
      "|    ep_rew_mean      | -0.103   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.79\n",
      "Episode length: 3.70 +/- 1.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015338878 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.0052      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0922  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009700983 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0592     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.0021      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=97.10 +/- 1.14\n",
      "Episode length: 2.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0789  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036266295 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.671        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.016       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.00146      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.53     |\n",
      "|    ep_rew_mean      | -0.0671  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048150495 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.705        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 0.000909     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0751  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038846522 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0095      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 0.000664     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51888f589fcf4f29a0c2763fa5422446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -2.27    |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011385232 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.406      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -0.312   |\n",
      "| time/               |          |\n",
      "|    fps              | 206      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010617765 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0357     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 10.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.5       |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01194719 |\n",
      "|    clip_fraction        | 0.1        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.052      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0858     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    value_loss           | 0.229      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 18.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012644303 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.088       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | -1.21    |\n",
      "|    ep_true_rew_mean | 39.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013144062 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | -0.992   |\n",
      "|    ep_true_rew_mean | 56.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=71.80 +/- 48.41\n",
      "Episode length: 8.20 +/- 8.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | 71.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018642006 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.033       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=72.40 +/- 48.72\n",
      "Episode length: 7.60 +/- 8.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.867   |\n",
      "|    ep_true_rew_mean | 65       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=84.10 +/- 36.38\n",
      "Episode length: 5.90 +/- 6.44\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.9        |\n",
      "|    mean_reward          | 84.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077869 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=71.50 +/- 48.25\n",
      "Episode length: 8.50 +/- 8.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.5      |\n",
      "|    mean_reward     | 71.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.84     |\n",
      "|    ep_rew_mean      | -0.494   |\n",
      "|    ep_true_rew_mean | 83.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020564862 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.17     |\n",
      "|    ep_rew_mean      | -0.352   |\n",
      "|    ep_true_rew_mean | 90.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01597142 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.967     |\n",
      "|    explained_variance   | 0.362      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.0703     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=95.70 +/- 0.90\n",
      "Episode length: 4.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.91     |\n",
      "|    ep_rew_mean      | -0.247   |\n",
      "|    ep_true_rew_mean | 93.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.80 +/- 0.87\n",
      "Episode length: 4.20 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020976255 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 0.0312      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.29     |\n",
      "|    ep_rew_mean      | -0.187   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02097376 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.621     |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0483    |\n",
      "|    value_loss           | 0.016      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.50 +/- 0.92\n",
      "Episode length: 4.50 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.6      |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 0.54\n",
      "Episode length: 4.10 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011413894 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.00836     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.111   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018258946 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0395     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.80 +/- 1.78\n",
      "Episode length: 3.20 +/- 1.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb4b12a0d74494baa87e494cefce07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 30       |\n",
      "| time/               |          |\n",
      "|    fps              | 225      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.9         |\n",
      "|    mean_reward          | 60.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050706606 |\n",
      "|    clip_fraction        | 0.0974      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | -0.121      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00899    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    value_loss           | 0.0912      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.959   |\n",
      "|    ep_true_rew_mean | 42.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 207      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.8       |\n",
      "|    mean_reward          | 36.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01770847 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.538     |\n",
      "|    explained_variance   | 0.313      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.05       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00995   |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.974   |\n",
      "|    ep_true_rew_mean | 44.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 204      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010587698 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.83     |\n",
      "|    ep_rew_mean      | -0.606   |\n",
      "|    ep_true_rew_mean | 69.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.80 +/- 56.18\n",
      "Episode length: 9.20 +/- 10.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.2        |\n",
      "|    mean_reward          | 60.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01809141 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.624     |\n",
      "|    explained_variance   | 0.458      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0367     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=23.50 +/- 59.41\n",
      "Episode length: 16.50 +/- 10.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.5     |\n",
      "|    mean_reward     | 23.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.14     |\n",
      "|    ep_rew_mean      | -0.535   |\n",
      "|    ep_true_rew_mean | 77.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.50 +/- 59.40\n",
      "Episode length: 16.50 +/- 10.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.5        |\n",
      "|    mean_reward          | 23.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026980244 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.18     |\n",
      "|    ep_rew_mean      | -0.35    |\n",
      "|    ep_true_rew_mean | 89.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015021326 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.54     |\n",
      "|    ep_rew_mean      | -0.289   |\n",
      "|    ep_true_rew_mean | 93.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013407454 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 0.0544      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.48     |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011274232 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 0.0253      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.67     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08137275 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.368     |\n",
      "|    explained_variance   | -0.0231    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.102     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.00816    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 1.73\n",
      "Episode length: 3.70 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 201      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051729098 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0703     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.00769     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 0.80\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 201      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.3       |\n",
      "|    mean_reward          | 96.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 11500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1244583 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.133    |\n",
      "|    explained_variance   | 0.304     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0378   |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -0.0391   |\n",
      "|    value_loss           | 0.00409   |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0706  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 202      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120433465 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.00041     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.38     |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028724581 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.0638      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0505     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0669     |\n",
      "|    value_loss           | 0.0218      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | -0.176   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.70 +/- 1.00\n",
      "Episode length: 4.30 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016778585 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.0407      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0623     |\n",
      "|    value_loss           | 0.0224      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.58     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 199      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.3       |\n",
      "|    mean_reward          | 96.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 15500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1676021 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.129    |\n",
      "|    explained_variance   | 0.079     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0625   |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0653   |\n",
      "|    value_loss           | 0.0113    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.0857  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038259454 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0725     |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00846    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.000995    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.90 +/- 0.94\n",
      "Episode length: 4.10 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "execution time: 267.8970160484314; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c58b4bcb944632967b721d596edde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | 1.58     |\n",
      "| time/               |          |\n",
      "|    fps              | 236      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006658717 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 6.78     |\n",
      "| time/               |          |\n",
      "|    fps              | 203      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.4         |\n",
      "|    mean_reward          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132568255 |\n",
      "|    clip_fraction        | 0.0915       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | 0.00332      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    value_loss           | 0.246        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=24.50 +/- 60.63\n",
      "Episode length: 15.50 +/- 11.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.5     |\n",
      "|    mean_reward     | 24.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 11.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008984935 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0162      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0785      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 16.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011119518 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0391      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0892      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.269       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.90 +/- 36.30\n",
      "Episode length: 22.90 +/- 6.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.9     |\n",
      "|    mean_reward     | -12.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 22.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=47.90 +/- 59.52\n",
      "Episode length: 12.10 +/- 10.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.1         |\n",
      "|    mean_reward          | 47.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131776165 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | 0.0871       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0571       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    value_loss           | 0.246        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | -1.15    |\n",
      "|    ep_true_rew_mean | 49.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=59.60 +/- 55.40\n",
      "Episode length: 10.40 +/- 9.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.4        |\n",
      "|    mean_reward          | 59.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015021659 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | -0.793   |\n",
      "|    ep_true_rew_mean | 73.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=72.50 +/- 48.75\n",
      "Episode length: 7.50 +/- 8.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.5         |\n",
      "|    mean_reward          | 72.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018508296 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=84.70 +/- 36.57\n",
      "Episode length: 5.30 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 84.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.06     |\n",
      "|    ep_rew_mean      | -0.45    |\n",
      "|    ep_true_rew_mean | 90.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022994006 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0609     |\n",
      "|    value_loss           | 0.0969      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=84.10 +/- 36.39\n",
      "Episode length: 5.90 +/- 6.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.57     |\n",
      "|    ep_rew_mean      | -0.322   |\n",
      "|    ep_true_rew_mean | 93.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03736016 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.923     |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0654    |\n",
      "|    value_loss           | 0.0445     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 0.64\n",
      "Episode length: 3.70 +/- 0.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.53     |\n",
      "|    ep_rew_mean      | -0.231   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03521478 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.702     |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0704    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0564    |\n",
      "|    value_loss           | 0.0186     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.98     |\n",
      "|    ep_rew_mean      | -0.121   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.40 +/- 1.62\n",
      "Episode length: 3.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017797364 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0679     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 0.00869     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2      |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024839684 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0679     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007478399 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a098ae3e084e42fcbc4dbbda92f4de1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -2.57    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012413237 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.462      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00208     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00843    |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -9.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011572468 |\n",
      "|    clip_fraction        | 0.0848      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -14.7    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01273831 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.044      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0729     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -12.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011160169 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0628      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -8.25    |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012909586 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0176      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -3.73    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895288 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0307      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0954      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 9.23     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012325935 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0291      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0854      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | -1.32    |\n",
      "|    ep_true_rew_mean | 30.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.9        |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011421389 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | -0.944   |\n",
      "|    ep_true_rew_mean | 62.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.80 +/- 36.61\n",
      "Episode length: 5.20 +/- 6.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 84.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015941247 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=71.90 +/- 48.46\n",
      "Episode length: 8.10 +/- 8.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | 71.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | -0.931   |\n",
      "|    ep_true_rew_mean | 62       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=84.30 +/- 36.45\n",
      "Episode length: 5.70 +/- 6.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015011266 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.8      |\n",
      "|    ep_rew_mean      | -0.555   |\n",
      "|    ep_true_rew_mean | 82.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019544827 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.31     |\n",
      "|    ep_rew_mean      | -0.447   |\n",
      "|    ep_true_rew_mean | 86.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=83.90 +/- 36.31\n",
      "Episode length: 6.10 +/- 6.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.1         |\n",
      "|    mean_reward          | 83.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020432487 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 0.0939      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.60 +/- 1.56\n",
      "Episode length: 3.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.33     |\n",
      "|    ep_rew_mean      | -0.277   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=84.30 +/- 36.45\n",
      "Episode length: 5.70 +/- 6.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023082172 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.795      |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.62     |\n",
      "|    ep_rew_mean      | -0.219   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03091272 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.0187     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=84.40 +/- 36.50\n",
      "Episode length: 5.60 +/- 6.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.121   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.50 +/- 0.67\n",
      "Episode length: 4.50 +/- 0.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02456345 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.398     |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    value_loss           | 0.0061     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.07     |\n",
      "|    ep_rew_mean      | -0.0936  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.60 +/- 0.49\n",
      "Episode length: 4.40 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009640954 |\n",
      "|    clip_fraction        | 0.0934      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00328     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.00332     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82     |\n",
      "|    ep_rew_mean      | -0.0769  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006061366 |\n",
      "|    clip_fraction        | 0.0506      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.202      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0581     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.0851  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062033413 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0169      |\n",
      "|    value_loss           | 0.000979     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.00 +/- 0.89\n",
      "Episode length: 4.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.0765  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.60 +/- 1.62\n",
      "Episode length: 3.40 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004464171 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000571    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0789  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003376987 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.000807    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=95.70 +/- 0.90\n",
      "Episode length: 4.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0721  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018865627 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00843     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00946     |\n",
      "|    value_loss           | 0.000545     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.85     |\n",
      "|    ep_rew_mean      | -0.0742  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04706409 |\n",
      "|    clip_fraction        | 0.0473     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.125     |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000408  |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    value_loss           | 0.000332   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389409e8871549be9bd1ae8daff786c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 1.75     |\n",
      "| time/               |          |\n",
      "|    fps              | 220      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014199698 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | -0.319      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00863    |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.57    |\n",
      "|    ep_true_rew_mean | 6.93     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008214839 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.0205      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 12.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008286283 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.088       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | -1.03    |\n",
      "|    ep_true_rew_mean | 47.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=48.00 +/- 59.62\n",
      "Episode length: 12.00 +/- 10.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012547685 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.836      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 44.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029549725 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.758   |\n",
      "|    ep_true_rew_mean | 64.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018876258 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.831      |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=60.80 +/- 56.18\n",
      "Episode length: 9.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 60.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.38     |\n",
      "|    ep_rew_mean      | -0.381   |\n",
      "|    ep_true_rew_mean | 89.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12         |\n",
      "|    mean_reward          | 48         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01667123 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.679     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0538     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=47.90 +/- 59.54\n",
      "Episode length: 12.10 +/- 10.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.41     |\n",
      "|    ep_rew_mean      | -0.204   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014088227 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | -0.0385     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.85     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.70 +/- 1.62\n",
      "Episode length: 3.30 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012306875 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.0212      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.90 +/- 1.64\n",
      "Episode length: 4.10 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014553646 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.062      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.0116      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033100035 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05     |\n",
      "|    ep_rew_mean      | -0.088   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049083354 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0202      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0158      |\n",
      "|    value_loss           | 0.00125      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.0859  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009679365 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.119      |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00301    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.00207     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0809  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050892048 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0946      |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00065     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.000617     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "execution time: 269.7919497489929; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad1e26464f54a9db077e580969610e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 237      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01045178 |\n",
      "|    clip_fraction        | 0.0659     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -0.0641    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -2.76    |\n",
      "| time/               |          |\n",
      "|    fps              | 201      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011264712 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0105     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 4.87     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011441149 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0172      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 16.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012522835 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0897      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0955      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 31.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009629637 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0879      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=47.80 +/- 59.45\n",
      "Episode length: 12.20 +/- 10.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | -1.04    |\n",
      "|    ep_true_rew_mean | 57.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10         |\n",
      "|    mean_reward          | 60         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01789313 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.306      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0322     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.4     |\n",
      "|    ep_rew_mean      | -0.752   |\n",
      "|    ep_true_rew_mean | 73.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018930495 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.51     |\n",
      "|    ep_rew_mean      | -0.485   |\n",
      "|    ep_true_rew_mean | 88.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024563741 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=84.90 +/- 36.66\n",
      "Episode length: 5.10 +/- 6.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 84.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.69     |\n",
      "|    ep_rew_mean      | -0.333   |\n",
      "|    ep_true_rew_mean | 93.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023260672 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.96       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2      |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022016458 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0481     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.0241      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.152   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027853068 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.0131      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.123   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020492647 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0533     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.0065      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.90 +/- 1.30\n",
      "Episode length: 3.10 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.92     |\n",
      "|    ep_rew_mean      | -0.0966  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.1       |\n",
      "|    mean_reward          | 95.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 13500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0193739 |\n",
      "|    clip_fraction        | 0.0788    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.252    |\n",
      "|    explained_variance   | 0.473     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00882  |\n",
      "|    n_updates            | 130       |\n",
      "|    policy_gradient_loss | -0.032    |\n",
      "|    value_loss           | 0.00256   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "training agent for task 648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe286c95d8fa43dd978f9e9bea6e640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -12.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 20.2      |\n",
      "|    mean_reward          | -0.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0082096 |\n",
      "|    clip_fraction        | 0.0693    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.6      |\n",
      "|    explained_variance   | -0.318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0122    |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -0.0122   |\n",
      "|    value_loss           | 0.111     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 2.92     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012303708 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0454     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 13.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=72.60 +/- 48.81\n",
      "Episode length: 7.40 +/- 8.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 72.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010776655 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0237      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 20.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 20.3     |\n",
      "|    mean_reward          | -0.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 4500     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.015056 |\n",
      "|    clip_fraction        | 0.196    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.53    |\n",
      "|    explained_variance   | 0.109    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.0777   |\n",
      "|    n_updates            | 40       |\n",
      "|    policy_gradient_loss | -0.0238  |\n",
      "|    value_loss           | 0.217    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    ep_true_rew_mean | 31.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=35.70 +/- 60.70\n",
      "Episode length: 14.30 +/- 10.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 14.3       |\n",
      "|    mean_reward          | 35.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01190353 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.081      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    value_loss           | 0.205      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.30 +/- 61.31\n",
      "Episode length: 13.70 +/- 11.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 57.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019612957 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=47.90 +/- 59.54\n",
      "Episode length: 12.10 +/- 10.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | -0.705   |\n",
      "|    ep_true_rew_mean | 70.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 72.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020041563 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.77     |\n",
      "|    ep_rew_mean      | -0.496   |\n",
      "|    ep_true_rew_mean | 84.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017622715 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00824     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=97.20 +/- 1.25\n",
      "Episode length: 2.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.26     |\n",
      "|    ep_rew_mean      | -0.341   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | 47.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019708907 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.0581      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.49     |\n",
      "|    ep_rew_mean      | -0.211   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=84.00 +/- 36.35\n",
      "Episode length: 6.00 +/- 6.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026159484 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0611     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.0199      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=72.30 +/- 48.66\n",
      "Episode length: 7.70 +/- 8.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.7      |\n",
      "|    mean_reward     | 72.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.165   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019884013 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016373657 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | -0.113   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.70 +/- 1.00\n",
      "Episode length: 4.30 +/- 1.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01783859 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.282     |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0355    |\n",
      "|    value_loss           | 0.00291    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.0927  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005327038 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.616       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00867    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.00164     |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/648965b8bd9da2a6366f5e6425181f7046f249845cc638ab1dbc0cb6b38490af_transfer_from_8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d42c67b6ed49b6a5539201ca7f9317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 4.07     |\n",
      "| time/               |          |\n",
      "|    fps              | 227      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06895791 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.732     |\n",
      "|    explained_variance   | -0.588     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 6.78     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059396476 |\n",
      "|    clip_fraction        | 0.0993       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.809       |\n",
      "|    explained_variance   | 0.0267       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00849     |\n",
      "|    value_loss           | 0.186        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 22.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012555184 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 32.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 162      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01054897 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.875     |\n",
      "|    explained_variance   | 0.238      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0695     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 44.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 164      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011322115 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.855      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0676      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=85.00 +/- 36.68\n",
      "Episode length: 5.00 +/- 6.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 42.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018097598 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.81       |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0955      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.40 +/- 55.61\n",
      "Episode length: 18.60 +/- 9.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.6     |\n",
      "|    mean_reward     | 11.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.884   |\n",
      "|    ep_true_rew_mean | 56       |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | 60.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029321019 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.21        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.90 +/- 48.96\n",
      "Episode length: 7.10 +/- 8.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | 72.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.77     |\n",
      "|    ep_rew_mean      | -0.396   |\n",
      "|    ep_true_rew_mean | 87.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=73.20 +/- 49.11\n",
      "Episode length: 6.80 +/- 9.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 73.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012374375 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.3      |\n",
      "|    ep_rew_mean      | -0.356   |\n",
      "|    ep_true_rew_mean | 91.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.40 +/- 0.80\n",
      "Episode length: 4.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015310013 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    value_loss           | 0.0757      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.8      |\n",
      "|    ep_rew_mean      | -0.235   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.90 +/- 1.30\n",
      "Episode length: 3.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009777039 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 0.024       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.20 +/- 0.60\n",
      "Episode length: 3.80 +/- 0.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=97.10 +/- 0.94\n",
      "Episode length: 2.90 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021447107 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0567     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31     |\n",
      "|    ep_rew_mean      | -0.116   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04070072 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.259     |\n",
      "|    explained_variance   | 0.33       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0475    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.00539    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.111   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018895548 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.0848  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22305027 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.286     |\n",
      "|    explained_variance   | 0.702      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.01      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    value_loss           | 0.000905   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | -0.727   |\n",
      "|    ep_true_rew_mean | 77.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027010899 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.00702     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.07     |\n",
      "|    ep_rew_mean      | -0.373   |\n",
      "|    ep_true_rew_mean | 91.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020068523 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | -0.0288     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "execution time: 240.0934100151062; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e040fa33f6463dbfb5eec7141110f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | -16.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 231      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014501803 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.431      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00365     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0791      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.94    |\n",
      "|    ep_true_rew_mean | -13.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011223364 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0227      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00652    |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -7.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135347275 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 0.000248     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 0.226        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 7.52     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01530272 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.51      |\n",
      "|    explained_variance   | 0.0417     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0569     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.243      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 29.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013475932 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.249       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 61.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018353054 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=72.10 +/- 48.56\n",
      "Episode length: 7.90 +/- 8.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | -0.72    |\n",
      "|    ep_true_rew_mean | 80       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020322423 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.77     |\n",
      "|    ep_rew_mean      | -0.517   |\n",
      "|    ep_true_rew_mean | 89.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.3         |\n",
      "|    mean_reward          | 72.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035347804 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0653     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.99     |\n",
      "|    ep_rew_mean      | -0.274   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.60 +/- 0.92\n",
      "Episode length: 3.40 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028789397 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.887      |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 0.0438      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06     |\n",
      "|    ep_rew_mean      | -0.193   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.80 +/- 1.54\n",
      "Episode length: 3.20 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018288132 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0858     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.20 +/- 1.60\n",
      "Episode length: 3.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4      |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03346449 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.462     |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0755    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0533    |\n",
      "|    value_loss           | 0.00771    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=97.20 +/- 1.25\n",
      "Episode length: 2.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.0965  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.20 +/- 1.08\n",
      "Episode length: 2.80 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023315892 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0798  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046941894 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0486      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0195      |\n",
      "|    value_loss           | 0.00188      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.70 +/- 0.46\n",
      "Episode length: 3.30 +/- 0.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.62     |\n",
      "|    ep_rew_mean      | -0.084   |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011919081 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.151      |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0359     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.43     |\n",
      "|    ep_rew_mean      | -0.0737  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005063027 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75     |\n",
      "|    ep_rew_mean      | -0.0876  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029089295 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    value_loss           | 0.00056      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.10 +/- 1.04\n",
      "Episode length: 2.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94     |\n",
      "|    ep_rew_mean      | -0.0931  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049772304 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.09        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00223     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00885     |\n",
      "|    value_loss           | 0.000949     |\n",
      "------------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5741bf677594453b0d273781c6b3020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -4.65    |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011905076 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.733      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00902    |\n",
      "|    value_loss           | 0.0754      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -4.79    |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009424237 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.00993     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00904    |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=36.90 +/- 61.90\n",
      "Episode length: 13.10 +/- 11.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.1     |\n",
      "|    mean_reward     | 36.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -2.52    |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008796118 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0135     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00801    |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 0.99     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=23.50 +/- 59.40\n",
      "Episode length: 16.50 +/- 10.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.5        |\n",
      "|    mean_reward          | 23.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010349363 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0395      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.257       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 5.66     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.6       |\n",
      "|    mean_reward          | -0.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01142386 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.0259     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.091      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 18.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010752484 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0891      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 29.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.3        |\n",
      "|    mean_reward          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013660207 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0912      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | -1.19    |\n",
      "|    ep_true_rew_mean | 47       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014859271 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0742      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.829   |\n",
      "|    ep_true_rew_mean | 72.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020591058 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00762     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=48.30 +/- 59.86\n",
      "Episode length: 11.70 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.7     |\n",
      "|    mean_reward     | 48.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | -0.677   |\n",
      "|    ep_true_rew_mean | 81.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=70.60 +/- 47.81\n",
      "Episode length: 9.40 +/- 7.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | 70.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019609105 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=71.90 +/- 48.48\n",
      "Episode length: 8.10 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | 71.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.23     |\n",
      "|    ep_rew_mean      | -0.481   |\n",
      "|    ep_true_rew_mean | 90.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=82.90 +/- 36.01\n",
      "Episode length: 7.10 +/- 6.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 82.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020210452 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00974     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.0702      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=83.80 +/- 36.29\n",
      "Episode length: 6.20 +/- 6.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | 83.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.14     |\n",
      "|    ep_rew_mean      | -0.305   |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=71.30 +/- 48.18\n",
      "Episode length: 8.70 +/- 8.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.7         |\n",
      "|    mean_reward          | 71.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017956495 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.734      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0234     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.0271      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=71.80 +/- 48.42\n",
      "Episode length: 8.20 +/- 8.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 71.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.84     |\n",
      "|    ep_rew_mean      | -0.266   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015115592 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.017       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.00 +/- 1.41\n",
      "Episode length: 5.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.18     |\n",
      "|    ep_rew_mean      | -0.217   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.92\n",
      "Episode length: 3.90 +/- 1.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016414247 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0487     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.0065      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.9      |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.70 +/- 1.79\n",
      "Episode length: 4.30 +/- 1.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011987232 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.00551     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.40 +/- 1.36\n",
      "Episode length: 4.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.60 +/- 1.74\n",
      "Episode length: 4.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018131018 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0493     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.50 +/- 1.36\n",
      "Episode length: 4.50 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041317637 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.153       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0309      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    value_loss           | 0.00133      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=97.20 +/- 1.33\n",
      "Episode length: 2.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00286302 |\n",
      "|    clip_fraction        | 0.0226     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | 0.826      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0114    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.50 +/- 1.43\n",
      "Episode length: 4.50 +/- 1.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033639297 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 0.000714     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=94.40 +/- 0.80\n",
      "Episode length: 5.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 94.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034259744 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.000486    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=94.70 +/- 1.00\n",
      "Episode length: 5.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96     |\n",
      "|    ep_rew_mean      | -0.285   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20914152 |\n",
      "|    clip_fraction        | 0.41       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0589    |\n",
      "|    value_loss           | 0.0259     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.165   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.10 +/- 1.22\n",
      "Episode length: 4.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 95.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076759234 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.80 +/- 1.08\n",
      "Episode length: 4.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7fdb31d336431096d60e295c2b8d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 231      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00999076 |\n",
      "|    clip_fraction        | 0.094      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.559     |\n",
      "|    explained_variance   | 0.0304     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0746     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.00695    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030931528 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0933       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00131      |\n",
      "|    value_loss           | 0.188        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015076953 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.134        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0948       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000416     |\n",
      "|    value_loss           | 0.23         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003602652 |\n",
      "|    clip_fraction        | 0.0351      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000366   |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030285493 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.478       |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0805       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    value_loss           | 0.244        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014291184 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000397    |\n",
      "|    value_loss           | 0.258        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026331116 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.173        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    value_loss           | 0.247        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024501693 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 0.26         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001563733 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.000267    |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068105076 |\n",
      "|    clip_fraction        | 0.0103        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.39         |\n",
      "|    explained_variance   | 0.154         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.124         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | 0.000263      |\n",
      "|    value_loss           | 0.259         |\n",
      "-------------------------------------------\n",
      "execution time: 271.06967210769653; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6317de21ef4244979bb8649e302cf40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.9     |\n",
      "|    ep_true_rew_mean | -11.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017921744 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.398      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0306      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.0842      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -1.57    |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011613523 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.0683     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 21.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017461404 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0425      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 26.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010814726 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.0698      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    ep_true_rew_mean | 41.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015797883 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.226       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 61.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.9        |\n",
      "|    mean_reward          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018712899 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.5      |\n",
      "|    ep_rew_mean      | -0.593   |\n",
      "|    ep_true_rew_mean | 86.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.70 +/- 56.11\n",
      "Episode length: 9.30 +/- 10.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.3        |\n",
      "|    mean_reward          | 60.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02673663 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.35     |\n",
      "|    ep_rew_mean      | -0.469   |\n",
      "|    ep_true_rew_mean | 89.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=85.50 +/- 36.84\n",
      "Episode length: 4.50 +/- 6.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 85.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030033972 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 0.0778      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.80 +/- 0.87\n",
      "Episode length: 3.20 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.73     |\n",
      "|    ep_rew_mean      | -0.254   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023904253 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0727     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 0.033       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.37     |\n",
      "|    ep_rew_mean      | -0.214   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021677792 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0636     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 0.0161      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=97.10 +/- 1.14\n",
      "Episode length: 2.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025142014 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0841     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0583     |\n",
      "|    value_loss           | 0.00953     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029016452 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.00388     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=97.30 +/- 1.27\n",
      "Episode length: 2.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.57     |\n",
      "|    ep_rew_mean      | -0.0852  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.30 +/- 1.19\n",
      "Episode length: 2.70 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.7         |\n",
      "|    mean_reward          | 97.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009784688 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.00232     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=97.10 +/- 1.45\n",
      "Episode length: 2.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75     |\n",
      "|    ep_rew_mean      | -0.0879  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026165415 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.585        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00204     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0161      |\n",
      "|    value_loss           | 0.00172      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.0777  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.40 +/- 1.36\n",
      "Episode length: 2.60 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.6          |\n",
      "|    mean_reward          | 97.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044014966 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.148       |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00712     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0168      |\n",
      "|    value_loss           | 0.00081      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.69     |\n",
      "|    ep_rew_mean      | -0.0813  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.80 +/- 0.87\n",
      "Episode length: 3.20 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004762077 |\n",
      "|    clip_fraction        | 0.0372      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.000794    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0761  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | -25       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1375781 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.161    |\n",
      "|    explained_variance   | 0.826     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00714  |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0152   |\n",
      "|    value_loss           | 0.000497  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79     |\n",
      "|    ep_rew_mean      | -0.197   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021305239 |\n",
      "|    clip_fraction        | 0.454       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.0506      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0404     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.067      |\n",
      "|    value_loss           | 0.0277      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13889582 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.215     |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0677    |\n",
      "|    value_loss           | 0.0104     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0856  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06321987 |\n",
      "|    clip_fraction        | 0.0633     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0981    |\n",
      "|    explained_variance   | 0.561      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00885   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf402539e1a42d8a853cc008722aca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -9.07    |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009408459 |\n",
      "|    clip_fraction        | 0.0948      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.609      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000911   |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    value_loss           | 0.0818      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -3.63    |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012935996 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0256     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00795    |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -5.45    |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076701264 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | 0.00152      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.09         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00851     |\n",
      "|    value_loss           | 0.221        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -3.02    |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010159168 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0106     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 3.4      |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010298206 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0222      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 4.48     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012630116 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0397      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | 12.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014211979 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.07        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0905      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 20       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.8        |\n",
      "|    mean_reward          | -0.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327974 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0708      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 20.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.9         |\n",
      "|    mean_reward          | 36.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099688545 |\n",
      "|    clip_fraction        | 0.0954       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0138      |\n",
      "|    value_loss           | 0.225        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | -1.41    |\n",
      "|    ep_true_rew_mean | 27.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010107583 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 50.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013760694 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | -0.836   |\n",
      "|    ep_true_rew_mean | 68.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.9       |\n",
      "|    mean_reward          | 48.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01730268 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.27      |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0325    |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=11.20 +/- 55.30\n",
      "Episode length: 18.80 +/- 9.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | 11.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | -0.69    |\n",
      "|    ep_true_rew_mean | 79.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015860606 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00482    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.02     |\n",
      "|    ep_rew_mean      | -0.515   |\n",
      "|    ep_true_rew_mean | 89       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=48.00 +/- 59.62\n",
      "Episode length: 12.00 +/- 10.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12         |\n",
      "|    mean_reward          | 48         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01685862 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.99      |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.0698     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.30 +/- 1.27\n",
      "Episode length: 2.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89     |\n",
      "|    ep_rew_mean      | -0.344   |\n",
      "|    ep_true_rew_mean | 93.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 1.72\n",
      "Episode length: 4.20 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019317161 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.807      |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.0353      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.00 +/- 2.14\n",
      "Episode length: 4.00 +/- 2.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.7      |\n",
      "|    ep_rew_mean      | -0.249   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012991973 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 2.32\n",
      "Episode length: 3.80 +/- 2.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.31     |\n",
      "|    ep_rew_mean      | -0.219   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.20 +/- 1.47\n",
      "Episode length: 4.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018771077 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    value_loss           | 0.00877     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.77     |\n",
      "|    ep_rew_mean      | -0.174   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.90 +/- 1.70\n",
      "Episode length: 4.10 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015054191 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.6         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.0054      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.80 +/- 1.60\n",
      "Episode length: 4.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010520277 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.50 +/- 1.80\n",
      "Episode length: 4.50 +/- 1.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.168   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 2.18\n",
      "Episode length: 3.80 +/- 2.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036737476 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00762     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 0.00196      |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4d0ad142bf4f87ac90da32bef77da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006905986 |\n",
      "|    clip_fraction        | 0.0157      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.0507      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.000809    |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 2500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068688486 |\n",
      "|    clip_fraction        | 0.00908       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0793       |\n",
      "|    explained_variance   | 0.0967        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0983        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 0.000275      |\n",
      "|    value_loss           | 0.192         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001987426 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0672     |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0981      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000623   |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030720502 |\n",
      "|    clip_fraction        | 0.00566       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0795       |\n",
      "|    explained_variance   | 0.149         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.105         |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 0.000115      |\n",
      "|    value_loss           | 0.249         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002695729 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0875     |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0926      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.000158    |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018200167 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0597      |\n",
      "|    explained_variance   | 0.149        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 0.251        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 7500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031622133 |\n",
      "|    clip_fraction        | 0.00176       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0512       |\n",
      "|    explained_variance   | 0.148         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.113         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 0.000454      |\n",
      "|    value_loss           | 0.252         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011550696 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0496      |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.129        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00064     |\n",
      "|    value_loss           | 0.263        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003796405 |\n",
      "|    clip_fraction        | 0.00723      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0526      |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000718    |\n",
      "|    value_loss           | 0.26         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035597396 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0409       |\n",
      "|    explained_variance   | 0.165         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.116         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000764     |\n",
      "|    value_loss           | 0.249         |\n",
      "-------------------------------------------\n",
      "execution time: 278.82722306251526; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5134cf4fefa84e288d057800a5373dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -4.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 227      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014452312 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.519      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.0806      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 2.69     |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012640387 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0652      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 16.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.5       |\n",
      "|    mean_reward          | -0.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01114896 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.0195     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0752     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 28.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013685885 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.35    |\n",
      "|    ep_true_rew_mean | 39.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012419637 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0936      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | 48       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | -1.1     |\n",
      "|    ep_true_rew_mean | 56       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023580864 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00401    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=72.30 +/- 48.66\n",
      "Episode length: 7.70 +/- 8.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.7      |\n",
      "|    mean_reward     | 72.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.97     |\n",
      "|    ep_rew_mean      | -0.546   |\n",
      "|    ep_true_rew_mean | 86       |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=84.10 +/- 36.39\n",
      "Episode length: 5.90 +/- 6.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.9        |\n",
      "|    mean_reward          | 84.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02257322 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0566    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=97.20 +/- 0.98\n",
      "Episode length: 2.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.45     |\n",
      "|    ep_rew_mean      | -0.487   |\n",
      "|    ep_true_rew_mean | 86.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=73.20 +/- 49.11\n",
      "Episode length: 6.80 +/- 9.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 73.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031783387 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 0.0955      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=85.00 +/- 36.69\n",
      "Episode length: 5.00 +/- 6.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.05     |\n",
      "|    ep_rew_mean      | -0.214   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.50 +/- 0.92\n",
      "Episode length: 4.50 +/- 0.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03085307 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.774     |\n",
      "|    explained_variance   | 0.148      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0565    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0472    |\n",
      "|    value_loss           | 0.0361     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | -0.153   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035144538 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0727     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.01        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.9      |\n",
      "|    ep_rew_mean      | -0.115   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018808812 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.37       |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.00476     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.0999  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014026284 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.00221     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.0932  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009681094 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.00175     |\n",
      "-----------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a45a40281a84e278e20226afc01a93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -4.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009127831 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.354      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0072     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -3.54    |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008579058 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.00254     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -3.89    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006833158 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.00056     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00449    |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -4.97    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090179425 |\n",
      "|    clip_fraction        | 0.0945       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.0347       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0866       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0123      |\n",
      "|    value_loss           | 0.238        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -0.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099435225 |\n",
      "|    clip_fraction        | 0.0768       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.000424     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.147        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 0.271        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 6.33     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011845909 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0176      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.088       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.263       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 18.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009380625 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | -1.59    |\n",
      "|    ep_true_rew_mean | 21       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013317674 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.0958      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 27.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010547206 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0892      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 44.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=23.60 +/- 59.53\n",
      "Episode length: 16.40 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010664503 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0689      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=11.10 +/- 55.14\n",
      "Episode length: 18.90 +/- 9.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.9     |\n",
      "|    mean_reward     | 11.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | -0.927   |\n",
      "|    ep_true_rew_mean | 63.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | 60.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013783374 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=36.50 +/- 61.50\n",
      "Episode length: 13.50 +/- 11.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.5     |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.915   |\n",
      "|    ep_true_rew_mean | 65.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018015744 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.92     |\n",
      "|    ep_rew_mean      | -0.611   |\n",
      "|    ep_true_rew_mean | 85.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017726213 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000467   |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.087       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=84.60 +/- 36.54\n",
      "Episode length: 5.40 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 84.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.93     |\n",
      "|    ep_rew_mean      | -0.449   |\n",
      "|    ep_true_rew_mean | 90.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.1        |\n",
      "|    mean_reward          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019891128 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0063     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0398     |\n",
      "|    value_loss           | 0.0612      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.57     |\n",
      "|    ep_rew_mean      | -0.387   |\n",
      "|    ep_true_rew_mean | 91.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=47.20 +/- 58.97\n",
      "Episode length: 12.80 +/- 10.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | 47.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014520291 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.83       |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    value_loss           | 0.0373      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=84.30 +/- 36.47\n",
      "Episode length: 5.70 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.7      |\n",
      "|    mean_reward     | 84.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.46     |\n",
      "|    ep_rew_mean      | -0.244   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010675989 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0501     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    value_loss           | 0.0127      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=94.50 +/- 0.92\n",
      "Episode length: 5.50 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | 94.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.35     |\n",
      "|    ep_rew_mean      | -0.224   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014328673 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0484     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 0.00944     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=95.80 +/- 1.66\n",
      "Episode length: 4.20 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.85     |\n",
      "|    ep_rew_mean      | -0.171   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.30 +/- 1.55\n",
      "Episode length: 4.70 +/- 1.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011541102 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0411     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    value_loss           | 0.00592     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.24     |\n",
      "|    ep_rew_mean      | -0.144   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=94.70 +/- 1.10\n",
      "Episode length: 5.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 94.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013460187 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0527     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.10 +/- 1.76\n",
      "Episode length: 3.90 +/- 1.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.20 +/- 1.54\n",
      "Episode length: 4.80 +/- 1.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.8        |\n",
      "|    mean_reward          | 95.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00683246 |\n",
      "|    clip_fraction        | 0.0494     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.191     |\n",
      "|    explained_variance   | 0.718      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0396    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    value_loss           | 0.00225    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21500, episode_reward=96.40 +/- 1.62\n",
      "Episode length: 3.60 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=94.30 +/- 1.49\n",
      "Episode length: 5.70 +/- 1.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.7          |\n",
      "|    mean_reward          | 94.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050006034 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0329      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0158      |\n",
      "|    value_loss           | 0.000958     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.10 +/- 1.51\n",
      "Episode length: 3.90 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.70 +/- 1.79\n",
      "Episode length: 3.30 +/- 1.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026152648 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.023       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 0.0008       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23500, episode_reward=95.60 +/- 1.85\n",
      "Episode length: 4.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.127   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034377244 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.118       |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00194     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    value_loss           | 0.00065      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020501397 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0931      |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00732     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 0.000869     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.28     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=95.30 +/- 1.62\n",
      "Episode length: 4.70 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021707118 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0302     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.000217    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.00 +/- 1.79\n",
      "Episode length: 4.00 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.25     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=95.70 +/- 1.73\n",
      "Episode length: 4.30 +/- 1.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022703446 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.000973    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=95.50 +/- 1.43\n",
      "Episode length: 4.50 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012845703 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0865     |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.000816    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=97.10 +/- 1.45\n",
      "Episode length: 2.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 28672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009268846 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.000314    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=94.70 +/- 1.00\n",
      "Episode length: 5.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 29       |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 29696    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=95.90 +/- 1.92\n",
      "Episode length: 4.10 +/- 1.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018128702 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0902     |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.000643    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22     |\n",
      "|    ep_rew_mean      | -0.127   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 30       |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 30720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063933437 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0738      |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0174      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 0.000127     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=95.00 +/- 1.48\n",
      "Episode length: 5.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.69     |\n",
      "|    ep_rew_mean      | -0.107   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 31       |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 31744    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029898854 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0816      |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00354     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 0.000202     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=95.70 +/- 1.73\n",
      "Episode length: 4.30 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.26     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 32       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 32768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072465083 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0696      |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0219      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00829     |\n",
      "|    value_loss           | 0.001        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=95.80 +/- 1.66\n",
      "Episode length: 4.20 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 33       |\n",
      "|    time_elapsed     | 178      |\n",
      "|    total_timesteps  | 33792    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006888841 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0721      |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0254      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 0.00035      |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccd6eb7ddb4479a89014585ec3610fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | -2.1     |\n",
      "|    ep_true_rew_mean | -19.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073241517 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.581       |\n",
      "|    explained_variance   | 0.0487       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000305     |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | -2.13    |\n",
      "|    ep_true_rew_mean | -20.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028861905 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0855       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00203      |\n",
      "|    value_loss           | 0.185        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -22.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034779306 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.113        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00089     |\n",
      "|    value_loss           | 0.239        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | -2.13    |\n",
      "|    ep_true_rew_mean | -19.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040001385 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.13         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000364    |\n",
      "|    value_loss           | 0.243        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -2.06    |\n",
      "|    ep_true_rew_mean | -13.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040858043 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    value_loss           | 0.253        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -2       |\n",
      "|    ep_true_rew_mean | -10.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005202283 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.9     |\n",
      "|    ep_true_rew_mean | -2.17    |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069044335 |\n",
      "|    clip_fraction        | 0.0751       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.139        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00737     |\n",
      "|    value_loss           | 0.264        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 6.63     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064737886 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.15    |\n",
      "|    ep_true_rew_mean | 55.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03408984 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.727     |\n",
      "|    explained_variance   | 0.0686     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0936     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.277      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | -0.837   |\n",
      "|    ep_true_rew_mean | 79.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028301455 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.54     |\n",
      "|    ep_rew_mean      | -0.426   |\n",
      "|    ep_true_rew_mean | 91.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.2       |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02492791 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.604     |\n",
      "|    explained_variance   | 0.134      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.0808     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.86     |\n",
      "|    ep_rew_mean      | -0.276   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.50 +/- 1.63\n",
      "Episode length: 4.50 +/- 1.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015686356 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0559     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 0.0261      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=95.00 +/- 1.18\n",
      "Episode length: 5.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.21     |\n",
      "|    ep_rew_mean      | -0.213   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=94.70 +/- 0.78\n",
      "Episode length: 5.30 +/- 0.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 94.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048331086 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0759     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    value_loss           | 0.0099      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.40 +/- 1.56\n",
      "Episode length: 4.60 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.77     |\n",
      "|    ep_rew_mean      | -0.262   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06333992 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.347     |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0572    |\n",
      "|    value_loss           | 0.0205     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.194   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07592918 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.192     |\n",
      "|    explained_variance   | 0.485      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 0.0076     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030338157 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.00 +/- 1.26\n",
      "Episode length: 5.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.50 +/- 1.80\n",
      "Episode length: 4.50 +/- 1.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07975857 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.073     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | 0.395      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.00 +/- 0.89\n",
      "Episode length: 5.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61     |\n",
      "|    ep_rew_mean      | -0.174   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078390405 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0984     |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.00758     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.00 +/- 1.48\n",
      "Episode length: 5.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 95          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001986959 |\n",
      "|    clip_fraction        | 0.0179      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0812     |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    value_loss           | 0.000409    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.50 +/- 1.63\n",
      "Episode length: 4.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.00 +/- 1.48\n",
      "Episode length: 5.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 95          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010610897 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0679     |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=95.50 +/- 1.69\n",
      "Episode length: 4.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08584492 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.142     |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.17       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.353      |\n",
      "|    value_loss           | 0.00023    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.20 +/- 1.78\n",
      "Episode length: 4.80 +/- 1.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17     |\n",
      "|    ep_rew_mean      | -0.212   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16303587 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0771    |\n",
      "|    explained_variance   | 0.273      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0293    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 0.0122     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.28     |\n",
      "|    ep_rew_mean      | -0.144   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015834384 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0522     |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "execution time: 379.10049700737; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4265c159d2434f1abdf74f3c581b6f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010130076 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.747      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00607     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    value_loss           | 0.0856      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -5.02    |\n",
      "| time/               |          |\n",
      "|    fps              | 207      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008131888 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0234     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | 1.41     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01350999 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.0351     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.072      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 13.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010856721 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0349      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 24.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012858214 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.081       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0981      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.09    |\n",
      "|    ep_true_rew_mean | 59.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018586198 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.743   |\n",
      "|    ep_true_rew_mean | 80.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.8       |\n",
      "|    mean_reward          | 48.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02161285 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.34       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0523    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=72.80 +/- 48.90\n",
      "Episode length: 7.20 +/- 8.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | -0.633   |\n",
      "|    ep_true_rew_mean | 84.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.30 +/- 55.84\n",
      "Episode length: 9.70 +/- 10.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.7         |\n",
      "|    mean_reward          | 60.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028139966 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000729    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.32     |\n",
      "|    ep_rew_mean      | -0.381   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039713383 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0578     |\n",
      "|    value_loss           | 0.0687      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.41     |\n",
      "|    ep_rew_mean      | -0.226   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024793232 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.714      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0768     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56     |\n",
      "|    ep_rew_mean      | -0.154   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014869072 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.063      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.00825     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.117   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03165037 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.374     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.00423    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64     |\n",
      "|    ep_rew_mean      | -0.0844  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017371617 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.236      |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.0931  |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.50 +/- 1.57\n",
      "Episode length: 3.50 +/- 1.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004836674 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.51        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 0.78\n",
      "Episode length: 3.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.67     |\n",
      "|    ep_rew_mean      | -0.071   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064919507 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.694        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00128     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.000954     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.30 +/- 0.78\n",
      "Episode length: 2.70 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0713  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035668053 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0919      |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00702     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.000705     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | -0.0783  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035443145 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0777      |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0241      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.00116      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.063   |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013610972 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.074       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000736     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0079      |\n",
      "|    value_loss           | 0.000525     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.45     |\n",
      "|    ep_rew_mean      | -0.068   |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059338636 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0564      |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00138     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00813     |\n",
      "|    value_loss           | 0.000143     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0762  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011782842 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0491      |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00742     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00858     |\n",
      "|    value_loss           | 0.000319     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97abd0d74f5f46a4b9784e2a02570d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -6.89    |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013075359 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.473      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0216      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    value_loss           | 0.0783      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -5.83    |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013512569 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.00636     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0305      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -8.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008879868 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | -0.00182    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.094       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 3.47     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011469525 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0272      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0856      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | 19.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008219581 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0812      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 28.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011132018 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0766      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 39.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012584999 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    ep_true_rew_mean | 45.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012634614 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 54.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.20 +/- 55.79\n",
      "Episode length: 9.80 +/- 10.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015656888 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.787   |\n",
      "|    ep_true_rew_mean | 73.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=59.70 +/- 55.45\n",
      "Episode length: 10.30 +/- 9.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.3        |\n",
      "|    mean_reward          | 59.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019157115 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00997    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.8      |\n",
      "|    ep_rew_mean      | -0.63    |\n",
      "|    ep_true_rew_mean | 80.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=83.60 +/- 36.25\n",
      "Episode length: 6.40 +/- 6.47\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 6.4       |\n",
      "|    mean_reward          | 83.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 11500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0207448 |\n",
      "|    clip_fraction        | 0.29      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.13     |\n",
      "|    explained_variance   | 0.432     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.022    |\n",
      "|    n_updates            | 110       |\n",
      "|    policy_gradient_loss | -0.0418   |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | 59.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.57     |\n",
      "|    ep_rew_mean      | -0.417   |\n",
      "|    ep_true_rew_mean | 92.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=71.90 +/- 48.46\n",
      "Episode length: 8.10 +/- 8.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.1        |\n",
      "|    mean_reward          | 71.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03765488 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.943     |\n",
      "|    explained_variance   | 0.427      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0202    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 0.0517     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=72.60 +/- 48.82\n",
      "Episode length: 7.40 +/- 8.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 72.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.53     |\n",
      "|    ep_rew_mean      | -0.413   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.00 +/- 1.55\n",
      "Episode length: 4.00 +/- 1.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023941338 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.768      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 0.0369      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.05     |\n",
      "|    ep_rew_mean      | -0.286   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.40 +/- 1.20\n",
      "Episode length: 4.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017181363 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0408     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.20 +/- 1.60\n",
      "Episode length: 3.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02     |\n",
      "|    ep_rew_mean      | -0.207   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.90 +/- 1.97\n",
      "Episode length: 4.10 +/- 1.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023539485 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 0.0109      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=94.90 +/- 0.70\n",
      "Episode length: 5.10 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 94.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.174   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01576899 |\n",
      "|    clip_fraction        | 0.0948     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.27      |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0432    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    value_loss           | 0.00489    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.00 +/- 2.00\n",
      "Episode length: 4.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.1        |\n",
      "|    mean_reward          | 95.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01001313 |\n",
      "|    clip_fraction        | 0.0326     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.80 +/- 1.72\n",
      "Episode length: 3.20 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15     |\n",
      "|    ep_rew_mean      | -0.127   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.60 +/- 1.56\n",
      "Episode length: 4.40 +/- 1.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016043427 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0257      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 0.00165      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.90 +/- 1.58\n",
      "Episode length: 4.10 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.00 +/- 1.41\n",
      "Episode length: 5.00 +/- 1.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 95           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013781317 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0179      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 0.000694     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.50 +/- 1.63\n",
      "Episode length: 4.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04335262 |\n",
      "|    clip_fraction        | 0.03       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    value_loss           | 0.000972   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.60 +/- 1.85\n",
      "Episode length: 3.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4      |\n",
      "|    ep_rew_mean      | -0.151   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=94.90 +/- 1.64\n",
      "Episode length: 5.10 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.1         |\n",
      "|    mean_reward          | 94.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006391144 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0999     |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.000665    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.50 +/- 1.91\n",
      "Episode length: 4.50 +/- 1.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003078856 |\n",
      "|    clip_fraction        | 0.0236      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0824     |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0026     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    value_loss           | 0.000596    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.70 +/- 1.68\n",
      "Episode length: 3.30 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee468f22db2748c1ad3ac2eda38fd27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -22.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027183514 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.794      |\n",
      "|    explained_variance   | -0.00701    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 34.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020252522 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 0.0293      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.26        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | -0.783   |\n",
      "|    ep_true_rew_mean | 77.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02190891 |\n",
      "|    clip_fraction        | 0.404      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.76      |\n",
      "|    explained_variance   | 0.0296     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0645     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.45     |\n",
      "|    ep_rew_mean      | -0.517   |\n",
      "|    ep_true_rew_mean | 91.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021456897 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.0428      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.13     |\n",
      "|    ep_rew_mean      | -0.3     |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055490118 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0497     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 0.0408      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=97.10 +/- 1.51\n",
      "Episode length: 2.90 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | -0.185   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=94.80 +/- 1.33\n",
      "Episode length: 5.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 94.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063169636 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    value_loss           | 0.00863     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=95.30 +/- 2.05\n",
      "Episode length: 4.70 +/- 2.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07254554 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.195     |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.123      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.40 +/- 1.50\n",
      "Episode length: 4.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.75     |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.70 +/- 1.35\n",
      "Episode length: 4.30 +/- 1.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06519283 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0184    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    value_loss           | 0.0107     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=94.70 +/- 0.90\n",
      "Episode length: 5.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.40 +/- 1.43\n",
      "Episode length: 4.60 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022568185 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0684     |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    value_loss           | 0.00214     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 1.80\n",
      "Episode length: 3.50 +/- 1.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13051079 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0336    |\n",
      "|    value_loss           | 0.000104   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.27     |\n",
      "|    ep_rew_mean      | -0.609   |\n",
      "|    ep_true_rew_mean | 86.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023553811 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.00421     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "execution time: 292.02061700820923; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085c33201286496cb5f589aca685f133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014109338 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.282      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00981     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -4.81    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008706082 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0149      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00987    |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -4.92    |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010633867 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0459      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0987      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -1.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01384576 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | 0.0529     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.129      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    value_loss           | 0.255      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | 22.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.9        |\n",
      "|    mean_reward          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012399141 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0368      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 28.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14           |\n",
      "|    mean_reward          | 36           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119798565 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0974       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0156      |\n",
      "|    value_loss           | 0.205        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.3     |\n",
      "|    mean_reward     | 48.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17       |\n",
      "|    ep_rew_mean      | -1.27    |\n",
      "|    ep_true_rew_mean | 41       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=73.10 +/- 49.06\n",
      "Episode length: 6.90 +/- 9.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.9         |\n",
      "|    mean_reward          | 73.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014207179 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.6     |\n",
      "|    ep_rew_mean      | -0.885   |\n",
      "|    ep_true_rew_mean | 60.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=36.00 +/- 61.01\n",
      "Episode length: 14.00 +/- 11.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018358674 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.7     |\n",
      "|    ep_rew_mean      | -0.841   |\n",
      "|    ep_true_rew_mean | 62.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.81\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 14.2      |\n",
      "|    mean_reward          | 35.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0195401 |\n",
      "|    clip_fraction        | 0.318     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.3      |\n",
      "|    explained_variance   | 0.386     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0617    |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0392   |\n",
      "|    value_loss           | 0.142     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=60.30 +/- 55.84\n",
      "Episode length: 9.70 +/- 10.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.95     |\n",
      "|    ep_rew_mean      | -0.426   |\n",
      "|    ep_true_rew_mean | 89       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025889453 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 0.0783      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.40 +/- 1.80\n",
      "Episode length: 3.60 +/- 1.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.44     |\n",
      "|    ep_rew_mean      | -0.234   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026265282 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0547     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 0.0294      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.24     |\n",
      "|    ep_rew_mean      | -0.198   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=97.20 +/- 1.66\n",
      "Episode length: 2.80 +/- 1.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022216788 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0481     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.20 +/- 1.25\n",
      "Episode length: 2.80 +/- 1.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | 97.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02374934 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.462     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.00728    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021066584 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.054      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.65     |\n",
      "|    ep_rew_mean      | -0.0841  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 1.57\n",
      "Episode length: 3.50 +/- 1.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010009498 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.00176     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64     |\n",
      "|    ep_rew_mean      | -0.0782  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039037908 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00171      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 0.00104      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0715  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.61\n",
      "Episode length: 3.00 +/- 1.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3         |\n",
      "|    mean_reward          | 97        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0017281 |\n",
      "|    clip_fraction        | 0.0126    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.823     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.000175  |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0104   |\n",
      "|    value_loss           | 0.00052   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bd742d3b584288ba8cbb4665e8124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -6.89    |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010757564 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.224      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000611    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -0.33    |\n",
      "| time/               |          |\n",
      "|    fps              | 200      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011188963 |\n",
      "|    clip_fraction        | 0.0861      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0316      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0352      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=61.10 +/- 56.37\n",
      "Episode length: 8.90 +/- 10.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.9      |\n",
      "|    mean_reward     | 61.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -1.15    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012068441 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 3.2      |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011317897 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0776      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 11.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.3         |\n",
      "|    mean_reward          | 11.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107979085 |\n",
      "|    clip_fraction        | 0.0888       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.226        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.53    |\n",
      "|    ep_true_rew_mean | 20.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14.4         |\n",
      "|    mean_reward          | 35.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114068175 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.108        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 0.213        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=47.80 +/- 59.46\n",
      "Episode length: 12.20 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 25.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.3        |\n",
      "|    mean_reward          | 48.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010216412 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=23.80 +/- 59.78\n",
      "Episode length: 16.20 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    ep_true_rew_mean | 27.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.00 +/- 59.62\n",
      "Episode length: 12.00 +/- 10.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007403303 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0673      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00735    |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=47.60 +/- 59.29\n",
      "Episode length: 12.40 +/- 10.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 28.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.00 +/- 55.66\n",
      "Episode length: 10.00 +/- 9.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10           |\n",
      "|    mean_reward          | 60           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070835114 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.357        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0745       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00913     |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.986   |\n",
      "|    ep_true_rew_mean | 49.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=35.50 +/- 60.51\n",
      "Episode length: 14.50 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.5        |\n",
      "|    mean_reward          | 35.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011827018 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb0eb219e47432a8f903c076d865734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -2.12    |\n",
      "|    ep_true_rew_mean | -19.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 230      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011485032 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | -0.0059     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000439   |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | -2.1     |\n",
      "|    ep_true_rew_mean | -19.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002072851 |\n",
      "|    clip_fraction        | 0.0291      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.0813      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -2.12    |\n",
      "|    ep_true_rew_mean | -20.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025060414 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000785    |\n",
      "|    value_loss           | 0.234        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003290331 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.0989      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.274       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -2.09    |\n",
      "|    ep_true_rew_mean | -17.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029235845 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.137        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 0.255        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -8.47    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023032457 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 0.272        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -1.78    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068950737 |\n",
      "|    clip_fraction        | 0.0654       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00799     |\n",
      "|    value_loss           | 0.283        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | 12.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.7       |\n",
      "|    mean_reward          | -12.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12680405 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.63      |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.124      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.28       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.13     |\n",
      "|    ep_rew_mean      | -0.578   |\n",
      "|    ep_true_rew_mean | 89.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040104423 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00384     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.77     |\n",
      "|    ep_rew_mean      | -0.435   |\n",
      "|    ep_true_rew_mean | 92.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02137162 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.557     |\n",
      "|    explained_variance   | 0.301      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.0694     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.9     |\n",
      "|    mean_reward     | 12.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96     |\n",
      "|    ep_rew_mean      | -0.269   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.1        |\n",
      "|    mean_reward          | 59.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024522457 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=72.30 +/- 48.68\n",
      "Episode length: 7.70 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.7      |\n",
      "|    mean_reward     | 72.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019392004 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0425     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.0109      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=95.50 +/- 1.69\n",
      "Episode length: 4.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.198   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.30 +/- 1.42\n",
      "Episode length: 4.70 +/- 1.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03476543 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.587      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.00552    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05644675 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.15      |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0301    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    value_loss           | 0.00284    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.00 +/- 1.18\n",
      "Episode length: 5.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.145   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.70 +/- 1.49\n",
      "Episode length: 4.30 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025330946 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0828     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.0292      |\n",
      "|    value_loss           | 0.000527    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.90 +/- 1.45\n",
      "Episode length: 4.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.18     |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=94.70 +/- 1.35\n",
      "Episode length: 5.30 +/- 1.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.3        |\n",
      "|    mean_reward          | 94.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08464497 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | 0.174      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.0194     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.90 +/- 1.58\n",
      "Episode length: 4.10 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.24     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.50 +/- 1.28\n",
      "Episode length: 4.50 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017092038 |\n",
      "|    clip_fraction        | 0.0547      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.60 +/- 1.36\n",
      "Episode length: 4.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048499345 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0891      |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0208      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.00118      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.50 +/- 2.01\n",
      "Episode length: 3.50 +/- 2.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.68     |\n",
      "|    ep_rew_mean      | -0.177   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.10 +/- 1.76\n",
      "Episode length: 3.90 +/- 1.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04957441 |\n",
      "|    clip_fraction        | 0.0448     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0715    |\n",
      "|    explained_variance   | 0.572      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 0.00362    |\n",
      "----------------------------------------\n",
      "execution time: 255.18912410736084; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786c54416cc648f0a0ec0bfc3c867bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -4.91    |\n",
      "| time/               |          |\n",
      "|    fps              | 227      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013683755 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.241      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -2.66    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008167307 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.00577    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0909      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    ep_true_rew_mean | -5.09    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010791214 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0152      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0903      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.225       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -6.04    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011207361 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.00628    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 11.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.30 +/- 61.31\n",
      "Episode length: 13.70 +/- 11.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013163823 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | -0.00971    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.39    |\n",
      "|    ep_true_rew_mean | 32.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.9        |\n",
      "|    mean_reward          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014097722 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0871      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.4     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 51.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015660202 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | -0.961   |\n",
      "|    ep_true_rew_mean | 67.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.9        |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019974649 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0349      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.85     |\n",
      "|    ep_rew_mean      | -0.513   |\n",
      "|    ep_true_rew_mean | 84.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.9         |\n",
      "|    mean_reward          | 72.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023237733 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0379     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=84.20 +/- 36.43\n",
      "Episode length: 5.80 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 84.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.25     |\n",
      "|    ep_rew_mean      | -0.318   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023935413 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0565     |\n",
      "|    value_loss           | 0.0384      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.1      |\n",
      "|    ep_rew_mean      | -0.202   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016006216 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0564     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0604     |\n",
      "|    value_loss           | 0.014       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.46     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01837176 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.616     |\n",
      "|    explained_variance   | 0.424      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0727    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0639    |\n",
      "|    value_loss           | 0.00837    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=97.40 +/- 1.02\n",
      "Episode length: 2.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | 97.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19     |\n",
      "|    ep_rew_mean      | -0.126   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023255592 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0666     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0969  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040042356 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.10 +/- 1.04\n",
      "Episode length: 2.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | -0.0705  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.60 +/- 0.80\n",
      "Episode length: 3.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003996851 |\n",
      "|    clip_fraction        | 0.0224      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.00 +/- 1.41\n",
      "Episode length: 3.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.69     |\n",
      "|    ep_rew_mean      | -0.0873  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.50 +/- 1.63\n",
      "Episode length: 4.50 +/- 1.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00309972 |\n",
      "|    clip_fraction        | 0.0183     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.122     |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00137   |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    value_loss           | 0.000678   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0794  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.30 +/- 1.55\n",
      "Episode length: 3.70 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008065385 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0924      |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000623    |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 0.000627     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.36     |\n",
      "|    ep_rew_mean      | -0.066   |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013542126 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0716      |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0221      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 0.000445     |\n",
      "------------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b05e01f7654433b68bd56b347f18ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -0.0652  |\n",
      "| time/               |          |\n",
      "|    fps              | 219      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013848841 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.13       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -1.46    |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.7        |\n",
      "|    mean_reward          | 24.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010862732 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0567      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 1.55     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01088875 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | 0.0413     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.128      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 10.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013142414 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0574      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 17.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 17.9      |\n",
      "|    mean_reward          | 12.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0133979 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.51     |\n",
      "|    explained_variance   | 0.0895    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0718    |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=48.90 +/- 60.34\n",
      "Episode length: 11.10 +/- 11.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.1     |\n",
      "|    mean_reward     | 48.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 18.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009029856 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0739      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.39    |\n",
      "|    ep_true_rew_mean | 30.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012510729 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 46.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.5        |\n",
      "|    mean_reward          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014328881 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0442      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=59.90 +/- 55.60\n",
      "Episode length: 10.10 +/- 9.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.938   |\n",
      "|    ep_true_rew_mean | 59.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=35.50 +/- 60.51\n",
      "Episode length: 14.50 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.5        |\n",
      "|    mean_reward          | 35.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011621201 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.70 +/- 48.85\n",
      "Episode length: 7.30 +/- 8.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | -0.744   |\n",
      "|    ep_true_rew_mean | 73.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013718199 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0286      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=47.80 +/- 59.45\n",
      "Episode length: 12.20 +/- 10.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.44     |\n",
      "|    ep_rew_mean      | -0.49    |\n",
      "|    ep_true_rew_mean | 91.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=60.00 +/- 55.66\n",
      "Episode length: 10.00 +/- 9.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030597016 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0861     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.0811      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.14     |\n",
      "|    ep_rew_mean      | -0.376   |\n",
      "|    ep_true_rew_mean | 92.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027937327 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0584     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.0392      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=95.50 +/- 1.75\n",
      "Episode length: 4.50 +/- 1.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.12     |\n",
      "|    ep_rew_mean      | -0.28    |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01424098 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.628     |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0754    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.015      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.40 +/- 1.85\n",
      "Episode length: 4.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.188   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013823863 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 0.00658     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.91\n",
      "Episode length: 3.60 +/- 1.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88     |\n",
      "|    ep_rew_mean      | -0.187   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.70 +/- 2.00\n",
      "Episode length: 4.30 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01406865 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.34      |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0472    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 0.00463    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.50 +/- 1.43\n",
      "Episode length: 4.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.82     |\n",
      "|    ep_rew_mean      | -0.173   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060215276 |\n",
      "|    clip_fraction        | 0.0546       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.713        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0205      |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.20 +/- 1.72\n",
      "Episode length: 4.80 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 1.80\n",
      "Episode length: 3.60 +/- 1.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011541899 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.80 +/- 1.72\n",
      "Episode length: 4.20 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032118992 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.018       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    value_loss           | 0.00178      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.50 +/- 2.01\n",
      "Episode length: 4.50 +/- 2.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.30 +/- 1.42\n",
      "Episode length: 4.70 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.7          |\n",
      "|    mean_reward          | 95.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021148305 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0966      |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0215      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0141      |\n",
      "|    value_loss           | 0.00147      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04922978 |\n",
      "|    clip_fraction        | 0.0347     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.927      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00878   |\n",
      "|    value_loss           | 0.000413   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21000, episode_reward=94.70 +/- 1.10\n",
      "Episode length: 5.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.1      |\n",
      "|    ep_rew_mean      | -0.317   |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.00 +/- 1.55\n",
      "Episode length: 5.00 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 95         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14891702 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.164     |\n",
      "|    explained_variance   | 0.122      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0548    |\n",
      "|    value_loss           | 0.0328     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.156   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.20 +/- 0.87\n",
      "Episode length: 4.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074491486 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0756     |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.00678     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.50 +/- 1.86\n",
      "Episode length: 3.50 +/- 1.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102346195 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0742      |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00605     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.000717     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.40 +/- 1.96\n",
      "Episode length: 3.60 +/- 1.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.24     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021535223 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00641    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.000327    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=95.00 +/- 1.41\n",
      "Episode length: 5.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.86     |\n",
      "|    ep_rew_mean      | -0.201   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12816009 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.132     |\n",
      "|    explained_variance   | 0.413      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0613    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    value_loss           | 0.0137     |\n",
      "----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c847d15db6f54c6e8ad8ce206ede2150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 212      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015800148 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.0187       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000464    |\n",
      "|    value_loss           | 0.0989       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024747895 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0882      |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 0.176        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004597202 |\n",
      "|    clip_fraction        | 0.00557      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0893       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000145     |\n",
      "|    value_loss           | 0.231        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 4500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031629746 |\n",
      "|    clip_fraction        | 0.00381       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0984       |\n",
      "|    explained_variance   | 0.16          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0986        |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 0.000192      |\n",
      "|    value_loss           | 0.243         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.14    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024084253 |\n",
      "|    clip_fraction        | 0.00898      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.151        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 0.239        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | -2.11    |\n",
      "|    ep_true_rew_mean | -21.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002263439 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000948   |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | -2.1     |\n",
      "|    ep_true_rew_mean | -20.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000738536 |\n",
      "|    clip_fraction        | 0.0115      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0976      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00066    |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | -2.01    |\n",
      "|    ep_true_rew_mean | -14.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18           |\n",
      "|    mean_reward          | 12           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043889754 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.127        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    value_loss           | 0.269        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.96    |\n",
      "|    ep_true_rew_mean | -10.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042702213 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00506    |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 10.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.7       |\n",
      "|    mean_reward          | -12.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09498895 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0947     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00933   |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.29     |\n",
      "|    ep_rew_mean      | -0.407   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037259974 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00817     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.58     |\n",
      "|    ep_rew_mean      | -0.264   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.70 +/- 1.79\n",
      "Episode length: 4.30 +/- 1.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04827734 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.452     |\n",
      "|    explained_variance   | 0.02       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.083     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.0253     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.90 +/- 1.51\n",
      "Episode length: 3.10 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.75     |\n",
      "|    ep_rew_mean      | -0.296   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.20 +/- 1.40\n",
      "Episode length: 4.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027021602 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.435      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0775     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 0.0319      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17     |\n",
      "|    ep_rew_mean      | -0.209   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.90 +/- 2.12\n",
      "Episode length: 4.10 +/- 2.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053917795 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0508     |\n",
      "|    value_loss           | 0.00987     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=94.80 +/- 0.98\n",
      "Episode length: 5.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 94.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.68     |\n",
      "|    ep_rew_mean      | -0.169   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 2.05\n",
      "Episode length: 3.30 +/- 2.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059399784 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.055      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.30 +/- 1.10\n",
      "Episode length: 4.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.73     |\n",
      "|    ep_rew_mean      | -0.269   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.70 +/- 1.73\n",
      "Episode length: 3.30 +/- 1.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086221315 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 0.0297      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.10 +/- 1.30\n",
      "Episode length: 4.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | -0.159   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07389103 |\n",
      "|    clip_fraction        | 0.0517     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.107     |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0121    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.00395    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.81     |\n",
      "|    ep_rew_mean      | -0.17    |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.80 +/- 1.60\n",
      "Episode length: 4.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068849707 |\n",
      "|    clip_fraction        | 0.083        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0818      |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0421      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0235      |\n",
      "|    value_loss           | 0.00105      |\n",
      "------------------------------------------\n",
      "execution time: 348.45393896102905; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2baa7f53e2a457d8e327cea64c968a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | -0.458   |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013772089 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.039       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00981    |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -0.379   |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008607441 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0155      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0899      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 1.47     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013763395 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0115     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.254       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 1.32     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009988336 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0399      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.257       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=23.90 +/- 59.90\n",
      "Episode length: 16.10 +/- 10.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 7.89     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.40 +/- 59.28\n",
      "Episode length: 16.60 +/- 10.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.6        |\n",
      "|    mean_reward          | 23.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012974402 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.54    |\n",
      "|    ep_true_rew_mean | 24.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.5       |\n",
      "|    mean_reward          | 11.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01193172 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.0686     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0907     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 0.244      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 50.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017197218 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=60.60 +/- 56.05\n",
      "Episode length: 9.40 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | -0.954   |\n",
      "|    ep_true_rew_mean | 64.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12         |\n",
      "|    mean_reward          | 48         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01572462 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.33      |\n",
      "|    explained_variance   | 0.298      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0422     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.97     |\n",
      "|    ep_rew_mean      | -0.532   |\n",
      "|    ep_true_rew_mean | 86       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021869333 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | 48       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.62     |\n",
      "|    ep_rew_mean      | -0.334   |\n",
      "|    ep_true_rew_mean | 92.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 72.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028444704 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0478     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 0.0602      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.02     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01563992 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.746     |\n",
      "|    explained_variance   | -0.245     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 0.0135     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.166   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023750046 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0624     |\n",
      "|    value_loss           | 0.0093      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.51     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020459862 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0728     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    value_loss           | 0.00533     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.092   |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019997073 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    value_loss           | 0.00212     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | -0.0724  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005247406 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.30 +/- 1.35\n",
      "Episode length: 2.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | -0.0816  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004547261 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00975    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | -0.0659  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 3.7           |\n",
      "|    mean_reward          | 96.3          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 17500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094103004 |\n",
      "|    clip_fraction        | 0.0165        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.128        |\n",
      "|    explained_variance   | 0.928         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0109       |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00919      |\n",
      "|    value_loss           | 0.000179      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.3      |\n",
      "|    ep_rew_mean      | -0.0617  |\n",
      "|    ep_true_rew_mean | 96.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.9          |\n",
      "|    mean_reward          | 97.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044582584 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0242      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.000814     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.077   |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004219542 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0936     |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.000721    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.59     |\n",
      "|    ep_rew_mean      | -0.0689  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3            |\n",
      "|    mean_reward          | 97           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009937957 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0851      |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00892     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00355     |\n",
      "|    value_loss           | 0.000151     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.00 +/- 1.67\n",
      "Episode length: 4.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7838ea8b9aec421bbfa8d54a9278a7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -2       |\n",
      "|    ep_true_rew_mean | -9.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 217      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012584316 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.238      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.95    |\n",
      "|    ep_true_rew_mean | -9.66    |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011490831 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0475      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0683      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00742    |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | 0.17     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012047934 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.035       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 7.21     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015120674 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.096       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 21.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013910509 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0857      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 28.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008806054 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 40.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012909216 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.11    |\n",
      "|    ep_true_rew_mean | 48.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008411523 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 56.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015328126 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.7     |\n",
      "|    ep_rew_mean      | -0.87    |\n",
      "|    ep_true_rew_mean | 65.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=35.70 +/- 60.71\n",
      "Episode length: 14.30 +/- 10.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015217089 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=60.50 +/- 55.99\n",
      "Episode length: 9.50 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.61     |\n",
      "|    ep_rew_mean      | -0.609   |\n",
      "|    ep_true_rew_mean | 83.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014748615 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00925    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.78     |\n",
      "|    ep_rew_mean      | -0.504   |\n",
      "|    ep_true_rew_mean | 86.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017512348 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.071       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.95     |\n",
      "|    ep_rew_mean      | -0.435   |\n",
      "|    ep_true_rew_mean | 88       |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.30 +/- 1.68\n",
      "Episode length: 4.70 +/- 1.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01893245 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.852     |\n",
      "|    explained_variance   | 0.586      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.0571     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.60 +/- 2.11\n",
      "Episode length: 4.40 +/- 2.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.01     |\n",
      "|    ep_rew_mean      | -0.269   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009921396 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.97     |\n",
      "|    ep_rew_mean      | -0.201   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 1.66\n",
      "Episode length: 4.20 +/- 1.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00902684 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.515     |\n",
      "|    explained_variance   | 0.507      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    value_loss           | 0.00878    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.50 +/- 2.29\n",
      "Episode length: 4.50 +/- 2.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014725042 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=95.40 +/- 1.36\n",
      "Episode length: 4.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | -0.157   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016142491 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0525     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.20 +/- 1.99\n",
      "Episode length: 3.80 +/- 1.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.20 +/- 1.40\n",
      "Episode length: 4.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010575144 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.60 +/- 1.50\n",
      "Episode length: 4.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006479129 |\n",
      "|    clip_fraction        | 0.0607      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.158      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.30 +/- 1.42\n",
      "Episode length: 4.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.60 +/- 1.74\n",
      "Episode length: 4.40 +/- 1.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027039838 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0105      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0123      |\n",
      "|    value_loss           | 0.000749     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.60 +/- 1.85\n",
      "Episode length: 4.40 +/- 1.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032855242 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0236      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0136      |\n",
      "|    value_loss           | 0.000653     |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc9422110e410c994a164630bd6971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023715396 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | -0.0251      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0341       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.101        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014757186 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0885      |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0836       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000933    |\n",
      "|    value_loss           | 0.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.16    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 3500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032044417 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0834       |\n",
      "|    explained_variance   | 0.117         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.117         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000818     |\n",
      "|    value_loss           | 0.236         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007401628 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.083       |\n",
      "|    explained_variance   | 0.151        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.116        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000414    |\n",
      "|    value_loss           | 0.234        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014205959 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.078       |\n",
      "|    explained_variance   | 0.126        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.137        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    value_loss           | 0.245        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010915711 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0732      |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.116        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000589    |\n",
      "|    value_loss           | 0.243        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 7500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024797092 |\n",
      "|    clip_fraction        | 0.00391       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0691       |\n",
      "|    explained_variance   | 0.159         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.135         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000557     |\n",
      "|    value_loss           | 0.234         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 8500          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044954295 |\n",
      "|    clip_fraction        | 0.00869       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0673       |\n",
      "|    explained_variance   | 0.155         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0814        |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000234     |\n",
      "|    value_loss           | 0.251         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.18    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008858326 |\n",
      "|    clip_fraction        | 0.0082       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0689      |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.112        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 0.248        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.16    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 25            |\n",
      "|    mean_reward          | -25           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069898774 |\n",
      "|    clip_fraction        | 0.00586       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0737       |\n",
      "|    explained_variance   | 0.176         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0893        |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000376     |\n",
      "|    value_loss           | 0.232         |\n",
      "-------------------------------------------\n",
      "execution time: 304.63741421699524; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01294d9422e04be6a868ac4e91c8aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -8.91    |\n",
      "| time/               |          |\n",
      "|    fps              | 215      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014162332 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.132      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -6.98    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013359204 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.00302     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0901      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 0.2      |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011367952 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0165      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.097       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.277       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 4.51     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00941219 |\n",
      "|    clip_fraction        | 0.0555     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | 0.00821    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 0.242      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 13       |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016137425 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0659      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 29.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.4       |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01242817 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.0859     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=24.30 +/- 60.39\n",
      "Episode length: 15.70 +/- 11.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | -1.11    |\n",
      "|    ep_true_rew_mean | 53.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018665742 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=47.90 +/- 59.52\n",
      "Episode length: 12.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.936   |\n",
      "|    ep_true_rew_mean | 67.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.50 +/- 60.02\n",
      "Episode length: 11.50 +/- 11.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.5       |\n",
      "|    mean_reward          | 48.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01426027 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.33      |\n",
      "|    explained_variance   | 0.314      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0216     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=48.50 +/- 60.02\n",
      "Episode length: 11.50 +/- 11.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.5     |\n",
      "|    mean_reward     | 48.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.91     |\n",
      "|    ep_rew_mean      | -0.539   |\n",
      "|    ep_true_rew_mean | 82.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | 59.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019123819 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00228     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0538     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=71.80 +/- 48.42\n",
      "Episode length: 8.20 +/- 8.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 71.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.82     |\n",
      "|    ep_rew_mean      | -0.34    |\n",
      "|    ep_true_rew_mean | 91.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022055069 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.0594      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.70 +/- 0.78\n",
      "Episode length: 3.30 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.7      |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021916568 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.785      |\n",
      "|    explained_variance   | 0.0993      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00483    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.0186      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.60 +/- 1.69\n",
      "Episode length: 3.40 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025426716 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0554     |\n",
      "|    value_loss           | 0.00821     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2      |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027077463 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0492     |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.0929  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=97.20 +/- 0.98\n",
      "Episode length: 2.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014468534 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.0774  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032068181 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.185       |\n",
      "|    explained_variance   | 0.612        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0192      |\n",
      "|    value_loss           | 0.00153      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.55\n",
      "Episode length: 3.70 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64     |\n",
      "|    ep_rew_mean      | -0.0703  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.70 +/- 1.10\n",
      "Episode length: 3.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003305704 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.000949    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.3      |\n",
      "|    ep_rew_mean      | -0.06    |\n",
      "|    ep_true_rew_mean | 96.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014019704 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00568      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00745     |\n",
      "|    value_loss           | 0.000303     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0893  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023829103 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00103      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.000653     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0783  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024781679 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.096       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00741     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.01        |\n",
      "|    value_loss           | 0.000599     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20528ab25d74393ad4b979fd33a2bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -7.14    |\n",
      "| time/               |          |\n",
      "|    fps              | 222      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01049905 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -0.0768    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0346     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -1.44    |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.5         |\n",
      "|    mean_reward          | -0.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096573485 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | -0.0578      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0363       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.162        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 5.58     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013397977 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0373      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 4.67     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010544149 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0461      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0902      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 6.14     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013091231 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0756      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 13.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010071786 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | -1.45    |\n",
      "|    ep_true_rew_mean | 30.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.7         |\n",
      "|    mean_reward          | -12.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136543745 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.161        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.091        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0234      |\n",
      "|    value_loss           | 0.234        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.9     |\n",
      "|    mean_reward     | 12.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    ep_true_rew_mean | 34.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.3        |\n",
      "|    mean_reward          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012291733 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0832      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=23.40 +/- 59.28\n",
      "Episode length: 16.60 +/- 10.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | 23.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | -1.23    |\n",
      "|    ep_true_rew_mean | 38.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=35.70 +/- 60.71\n",
      "Episode length: 14.30 +/- 10.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016277991 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0896      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.917   |\n",
      "|    ep_true_rew_mean | 67.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=47.60 +/- 59.29\n",
      "Episode length: 12.40 +/- 10.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | 47.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019611614 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=47.50 +/- 59.21\n",
      "Episode length: 12.50 +/- 10.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.5     |\n",
      "|    mean_reward     | 47.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.49     |\n",
      "|    ep_rew_mean      | -0.586   |\n",
      "|    ep_true_rew_mean | 84.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=84.20 +/- 36.43\n",
      "Episode length: 5.80 +/- 6.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 84.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023871912 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.1      |\n",
      "|    ep_rew_mean      | -0.376   |\n",
      "|    ep_true_rew_mean | 92.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.92\n",
      "Episode length: 4.10 +/- 1.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020463163 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0484     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.0528      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=70.60 +/- 47.82\n",
      "Episode length: 9.40 +/- 7.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 70.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.15     |\n",
      "|    ep_rew_mean      | -0.29    |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.90 +/- 1.30\n",
      "Episode length: 3.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017448474 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.717      |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.0276      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.30 +/- 0.90\n",
      "Episode length: 4.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.93     |\n",
      "|    ep_rew_mean      | -0.261   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.60 +/- 2.29\n",
      "Episode length: 4.40 +/- 2.29\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01792796 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.572     |\n",
      "|    explained_variance   | 0.488      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00985   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0492    |\n",
      "|    value_loss           | 0.0146     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.9      |\n",
      "|    ep_rew_mean      | -0.194   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 1.73\n",
      "Episode length: 3.30 +/- 1.73\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.3       |\n",
      "|    mean_reward          | 96.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 15500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186381 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.407    |\n",
      "|    explained_variance   | 0.627     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0387   |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0441   |\n",
      "|    value_loss           | 0.00544   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.00 +/- 1.73\n",
      "Episode length: 5.00 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016136158 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.25     |\n",
      "|    ep_rew_mean      | -0.144   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004378038 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.70 +/- 1.00\n",
      "Episode length: 4.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43     |\n",
      "|    ep_rew_mean      | -0.157   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.50 +/- 1.12\n",
      "Episode length: 4.50 +/- 1.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021185223 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00771     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.00128      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.70 +/- 1.42\n",
      "Episode length: 4.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1656f7d64934d4a8902af92c875a1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015269144 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.101      |\n",
      "|    explained_variance   | -0.0274     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -2.12    |\n",
      "|    ep_true_rew_mean | -19.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006092287 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.0869      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | -12.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 17.9         |\n",
      "|    mean_reward          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065250155 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.21         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -9.74    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.3         |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031802484 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 0.15         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    value_loss           | 0.248        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -7.45    |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029463548 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.13    |\n",
      "|    ep_true_rew_mean | 55.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06782262 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | 0.148      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0992     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.43     |\n",
      "|    ep_rew_mean      | -0.408   |\n",
      "|    ep_true_rew_mean | 91.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 164      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024778988 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.0188      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00752    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.56     |\n",
      "|    ep_rew_mean      | -0.259   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 163      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=94.70 +/- 0.90\n",
      "Episode length: 5.30 +/- 0.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 94.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018141456 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.435      |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0474     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0535     |\n",
      "|    value_loss           | 0.0342      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.00 +/- 1.10\n",
      "Episode length: 5.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.45     |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 165      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.30 +/- 1.95\n",
      "Episode length: 3.70 +/- 1.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017033078 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=95.70 +/- 1.49\n",
      "Episode length: 4.30 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.40 +/- 1.69\n",
      "Episode length: 4.60 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020257127 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.00752     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.54     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.50 +/- 1.80\n",
      "Episode length: 4.50 +/- 1.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07546442 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.626      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0396    |\n",
      "|    value_loss           | 0.00367    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.80 +/- 1.83\n",
      "Episode length: 4.20 +/- 1.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.20 +/- 1.25\n",
      "Episode length: 4.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002223154 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    value_loss           | 0.000523    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.70 +/- 1.42\n",
      "Episode length: 4.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.50 +/- 1.80\n",
      "Episode length: 3.50 +/- 1.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059102327 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0948      |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000384    |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    value_loss           | 0.000647     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.41     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.50 +/- 1.12\n",
      "Episode length: 4.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010397963 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0721     |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.000906    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.85\n",
      "Episode length: 3.30 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108888075 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0754      |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000124    |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 0.000302     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=83.60 +/- 36.25\n",
      "Episode length: 6.40 +/- 6.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | 83.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.30 +/- 1.49\n",
      "Episode length: 4.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010638971 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.089      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.000417    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 22.8      |\n",
      "|    mean_reward          | -12.8     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2738337 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | 0.967     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0128   |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0281   |\n",
      "|    value_loss           | 0.000229  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | -0.798   |\n",
      "|    ep_true_rew_mean | 67.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.1       |\n",
      "|    mean_reward          | 11.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04743881 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.276     |\n",
      "|    explained_variance   | 0.0984     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0213     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0318    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.38     |\n",
      "|    ep_rew_mean      | -0.411   |\n",
      "|    ep_true_rew_mean | 92.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 20.2     |\n",
      "|    mean_reward          | -0.2     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 19500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.015974 |\n",
      "|    clip_fraction        | 0.213    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.251   |\n",
      "|    explained_variance   | 0.26     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0208  |\n",
      "|    n_updates            | 190      |\n",
      "|    policy_gradient_loss | -0.0394  |\n",
      "|    value_loss           | 0.0749   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.03     |\n",
      "|    ep_rew_mean      | -0.296   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23690335 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.242     |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0708    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0489    |\n",
      "|    value_loss           | 0.0383     |\n",
      "----------------------------------------\n",
      "execution time: 342.4102728366852; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbc82f6638b4d1ea8f4d9ae2fe55b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -0.0667  |\n",
      "| time/               |          |\n",
      "|    fps              | 217      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136445835 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.19        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.114        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 2.92     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012008855 |\n",
      "|    clip_fraction        | 0.0937      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0158      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 11.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015396638 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0385      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0935      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 23.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 12          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011924511 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0402      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | -1.37    |\n",
      "|    ep_true_rew_mean | 32       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015392619 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0835      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | -1.03    |\n",
      "|    ep_true_rew_mean | 58.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.3        |\n",
      "|    mean_reward          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014507778 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.812   |\n",
      "|    ep_true_rew_mean | 64.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014776245 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0151      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | -0.629   |\n",
      "|    ep_true_rew_mean | 77       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.90 +/- 48.95\n",
      "Episode length: 7.10 +/- 8.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021962142 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00906     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.42     |\n",
      "|    ep_rew_mean      | -0.395   |\n",
      "|    ep_true_rew_mean | 90.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.3         |\n",
      "|    mean_reward          | 72.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025905937 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0614     |\n",
      "|    value_loss           | 0.0682      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.41     |\n",
      "|    ep_rew_mean      | -0.299   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019498259 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.863      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 0.0389      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.5      |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01665442 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.726     |\n",
      "|    explained_variance   | 0.503      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.056     |\n",
      "|    value_loss           | 0.0269     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 0.92\n",
      "Episode length: 3.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.63     |\n",
      "|    ep_rew_mean      | -0.161   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.90 +/- 0.94\n",
      "Episode length: 3.10 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016516656 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.00858     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.20 +/- 1.66\n",
      "Episode length: 3.80 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0972  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029695405 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2      |\n",
      "|    ep_rew_mean      | -0.118   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041253626 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00786     |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.30 +/- 0.90\n",
      "Episode length: 2.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.00 +/- 0.89\n",
      "Episode length: 3.00 +/- 0.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06807113 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.209     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.0102     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.0888  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046175774 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0173      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 0.00151      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0736  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014185169 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00859     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00907     |\n",
      "|    value_loss           | 0.000463     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.46     |\n",
      "|    ep_rew_mean      | -0.0727  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026230898 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.703        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.034       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    value_loss           | 0.00109      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.0821  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002650008 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0934     |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 0.000678    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=97.20 +/- 1.08\n",
      "Episode length: 2.80 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.2      |\n",
      "|    ep_rew_mean      | -0.0551  |\n",
      "|    ep_true_rew_mean | 96.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.80 +/- 1.78\n",
      "Episode length: 3.20 +/- 1.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012557885 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0746      |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00102     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00614     |\n",
      "|    value_loss           | 0.000578     |\n",
      "------------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17fcc74e511451bbb96710447bea8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -7       |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008880746 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.173      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -1.48    |\n",
      "| time/               |          |\n",
      "|    fps              | 198      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012499901 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.00112    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 6.52     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012556223 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.032       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0623      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.236       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 21.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014561626 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0972      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0851      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.222       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | -1.35    |\n",
      "|    ep_true_rew_mean | 39.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.60 +/- 48.81\n",
      "Episode length: 20.60 +/- 8.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012212556 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | -1.26    |\n",
      "|    ep_true_rew_mean | 47.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.9        |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016444534 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.20 +/- 55.30\n",
      "Episode length: 18.80 +/- 9.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | 11.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 52.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.00 +/- 59.61\n",
      "Episode length: 12.00 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 48          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013924021 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.30 +/- 55.45\n",
      "Episode length: 18.70 +/- 9.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.7     |\n",
      "|    mean_reward     | 11.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | -0.655   |\n",
      "|    ep_true_rew_mean | 79.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.3       |\n",
      "|    mean_reward          | 23.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01809177 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.36     |\n",
      "|    ep_rew_mean      | -0.574   |\n",
      "|    ep_true_rew_mean | 90.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.70 +/- 36.59\n",
      "Episode length: 5.30 +/- 6.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022039011 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0615     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.0789      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.9      |\n",
      "|    ep_rew_mean      | -0.431   |\n",
      "|    ep_true_rew_mean | 92.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.70 +/- 1.35\n",
      "Episode length: 4.30 +/- 1.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02576193 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.856     |\n",
      "|    explained_variance   | 0.513      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0367    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    value_loss           | 0.0499     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=71.40 +/- 48.21\n",
      "Episode length: 8.60 +/- 8.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 71.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.93     |\n",
      "|    ep_rew_mean      | -0.274   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=84.30 +/- 36.46\n",
      "Episode length: 5.70 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016557492 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0486     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.0219      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=60.30 +/- 55.86\n",
      "Episode length: 9.70 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.01     |\n",
      "|    ep_rew_mean      | -0.271   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.70 +/- 1.68\n",
      "Episode length: 4.30 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012598041 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.1      |\n",
      "|    ep_rew_mean      | -0.206   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.70 +/- 1.95\n",
      "Episode length: 4.30 +/- 1.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016585402 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0564     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.00613     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.00 +/- 1.84\n",
      "Episode length: 4.00 +/- 1.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.66     |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.50 +/- 1.96\n",
      "Episode length: 4.50 +/- 1.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02071694 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.608      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    value_loss           | 0.00376    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.10 +/- 1.92\n",
      "Episode length: 4.90 +/- 1.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.9          |\n",
      "|    mean_reward          | 95.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065507595 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0175      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0183      |\n",
      "|    value_loss           | 0.00189      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.50 +/- 1.75\n",
      "Episode length: 4.50 +/- 1.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036460147 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0231      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    value_loss           | 0.0013       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.40 +/- 2.11\n",
      "Episode length: 4.60 +/- 2.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.7          |\n",
      "|    mean_reward          | 95.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021087776 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.021       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 0.000624     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.81\n",
      "Episode length: 3.90 +/- 1.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65     |\n",
      "|    ep_rew_mean      | -0.153   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.50 +/- 1.20\n",
      "Episode length: 4.50 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036180173 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0895      |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000565     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    value_loss           | 0.00131      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61     |\n",
      "|    ep_rew_mean      | -0.159   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.20 +/- 1.60\n",
      "Episode length: 3.80 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013162105 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0716      |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00041      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00923     |\n",
      "|    value_loss           | 0.000547     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006655079 |\n",
      "|    clip_fraction        | 0.0236      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0719     |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000264   |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00822    |\n",
      "|    value_loss           | 0.00065     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.10 +/- 1.87\n",
      "Episode length: 3.90 +/- 1.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.3       |\n",
      "|    mean_reward          | -0.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12485606 |\n",
      "|    clip_fraction        | 0.295      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.291     |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    value_loss           | 0.000225   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.26     |\n",
      "|    ep_rew_mean      | -0.501   |\n",
      "|    ep_true_rew_mean | 89.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.40 +/- 1.62\n",
      "Episode length: 3.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051274545 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0488      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0425     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 0.0741      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.00 +/- 1.67\n",
      "Episode length: 4.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.71     |\n",
      "|    ep_rew_mean      | -0.27    |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=95.40 +/- 1.85\n",
      "Episode length: 4.60 +/- 1.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | 95.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01565636 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.465     |\n",
      "|    explained_variance   | 0.269      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0551    |\n",
      "|    value_loss           | 0.0269     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.4      |\n",
      "|    ep_rew_mean      | -0.239   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=94.80 +/- 0.98\n",
      "Episode length: 5.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 94.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010440806 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.0166      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=95.30 +/- 1.49\n",
      "Episode length: 4.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e4b55b1713449692ae103967729bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | 14.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06994907 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.384     |\n",
      "|    explained_variance   | 0.0301     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0516     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00722   |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 62.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027983692 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.0679      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.14     |\n",
      "|    ep_rew_mean      | -0.516   |\n",
      "|    ep_true_rew_mean | 90.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04890436 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.378     |\n",
      "|    explained_variance   | 0.134      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0561    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.59     |\n",
      "|    ep_rew_mean      | -0.349   |\n",
      "|    ep_true_rew_mean | 93.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=96.20 +/- 1.60\n",
      "Episode length: 3.80 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028468069 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.061       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 0.056       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94     |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06094743 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.266     |\n",
      "|    explained_variance   | 0.245      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.0189     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.7      |\n",
      "|    ep_rew_mean      | -0.173   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.40 +/- 1.74\n",
      "Episode length: 3.60 +/- 1.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11781042 |\n",
      "|    clip_fraction        | 0.069      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.1       |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.00475    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=94.70 +/- 1.62\n",
      "Episode length: 5.30 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45     |\n",
      "|    ep_rew_mean      | -0.151   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.20 +/- 1.99\n",
      "Episode length: 3.80 +/- 1.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021872208 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0615      |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0172      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 0.000671     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.80 +/- 1.94\n",
      "Episode length: 4.20 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.126   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025455207 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0485      |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00402     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 0.000364     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35     |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.50 +/- 1.86\n",
      "Episode length: 3.50 +/- 1.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026600547 |\n",
      "|    clip_fraction        | 0.00947      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0461      |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0241      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    value_loss           | 0.00044      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.40 +/- 1.91\n",
      "Episode length: 4.60 +/- 1.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.6          |\n",
      "|    mean_reward          | 95.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020730817 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0396      |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000855    |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00515     |\n",
      "|    value_loss           | 0.000118     |\n",
      "------------------------------------------\n",
      "execution time: 304.68880677223206; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b58a9e28b34df29d24f037a2a2bcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 4.11     |\n",
      "| time/               |          |\n",
      "|    fps              | 213      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.1        |\n",
      "|    mean_reward          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011707767 |\n",
      "|    clip_fraction        | 0.0852      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0788     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 7.98     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011966901 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0189      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.52    |\n",
      "|    ep_true_rew_mean | 20.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | 24.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011214357 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0461      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=48.50 +/- 60.02\n",
      "Episode length: 11.50 +/- 11.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.5     |\n",
      "|    mean_reward     | 48.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 36.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.3        |\n",
      "|    mean_reward          | 23.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013016477 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | -0.975   |\n",
      "|    ep_true_rew_mean | 61.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.16\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.5        |\n",
      "|    mean_reward          | 60.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01657297 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.048      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.60 +/- 61.60\n",
      "Episode length: 13.40 +/- 11.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | 36.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.771   |\n",
      "|    ep_true_rew_mean | 73.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.90 +/- 48.95\n",
      "Episode length: 7.10 +/- 8.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020418005 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.301       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.56     |\n",
      "|    ep_rew_mean      | -0.495   |\n",
      "|    ep_true_rew_mean | 87.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025030874 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.01     |\n",
      "|    ep_rew_mean      | -0.276   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028769894 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.0436      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.40 +/- 1.20\n",
      "Episode length: 4.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.14     |\n",
      "|    ep_rew_mean      | -0.206   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028173856 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.714      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0686     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0593     |\n",
      "|    value_loss           | 0.0215      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.50 +/- 0.81\n",
      "Episode length: 3.50 +/- 0.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.9      |\n",
      "|    ep_rew_mean      | -0.177   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015837975 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0606     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.0105      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.112   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022492792 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0478     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.00619     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.50 +/- 1.36\n",
      "Episode length: 4.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0942  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022042772 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.90 +/- 0.94\n",
      "Episode length: 3.10 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.101   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018277444 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.00192     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.80 +/- 1.66\n",
      "Episode length: 3.20 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0776  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045000403 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00436     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 0.000928     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61eaaec21954877a27b517b91c84505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -4.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 207      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013087359 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0915     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | 3.75     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014122264 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0868      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 13.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015228796 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0691      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.243       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | -1.59    |\n",
      "|    ep_true_rew_mean | 21.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014459869 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.118       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.238       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.51    |\n",
      "|    ep_true_rew_mean | 22.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016619856 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.0808      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 52.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016569592 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.925   |\n",
      "|    ep_true_rew_mean | 68       |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-12.90 +/- 36.30\n",
      "Episode length: 22.90 +/- 6.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.9        |\n",
      "|    mean_reward          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015698258 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | -0.756   |\n",
      "|    ep_true_rew_mean | 77.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=83.30 +/- 36.14\n",
      "Episode length: 6.70 +/- 6.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.7         |\n",
      "|    mean_reward          | 83.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021929296 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00386    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | -0.532   |\n",
      "|    ep_true_rew_mean | 89.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.30 +/- 36.45\n",
      "Episode length: 5.70 +/- 6.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.7        |\n",
      "|    mean_reward          | 84.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151302 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.974     |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    value_loss           | 0.0948     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=83.30 +/- 36.11\n",
      "Episode length: 6.70 +/- 6.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.7      |\n",
      "|    mean_reward     | 83.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.71     |\n",
      "|    ep_rew_mean      | -0.338   |\n",
      "|    ep_true_rew_mean | 93.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=83.10 +/- 36.07\n",
      "Episode length: 6.90 +/- 6.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.9         |\n",
      "|    mean_reward          | 83.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016046051 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.0309      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=71.10 +/- 48.07\n",
      "Episode length: 8.90 +/- 8.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.9      |\n",
      "|    mean_reward     | 71.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.9      |\n",
      "|    ep_rew_mean      | -0.267   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.60 +/- 1.74\n",
      "Episode length: 3.40 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017778257 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    value_loss           | 0.0188      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=95.00 +/- 1.26\n",
      "Episode length: 5.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89     |\n",
      "|    ep_rew_mean      | -0.192   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.50 +/- 1.20\n",
      "Episode length: 4.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013267885 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.00834     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.30 +/- 1.10\n",
      "Episode length: 4.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.97     |\n",
      "|    ep_rew_mean      | -0.2     |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.40 +/- 1.62\n",
      "Episode length: 4.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019866101 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.597       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.00609     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.10 +/- 1.14\n",
      "Episode length: 4.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.165   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.00 +/- 1.61\n",
      "Episode length: 4.00 +/- 1.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004316171 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.00207     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.50 +/- 1.57\n",
      "Episode length: 4.50 +/- 1.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.46     |\n",
      "|    ep_rew_mean      | -0.161   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063895667 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.761        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0162      |\n",
      "|    value_loss           | 0.00206      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.162   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.90 +/- 1.70\n",
      "Episode length: 4.10 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001837844 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00633    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.0015      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.25     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.66\n",
      "Episode length: 3.80 +/- 1.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012699177 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00163     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00809     |\n",
      "|    value_loss           | 0.000378     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.80 +/- 1.25\n",
      "Episode length: 4.20 +/- 1.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058700605 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000471     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 0.000912     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.90 +/- 2.21\n",
      "Episode length: 4.10 +/- 2.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.7          |\n",
      "|    mean_reward          | 95.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014664647 |\n",
      "|    clip_fraction        | 0.016        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0991      |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00196      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.007       |\n",
      "|    value_loss           | 0.000428     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.145   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039704605 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0987      |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00154      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0079      |\n",
      "|    value_loss           | 0.000273     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.00 +/- 1.90\n",
      "Episode length: 4.00 +/- 1.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.20 +/- 1.89\n",
      "Episode length: 3.80 +/- 1.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017221427 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0919      |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 0.000405     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.10 +/- 1.51\n",
      "Episode length: 4.90 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a710ffa957419d98c8750db789f80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -2.56    |\n",
      "| time/               |          |\n",
      "|    fps              | 214      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07902311 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.503     |\n",
      "|    explained_variance   | 0.00769    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0562     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | -1.26    |\n",
      "|    ep_true_rew_mean | 52.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022534853 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.0495      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0683      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.76     |\n",
      "|    ep_rew_mean      | -0.564   |\n",
      "|    ep_true_rew_mean | 89.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=95.30 +/- 1.62\n",
      "Episode length: 4.70 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030269101 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.0667      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.47     |\n",
      "|    ep_rew_mean      | -0.337   |\n",
      "|    ep_true_rew_mean | 93.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02284365 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.406     |\n",
      "|    explained_variance   | 0.1        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 0.0592     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=95.10 +/- 1.58\n",
      "Episode length: 4.90 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.34     |\n",
      "|    ep_rew_mean      | -0.238   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=94.80 +/- 1.17\n",
      "Episode length: 5.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 94.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032979432 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.017       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=96.50 +/- 1.75\n",
      "Episode length: 3.50 +/- 1.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | -0.164   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=95.80 +/- 1.54\n",
      "Episode length: 4.20 +/- 1.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08054821 |\n",
      "|    clip_fraction        | 0.0631     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.161     |\n",
      "|    explained_variance   | 0.481      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.00703    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=96.20 +/- 1.60\n",
      "Episode length: 3.80 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.144   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016774867 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0982     |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.139   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05503267 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.152     |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.94     |\n",
      "|    ep_rew_mean      | -0.307   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.60 +/- 1.28\n",
      "Episode length: 4.40 +/- 1.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07655026 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.19      |\n",
      "|    explained_variance   | 0.0386     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0541    |\n",
      "|    value_loss           | 0.0451     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.00 +/- 1.55\n",
      "Episode length: 4.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.04     |\n",
      "|    ep_rew_mean      | -0.195   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11287046 |\n",
      "|    clip_fraction        | 0.302      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0543    |\n",
      "|    value_loss           | 0.0105     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 1.91\n",
      "Episode length: 4.40 +/- 1.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.151   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014439138 |\n",
      "|    clip_fraction        | 0.0767      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0774     |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "execution time: 266.6720061302185; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb29412687bc4d0788425ff0015ba089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | 4.13     |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014193204 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.198      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 7.14     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.5        |\n",
      "|    mean_reward          | 11.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010922173 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.061       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 18.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.4        |\n",
      "|    mean_reward          | 48.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012357989 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0505      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0913      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 42.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014885835 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.1     |\n",
      "|    ep_true_rew_mean | 53.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014666524 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0766      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.854   |\n",
      "|    ep_true_rew_mean | 73.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024417358 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000712    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.49     |\n",
      "|    ep_rew_mean      | -0.549   |\n",
      "|    ep_true_rew_mean | 84.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026744848 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00334    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.02     |\n",
      "|    ep_rew_mean      | -0.351   |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.50 +/- 36.52\n",
      "Episode length: 5.50 +/- 6.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.5         |\n",
      "|    mean_reward          | 84.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039712492 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.061      |\n",
      "|    value_loss           | 0.0511      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=84.40 +/- 36.49\n",
      "Episode length: 5.60 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5        |\n",
      "|    ep_rew_mean      | -0.18    |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021832168 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.696      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0494     |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=97.10 +/- 1.22\n",
      "Episode length: 2.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026853517 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0452     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    value_loss           | 0.00942     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 1.02\n",
      "Episode length: 4.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017860878 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.107   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026252363 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.00174     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.90 +/- 1.70\n",
      "Episode length: 3.10 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.0769  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060636783 |\n",
      "|    clip_fraction        | 0.0543       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.029       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0223      |\n",
      "|    value_loss           | 0.00229      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.38     |\n",
      "|    ep_rew_mean      | -0.0655  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005949029 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.000899    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.80 +/- 0.87\n",
      "Episode length: 4.20 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | -0.0732  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.60 +/- 1.02\n",
      "Episode length: 4.40 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030077766 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.016       |\n",
      "|    value_loss           | 0.000586     |\n",
      "------------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3225bbc9d64d4e61930abde20daff3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 1.71     |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119738635 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.135       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0341       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0182      |\n",
      "|    value_loss           | 0.147        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -2.49    |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00783301 |\n",
      "|    clip_fraction        | 0.0517     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | 0.00626    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0569     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00769   |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -2.55    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009185335 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0289      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 0.44     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134179145 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.064        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0812       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 0.228        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | 11.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014263209 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.083       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.241       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.35    |\n",
      "|    ep_true_rew_mean | 34.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.50 +/- 59.40\n",
      "Episode length: 16.50 +/- 10.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.5        |\n",
      "|    mean_reward          | 23.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013488749 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.075       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 55.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016595298 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.874   |\n",
      "|    ep_true_rew_mean | 67.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=35.60 +/- 60.61\n",
      "Episode length: 14.40 +/- 10.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016561747 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.57     |\n",
      "|    ep_rew_mean      | -0.527   |\n",
      "|    ep_true_rew_mean | 87.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021290373 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0575     |\n",
      "|    value_loss           | 0.0763      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.52     |\n",
      "|    ep_rew_mean      | -0.404   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.20 +/- 1.72\n",
      "Episode length: 3.80 +/- 1.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02386961 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.956     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0465    |\n",
      "|    value_loss           | 0.0475     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.00 +/- 1.90\n",
      "Episode length: 4.00 +/- 1.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.17     |\n",
      "|    ep_rew_mean      | -0.303   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.00 +/- 1.95\n",
      "Episode length: 4.00 +/- 1.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018474028 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0465     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.0276      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.87\n",
      "Episode length: 3.90 +/- 1.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.68     |\n",
      "|    ep_rew_mean      | -0.254   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.90 +/- 1.81\n",
      "Episode length: 3.10 +/- 1.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013683066 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0567     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 0.0109      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.35     |\n",
      "|    ep_rew_mean      | -0.225   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020603279 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.51     |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016281115 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.621       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48     |\n",
      "|    ep_rew_mean      | -0.162   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=94.90 +/- 1.04\n",
      "Episode length: 5.10 +/- 1.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.1        |\n",
      "|    mean_reward          | 94.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00746007 |\n",
      "|    clip_fraction        | 0.0597     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.213     |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0135    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    value_loss           | 0.00269    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.64     |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 0.75\n",
      "Episode length: 3.80 +/- 0.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047144243 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.157       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00808     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0161      |\n",
      "|    value_loss           | 0.00181      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.10 +/- 0.83\n",
      "Episode length: 4.90 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23881881 |\n",
      "|    clip_fraction        | 0.0511     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.173     |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.3       |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | 0.0967     |\n",
      "|    value_loss           | 0.000714   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cff0914b744c62a112f700a4319bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | 1.78     |\n",
      "| time/               |          |\n",
      "|    fps              | 215      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028268244 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.0406      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.024       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 35       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036253773 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.0473      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -1       |\n",
      "|    ep_true_rew_mean | 68.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032908194 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.0844      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0775      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.94     |\n",
      "|    ep_rew_mean      | -0.592   |\n",
      "|    ep_true_rew_mean | 89.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020463336 |\n",
      "|    clip_fraction        | 0.401       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | -0.00249    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=94.90 +/- 1.30\n",
      "Episode length: 5.10 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 94.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.2      |\n",
      "|    ep_rew_mean      | -0.406   |\n",
      "|    ep_true_rew_mean | 92.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=95.10 +/- 1.14\n",
      "Episode length: 4.90 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 95.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040794663 |\n",
      "|    clip_fraction        | 0.431       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0577     |\n",
      "|    value_loss           | 0.0574      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=96.00 +/- 1.73\n",
      "Episode length: 4.00 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.51     |\n",
      "|    ep_rew_mean      | -0.256   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05986215 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.333     |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.039     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0531    |\n",
      "|    value_loss           | 0.0269     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=96.70 +/- 1.85\n",
      "Episode length: 3.30 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.85     |\n",
      "|    ep_rew_mean      | -0.191   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008969085 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.00849     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | -0.171   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.60 +/- 1.91\n",
      "Episode length: 4.40 +/- 1.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019336952 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.134      |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=95.10 +/- 1.70\n",
      "Episode length: 4.90 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.20 +/- 1.08\n",
      "Episode length: 3.80 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055314302 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0327      |\n",
      "|    value_loss           | 0.000802    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=94.90 +/- 1.30\n",
      "Episode length: 5.10 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 94.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | -0.198   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.40 +/- 1.50\n",
      "Episode length: 4.60 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075056925 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0954     |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0612     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 0.00887     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.80 +/- 1.60\n",
      "Episode length: 4.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.123   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01510069 |\n",
      "|    clip_fraction        | 0.0346     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0724    |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00993   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.00292    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.74\n",
      "Episode length: 3.60 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.8      |\n",
      "|    ep_rew_mean      | -0.163   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.50 +/- 1.50\n",
      "Episode length: 4.50 +/- 1.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10018692 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.19      |\n",
      "|    explained_variance   | 0.935      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.056     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.192      |\n",
      "|    value_loss           | 0.000449   |\n",
      "----------------------------------------\n",
      "execution time: 255.29710698127747; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aece2237d2744d6aeb1e057ac1dcd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | 4.38     |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011166027 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | 0.824    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.3        |\n",
      "|    mean_reward          | 48.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011693478 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0271      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 14.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009222737 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 31.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014346994 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.0533      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0618      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.3     |\n",
      "|    ep_true_rew_mean | 46       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.40 +/- 55.60\n",
      "Episode length: 18.60 +/- 9.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | 11.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014181057 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.0731      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0807      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | -1.04    |\n",
      "|    ep_true_rew_mean | 61.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.7        |\n",
      "|    mean_reward          | 24.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018207412 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.82     |\n",
      "|    ep_rew_mean      | -0.53    |\n",
      "|    ep_true_rew_mean | 89.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=61.00 +/- 56.30\n",
      "Episode length: 9.00 +/- 10.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9          |\n",
      "|    mean_reward          | 61         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02660081 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.17      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.013     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.0975     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.89     |\n",
      "|    ep_rew_mean      | -0.429   |\n",
      "|    ep_true_rew_mean | 87.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025620766 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.0762      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.52     |\n",
      "|    ep_rew_mean      | -0.231   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023027368 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.832      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.054      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 0.0304      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.15     |\n",
      "|    ep_rew_mean      | -0.199   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.60 +/- 1.50\n",
      "Episode length: 4.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015709676 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0389     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 0.0138      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.00 +/- 0.77\n",
      "Episode length: 4.00 +/- 0.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025878344 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0918     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.00928     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.103   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01677877 |\n",
      "|    clip_fraction        | 0.0979     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.31      |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    value_loss           | 0.00353    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.92     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008669724 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.00204     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.53     |\n",
      "|    ep_rew_mean      | -0.0767  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=97.00 +/- 0.89\n",
      "Episode length: 3.00 +/- 0.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00431336 |\n",
      "|    clip_fraction        | 0.0393     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.161     |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.80 +/- 0.87\n",
      "Episode length: 3.20 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.67     |\n",
      "|    ep_rew_mean      | -0.0827  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023715927 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00476      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 0.000798     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.45     |\n",
      "|    ep_rew_mean      | -0.066   |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033813794 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00857     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 0.000602     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.00 +/- 1.55\n",
      "Episode length: 3.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.0711  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022755482 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0893      |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0265      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00737     |\n",
      "|    value_loss           | 0.000243     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0685  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052553546 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0702      |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00817     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0151      |\n",
      "|    value_loss           | 0.00041      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b788434e7d684a4599f79e89f04171ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -1.94    |\n",
      "|    ep_true_rew_mean | -9.86    |\n",
      "| time/               |          |\n",
      "|    fps              | 213      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009427997 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.215      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -9.57    |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011876972 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0465      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -6.03    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009417431 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0357      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 1.85     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011597142 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0404      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 9.17     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011536179 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0213      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 6.56     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.1       |\n",
      "|    mean_reward          | 23.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01035739 |\n",
      "|    clip_fraction        | 0.0831     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.0614     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.093      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | -1.54    |\n",
      "|    ep_true_rew_mean | 19.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014321506 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0906      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.41    |\n",
      "|    ep_true_rew_mean | 30.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 10.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012914492 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0908      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 0.226       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=23.90 +/- 59.90\n",
      "Episode length: 16.10 +/- 10.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 53.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.50 +/- 60.02\n",
      "Episode length: 11.50 +/- 11.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.5        |\n",
      "|    mean_reward          | 48.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016846912 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.7     |\n",
      "|    ep_rew_mean      | -0.875   |\n",
      "|    ep_true_rew_mean | 62.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.2        |\n",
      "|    mean_reward          | 35.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017587978 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | -0.807   |\n",
      "|    ep_true_rew_mean | 68.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.6       |\n",
      "|    mean_reward          | 48.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01541236 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | 0.464      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0347     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.41     |\n",
      "|    ep_rew_mean      | -0.483   |\n",
      "|    ep_true_rew_mean | 87.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=71.90 +/- 48.48\n",
      "Episode length: 8.10 +/- 8.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.1         |\n",
      "|    mean_reward          | 71.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015744053 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0475     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.0891      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=71.90 +/- 48.48\n",
      "Episode length: 8.10 +/- 8.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | 71.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.63     |\n",
      "|    ep_rew_mean      | -0.404   |\n",
      "|    ep_true_rew_mean | 90.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.90\n",
      "Episode length: 3.70 +/- 1.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024030326 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 0.0522      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=83.20 +/- 36.08\n",
      "Episode length: 6.80 +/- 6.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.8      |\n",
      "|    mean_reward     | 83.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.43     |\n",
      "|    ep_rew_mean      | -0.312   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.60 +/- 1.36\n",
      "Episode length: 4.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016356252 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0533     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.50 +/- 1.12\n",
      "Episode length: 4.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.19     |\n",
      "|    ep_rew_mean      | -0.213   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017136231 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0275     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 0.00809     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.20 +/- 1.94\n",
      "Episode length: 4.80 +/- 1.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024825279 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.50 +/- 1.57\n",
      "Episode length: 4.50 +/- 1.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | -0.185   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013061539 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0634     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=96.00 +/- 1.26\n",
      "Episode length: 4.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.41     |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.00 +/- 1.48\n",
      "Episode length: 5.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 95          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008777331 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.70 +/- 1.62\n",
      "Episode length: 4.30 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.60 +/- 1.91\n",
      "Episode length: 4.40 +/- 1.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027810787 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00896     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 0.00116      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.60 +/- 1.56\n",
      "Episode length: 4.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15     |\n",
      "|    ep_rew_mean      | -0.125   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002019593 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.101      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 0.000463    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.70 +/- 1.73\n",
      "Episode length: 4.30 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.28     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00253331 |\n",
      "|    clip_fraction        | 0.0272     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0815    |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00885   |\n",
      "|    value_loss           | 0.000621   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.50 +/- 1.63\n",
      "Episode length: 4.50 +/- 1.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.23     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.00 +/- 1.61\n",
      "Episode length: 4.00 +/- 1.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008284169 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0836     |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.00755     |\n",
      "|    value_loss           | 0.000476    |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bfc16b916a4712883e555ea565a513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.99    |\n",
      "|    ep_true_rew_mean | -7.16    |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06875011 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.631     |\n",
      "|    explained_variance   | 0.00763    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0402     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00684   |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 38.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017761711 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.0705      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.221       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.3     |\n",
      "|    ep_rew_mean      | -0.882   |\n",
      "|    ep_true_rew_mean | 78.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021400455 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.0538      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.92     |\n",
      "|    ep_rew_mean      | -0.466   |\n",
      "|    ep_true_rew_mean | 92.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035199873 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.24     |\n",
      "|    ep_rew_mean      | -0.228   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=95.50 +/- 1.20\n",
      "Episode length: 4.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061902564 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | -0.171   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | 96.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04078233 |\n",
      "|    clip_fraction        | 0.094      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.422      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.00954    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=96.50 +/- 1.86\n",
      "Episode length: 3.50 +/- 1.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.62     |\n",
      "|    ep_rew_mean      | -0.169   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076768617 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0933      |\n",
      "|    explained_variance   | 0.767        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00551     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0108      |\n",
      "|    value_loss           | 0.00179      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=95.90 +/- 1.64\n",
      "Episode length: 4.10 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5      |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.20 +/- 1.08\n",
      "Episode length: 4.80 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017399685 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.89e-06    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56     |\n",
      "|    ep_rew_mean      | -0.162   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09778008 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.111     |\n",
      "|    explained_variance   | 0.634      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    value_loss           | 0.0029     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=94.70 +/- 1.10\n",
      "Episode length: 5.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 94.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.95     |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03442127 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.689      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.0031     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.00 +/- 1.84\n",
      "Episode length: 5.00 +/- 1.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.38     |\n",
      "|    ep_rew_mean      | -0.152   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.40 +/- 1.80\n",
      "Episode length: 4.60 +/- 1.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | 95.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05653897 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.16      |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "execution time: 295.2100598812103; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5024415a164678ae6f74dd4f60adde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -2.44    |\n",
      "| time/               |          |\n",
      "|    fps              | 215      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016035901 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.348      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00528    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.0804      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -4.78    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075752973 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | -0.0323      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0331       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00667     |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -5.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070007266 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | -0.00187     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.072        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 0.247        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -8.05    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008436882 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | -0.0103     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -0.11    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009732361 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0591      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | 13.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009554101 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0407      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0993      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.248       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 20.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.40 +/- 61.41\n",
      "Episode length: 13.60 +/- 11.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011548857 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0884      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=48.10 +/- 59.70\n",
      "Episode length: 11.90 +/- 10.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 36.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012457567 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.966   |\n",
      "|    ep_true_rew_mean | 62.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 23.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015212576 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0868      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=36.00 +/- 61.01\n",
      "Episode length: 14.00 +/- 11.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | -0.798   |\n",
      "|    ep_true_rew_mean | 76.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=84.20 +/- 36.43\n",
      "Episode length: 5.80 +/- 6.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 84.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018385984 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=84.80 +/- 36.62\n",
      "Episode length: 5.20 +/- 6.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.78     |\n",
      "|    ep_rew_mean      | -0.461   |\n",
      "|    ep_true_rew_mean | 89.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=84.40 +/- 36.49\n",
      "Episode length: 5.60 +/- 6.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.6        |\n",
      "|    mean_reward          | 84.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02045834 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00852   |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0474    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=84.10 +/- 36.39\n",
      "Episode length: 5.90 +/- 6.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.69     |\n",
      "|    ep_rew_mean      | -0.329   |\n",
      "|    ep_true_rew_mean | 93.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021752413 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.896      |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0376     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0527     |\n",
      "|    value_loss           | 0.0592      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=71.40 +/- 48.21\n",
      "Episode length: 8.60 +/- 8.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 71.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.82     |\n",
      "|    ep_rew_mean      | -0.251   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03027078 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.709     |\n",
      "|    explained_variance   | 0.251      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0623    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.0302     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.2        |\n",
      "|    mean_reward          | 96.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02515854 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.506     |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0572    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.0112     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.3      |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.10 +/- 0.54\n",
      "Episode length: 3.90 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011009572 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.355      |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.00585     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.40 +/- 1.85\n",
      "Episode length: 3.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0864  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.00 +/- 0.63\n",
      "Episode length: 4.00 +/- 0.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 96           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055199135 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.552        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0455      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0233      |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0924  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031003298 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0212      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0216      |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.50 +/- 1.69\n",
      "Episode length: 3.50 +/- 1.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.72     |\n",
      "|    ep_rew_mean      | -0.0837  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.00 +/- 1.55\n",
      "Episode length: 4.00 +/- 1.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008376248 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.62     |\n",
      "|    ep_rew_mean      | -0.0787  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.60 +/- 0.80\n",
      "Episode length: 4.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031998556 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00723     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 0.000523     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103dd09eb48a4be6b12e6563816c78af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -14.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 226      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03562814 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.566     |\n",
      "|    explained_variance   | -0.0292    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0111     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00187   |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.38    |\n",
      "|    ep_true_rew_mean | 24.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.2       |\n",
      "|    mean_reward          | -0.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03868773 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.611     |\n",
      "|    explained_variance   | -0.0256    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0738     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.837   |\n",
      "|    ep_true_rew_mean | 67.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019108456 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.07        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.84     |\n",
      "|    ep_rew_mean      | -0.595   |\n",
      "|    ep_true_rew_mean | 84.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | 59.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014131022 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.05       |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96     |\n",
      "|    ep_rew_mean      | -0.263   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03286949 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.459     |\n",
      "|    explained_variance   | 0.0616     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0628    |\n",
      "|    value_loss           | 0.0604     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=97.00 +/- 1.26\n",
      "Episode length: 3.00 +/- 1.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.96     |\n",
      "|    ep_rew_mean      | -0.181   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04440777 |\n",
      "|    clip_fraction        | 0.399      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.318     |\n",
      "|    explained_variance   | 0.0819     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0509    |\n",
      "|    value_loss           | 0.0313     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97     |\n",
      "|    ep_rew_mean      | -0.102   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.5       |\n",
      "|    mean_reward          | 96.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0639448 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.187    |\n",
      "|    explained_variance   | 0.144     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0497   |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    value_loss           | 0.006     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0841  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.30 +/- 0.64\n",
      "Episode length: 3.70 +/- 0.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014253827 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00825    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.000858    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.45     |\n",
      "|    ep_rew_mean      | -0.0712  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018786855 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.106      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00281    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    value_loss           | 0.000673    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.37     |\n",
      "|    ep_rew_mean      | -0.0701  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=97.10 +/- 1.51\n",
      "Episode length: 2.90 +/- 1.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.9          |\n",
      "|    mean_reward          | 97.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089384345 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0841      |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0226      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0251      |\n",
      "|    value_loss           | 0.00083      |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.59     |\n",
      "|    ep_rew_mean      | -0.0705  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015657298 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0782     |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00182    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.000396    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | -0.0722  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000772152 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0691     |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000829    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    value_loss           | 0.000683    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0674  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.1          |\n",
      "|    mean_reward          | 96.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030831592 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0661      |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0336      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 0.00056      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | -0.0714  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004306246 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0728     |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00631    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00736    |\n",
      "|    value_loss           | 0.000262    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0646  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029084435 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0998     |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0626     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    value_loss           | 0.000394    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "execution time: 193.98860383033752; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cfca54498a4d71ae8a31100569c221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | 8.12     |\n",
      "| time/               |          |\n",
      "|    fps              | 207      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008267615 |\n",
      "|    clip_fraction        | 0.0596      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.484      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    value_loss           | 0.0811      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 6.82     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=35.70 +/- 60.72\n",
      "Episode length: 14.30 +/- 10.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008664904 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.00647     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=47.20 +/- 58.97\n",
      "Episode length: 12.80 +/- 10.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | 47.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 3.21     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=47.80 +/- 59.45\n",
      "Episode length: 12.20 +/- 10.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.2        |\n",
      "|    mean_reward          | 47.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012193982 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0305      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=35.70 +/- 60.71\n",
      "Episode length: 14.30 +/- 10.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.3     |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 12.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009711377 |\n",
      "|    clip_fraction        | 0.093       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0762      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 20.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011503721 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 37.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=35.70 +/- 60.71\n",
      "Episode length: 14.30 +/- 10.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012597349 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=60.30 +/- 55.86\n",
      "Episode length: 9.70 +/- 10.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.12    |\n",
      "|    ep_true_rew_mean | 51.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017258191 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=71.20 +/- 48.11\n",
      "Episode length: 8.80 +/- 8.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 71.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | -0.78    |\n",
      "|    ep_true_rew_mean | 72.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.90 +/- 1.64\n",
      "Episode length: 3.10 +/- 1.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021138929 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.30 +/- 1.55\n",
      "Episode length: 3.70 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.05     |\n",
      "|    ep_rew_mean      | -0.473   |\n",
      "|    ep_true_rew_mean | 84       |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017147006 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.36     |\n",
      "|    ep_rew_mean      | -0.474   |\n",
      "|    ep_true_rew_mean | 88.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=97.30 +/- 1.27\n",
      "Episode length: 2.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.7         |\n",
      "|    mean_reward          | 97.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020628955 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.0735      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=96.90 +/- 1.76\n",
      "Episode length: 3.10 +/- 1.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.61     |\n",
      "|    ep_rew_mean      | -0.258   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018150724 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.0315      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.50 +/- 1.80\n",
      "Episode length: 3.50 +/- 1.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06     |\n",
      "|    ep_rew_mean      | -0.204   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 0.94\n",
      "Episode length: 4.10 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025384463 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0824     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4      |\n",
      "|    ep_rew_mean      | -0.15    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=97.10 +/- 1.14\n",
      "Episode length: 2.90 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026968554 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0698     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 0.014       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012512881 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0736     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.20 +/- 1.40\n",
      "Episode length: 2.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.103   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007933509 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.218      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.90 +/- 0.54\n",
      "Episode length: 4.10 +/- 0.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d5516b8ec47a88297dff213341ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -13.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007918252 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.00661     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000864   |\n",
      "|    value_loss           | 0.0958      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -14      |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007837101 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00122     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -12.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012465619 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.000716    |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -10.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113745015 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | 0.185        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.096        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 9.62e-05     |\n",
      "|    value_loss           | 0.187        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 3.63     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010099124 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.734      |\n",
      "|    explained_variance   | 0.068       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00969    |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | -1.3     |\n",
      "|    ep_true_rew_mean | 21.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.50 +/- 48.75\n",
      "Episode length: 7.50 +/- 8.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.5         |\n",
      "|    mean_reward          | 72.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012065968 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.9     |\n",
      "|    ep_rew_mean      | -0.971   |\n",
      "|    ep_true_rew_mean | 43.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | 60.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010076091 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.703      |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.80 +/- 48.91\n",
      "Episode length: 7.20 +/- 8.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | -0.708   |\n",
      "|    ep_true_rew_mean | 63       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.6         |\n",
      "|    mean_reward          | 48.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138242915 |\n",
      "|    clip_fraction        | 0.192        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.67        |\n",
      "|    explained_variance   | 0.309        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0678       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    value_loss           | 0.198        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.04     |\n",
      "|    ep_rew_mean      | -0.375   |\n",
      "|    ep_true_rew_mean | 86       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.2        |\n",
      "|    mean_reward          | 60.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04064425 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.576     |\n",
      "|    explained_variance   | 0.511      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.60 +/- 48.80\n",
      "Episode length: 7.40 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 72.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.57     |\n",
      "|    ep_rew_mean      | -0.325   |\n",
      "|    ep_true_rew_mean | 89.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016578574 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.52        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    value_loss           | 0.0719      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=72.90 +/- 48.95\n",
      "Episode length: 7.10 +/- 8.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.1      |\n",
      "|    mean_reward     | 72.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.85     |\n",
      "|    ep_rew_mean      | -0.241   |\n",
      "|    ep_true_rew_mean | 94.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=84.90 +/- 36.65\n",
      "Episode length: 5.10 +/- 6.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.1         |\n",
      "|    mean_reward          | 84.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020933475 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    value_loss           | 0.0339      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65     |\n",
      "|    ep_rew_mean      | -0.157   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=71.80 +/- 48.40\n",
      "Episode length: 8.20 +/- 8.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | 71.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009765098 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000521    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.0176      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=84.50 +/- 36.52\n",
      "Episode length: 5.50 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | 84.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.43     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=84.30 +/- 36.45\n",
      "Episode length: 5.70 +/- 6.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.7        |\n",
      "|    mean_reward          | 84.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01659052 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.331     |\n",
      "|    explained_variance   | 0.469      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    value_loss           | 0.00778    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=84.90 +/- 36.65\n",
      "Episode length: 5.10 +/- 6.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 84.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020545438 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=97.20 +/- 0.87\n",
      "Episode length: 2.80 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0863  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.60 +/- 1.28\n",
      "Episode length: 4.40 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013908431 |\n",
      "|    clip_fraction        | 0.0769      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.79\n",
      "Episode length: 3.70 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.9      |\n",
      "|    ep_rew_mean      | -0.0905  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007432726 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0812  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006317947 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.60 +/- 0.92\n",
      "Episode length: 3.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.0842  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010198096 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00452    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00211     |\n",
      "|    value_loss           | 0.000654    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=97.00 +/- 1.34\n",
      "Episode length: 3.00 +/- 1.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.51     |\n",
      "|    ep_rew_mean      | -0.0747  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039184332 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00321    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.000858    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.00 +/- 1.00\n",
      "Episode length: 4.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.85     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.8       |\n",
      "|    mean_reward          | 95.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0712515 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | 0.577     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0516   |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | -0.0503   |\n",
      "|    value_loss           | 0.00223   |\n",
      "---------------------------------------\n",
      "execution time: 196.38495635986328; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c5831ddd8249648cd36927db603c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -0.378   |\n",
      "| time/               |          |\n",
      "|    fps              | 214      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015613979 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.409      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.0811      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -0.278   |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740567 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -4.58    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008142681 |\n",
      "|    clip_fraction        | 0.0829      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -8.64e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -0.05    |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075734295 |\n",
      "|    clip_fraction        | 0.0665       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.57        |\n",
      "|    explained_variance   | 0.0469       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0768       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    value_loss           | 0.245        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 7.85     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010858379 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0439      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0833      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 14.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264357 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0483      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | -1.37    |\n",
      "|    ep_true_rew_mean | 30.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014383664 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-1.20 +/- 47.60\n",
      "Episode length: 21.20 +/- 7.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.2     |\n",
      "|    mean_reward     | -1.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.2     |\n",
      "|    ep_rew_mean      | -1.19    |\n",
      "|    ep_true_rew_mean | 39.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.30 +/- 48.67\n",
      "Episode length: 7.70 +/- 8.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.7         |\n",
      "|    mean_reward          | 72.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012733106 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0894      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.965   |\n",
      "|    ep_true_rew_mean | 56.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.70 +/- 36.58\n",
      "Episode length: 5.30 +/- 6.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012955867 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=95.70 +/- 1.49\n",
      "Episode length: 4.30 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.3     |\n",
      "|    ep_rew_mean      | -0.836   |\n",
      "|    ep_true_rew_mean | 68.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.74\n",
      "Episode length: 3.60 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017975222 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0839      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=95.50 +/- 1.12\n",
      "Episode length: 4.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.91     |\n",
      "|    ep_rew_mean      | -0.63    |\n",
      "|    ep_true_rew_mean | 78.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.90 +/- 1.37\n",
      "Episode length: 3.10 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016814062 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.9      |\n",
      "|    ep_rew_mean      | -0.43    |\n",
      "|    ep_true_rew_mean | 91.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021114953 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00581    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 0.0925      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.90 +/- 1.51\n",
      "Episode length: 3.10 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.2      |\n",
      "|    ep_rew_mean      | -0.356   |\n",
      "|    ep_true_rew_mean | 92.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024142481 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.832      |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.056      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 0.0616      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.10 +/- 1.81\n",
      "Episode length: 3.90 +/- 1.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.29     |\n",
      "|    ep_rew_mean      | -0.226   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031094609 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0327     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.80 +/- 1.08\n",
      "Episode length: 4.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45     |\n",
      "|    ep_rew_mean      | -0.141   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 1.33\n",
      "Episode length: 4.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021029118 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.00797     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.13    |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027311929 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.80 +/- 0.75\n",
      "Episode length: 4.20 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b97c0dc8cc7421993a7d2d6f8f31017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -22      |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009902093 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.804      |\n",
      "|    explained_variance   | 0.0497      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    value_loss           | 0.0941      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | -9.12    |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014888942 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.778      |\n",
      "|    explained_variance   | 0.0568      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0858      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 10.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006978007 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.761      |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | -1.21    |\n",
      "|    ep_true_rew_mean | 24.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015277875 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.753      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.24    |\n",
      "|    ep_true_rew_mean | 21.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.10 +/- 61.10\n",
      "Episode length: 13.90 +/- 11.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011128773 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.756      |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | -1.08    |\n",
      "|    ep_true_rew_mean | 30.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012996253 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=60.40 +/- 55.92\n",
      "Episode length: 9.60 +/- 10.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | -0.977   |\n",
      "|    ep_true_rew_mean | 37.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008160182 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.77       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.5     |\n",
      "|    ep_rew_mean      | -0.943   |\n",
      "|    ep_true_rew_mean | 39.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.8         |\n",
      "|    mean_reward          | 36.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061210366 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.773       |\n",
      "|    explained_variance   | 0.391        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.087        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.000175     |\n",
      "|    value_loss           | 0.161        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.907   |\n",
      "|    ep_true_rew_mean | 42       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009090073 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0757      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.30 +/- 59.16\n",
      "Episode length: 16.70 +/- 10.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.7     |\n",
      "|    mean_reward     | 23.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | -0.908   |\n",
      "|    ep_true_rew_mean | 40.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.9         |\n",
      "|    mean_reward          | 36.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077730026 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0876       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    value_loss           | 0.188        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.903   |\n",
      "|    ep_true_rew_mean | 42       |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=35.70 +/- 60.71\n",
      "Episode length: 14.30 +/- 10.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14.3         |\n",
      "|    mean_reward          | 35.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053772936 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0.405        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0663       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | 35.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -0.932   |\n",
      "|    ep_true_rew_mean | 39.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=60.90 +/- 56.24\n",
      "Episode length: 9.10 +/- 10.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.1         |\n",
      "|    mean_reward          | 60.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010202173 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=72.50 +/- 48.77\n",
      "Episode length: 7.50 +/- 8.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | -0.881   |\n",
      "|    ep_true_rew_mean | 44.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.3       |\n",
      "|    mean_reward          | 48.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932799 |\n",
      "|    clip_fraction        | 0.0967     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.695     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0927     |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | -0.723   |\n",
      "|    ep_true_rew_mean | 55       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.3       |\n",
      "|    mean_reward          | 48.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00959994 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0556     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | -0.709   |\n",
      "|    ep_true_rew_mean | 57.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013756331 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=61.00 +/- 56.31\n",
      "Episode length: 9.00 +/- 10.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 61       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.733   |\n",
      "|    ep_true_rew_mean | 57.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011311771 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.717      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.7     |\n",
      "|    mean_reward     | 36.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.94     |\n",
      "|    ep_rew_mean      | -0.526   |\n",
      "|    ep_true_rew_mean | 70.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010559019 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.7        |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=35.80 +/- 60.80\n",
      "Episode length: 14.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | 35.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.7      |\n",
      "|    ep_rew_mean      | -0.402   |\n",
      "|    ep_true_rew_mean | 82.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011429502 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "execution time: 196.20352602005005; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a59b3845d47bba7db6281b1c2bf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -0.304   |\n",
      "| time/               |          |\n",
      "|    fps              | 209      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010966543 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.338      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0766      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 0.806    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008965802 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.00891    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0736      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | -2.24    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007021368 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0203      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00923    |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 0.17     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=23.80 +/- 59.78\n",
      "Episode length: 16.20 +/- 10.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 23.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009697773 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.0584     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 2.38     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 23.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012329941 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0409      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.276       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | 20.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013278954 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.096       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 34.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013370689 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0991      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-12.90 +/- 36.30\n",
      "Episode length: 22.90 +/- 6.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.9     |\n",
      "|    mean_reward     | -12.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 41.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015033066 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | -1.04    |\n",
      "|    ep_true_rew_mean | 54.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014917525 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.231       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.21        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.781   |\n",
      "|    ep_true_rew_mean | 77.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=47.60 +/- 59.29\n",
      "Episode length: 12.40 +/- 10.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | 47.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017819565 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0454      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=84.90 +/- 36.68\n",
      "Episode length: 5.10 +/- 6.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 84.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.37     |\n",
      "|    ep_rew_mean      | -0.553   |\n",
      "|    ep_true_rew_mean | 85.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=47.90 +/- 59.54\n",
      "Episode length: 12.10 +/- 10.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.1       |\n",
      "|    mean_reward          | 47.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01705062 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.312      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0381     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.39     |\n",
      "|    ep_rew_mean      | -0.32    |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.70 +/- 1.68\n",
      "Episode length: 3.30 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026994333 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0508     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.0497      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.68     |\n",
      "|    ep_rew_mean      | -0.235   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022677876 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0777     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.0295      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49     |\n",
      "|    ep_rew_mean      | -0.153   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018168245 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.07       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 1.40\n",
      "Episode length: 4.20 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019112997 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.00752     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0945  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011247711 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0584     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    value_loss           | 0.00246     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0819  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006085267 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0941  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.00 +/- 0.77\n",
      "Episode length: 4.00 +/- 0.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005705019 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00247    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.00093     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.90 +/- 1.64\n",
      "Episode length: 3.10 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0737  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.70 +/- 0.64\n",
      "Episode length: 3.30 +/- 0.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073323213 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.016       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.00093      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.69     |\n",
      "|    ep_rew_mean      | -0.0842  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01337189 |\n",
      "|    clip_fraction        | 0.046      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650b42e5449b4a3fb734cc9e5f921926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 214      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010594258 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | -0.00591    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00465     |\n",
      "|    value_loss           | 0.0899      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -1.96    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01826163 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.698     |\n",
      "|    explained_variance   | 0.274      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0696     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.000257   |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -22.5    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009415155 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00174     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -9.14    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011899307 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.718      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.8     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 6.25     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=24.50 +/- 60.63\n",
      "Episode length: 15.50 +/- 11.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.5         |\n",
      "|    mean_reward          | 24.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114778755 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.744       |\n",
      "|    explained_variance   | -0.000869    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 0.201        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18       |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 16       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=84.20 +/- 36.41\n",
      "Episode length: 5.80 +/- 6.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.8         |\n",
      "|    mean_reward          | 84.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015781716 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.747      |\n",
      "|    explained_variance   | 0.0827      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0973      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -0.921   |\n",
      "|    ep_true_rew_mean | 41.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.6         |\n",
      "|    mean_reward          | 84.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019198336 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.693      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0934      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=85.00 +/- 36.68\n",
      "Episode length: 5.00 +/- 6.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 85       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.97     |\n",
      "|    ep_rew_mean      | -0.539   |\n",
      "|    ep_true_rew_mean | 68       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.60 +/- 36.56\n",
      "Episode length: 5.40 +/- 6.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016459627 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.446       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=60.70 +/- 56.11\n",
      "Episode length: 9.30 +/- 10.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.01     |\n",
      "|    ep_rew_mean      | -0.533   |\n",
      "|    ep_true_rew_mean | 68       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.9       |\n",
      "|    mean_reward          | 48.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00976419 |\n",
      "|    clip_fraction        | 0.0849     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.559     |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0621     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0058    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=60.90 +/- 56.25\n",
      "Episode length: 9.10 +/- 10.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.1      |\n",
      "|    mean_reward     | 60.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.77     |\n",
      "|    ep_rew_mean      | -0.433   |\n",
      "|    ep_true_rew_mean | 76.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005582652 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 48.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.77     |\n",
      "|    ep_rew_mean      | -0.346   |\n",
      "|    ep_true_rew_mean | 82.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=85.20 +/- 36.75\n",
      "Episode length: 4.80 +/- 6.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 85.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010314617 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.74     |\n",
      "|    ep_rew_mean      | -0.316   |\n",
      "|    ep_true_rew_mean | 88.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=84.80 +/- 36.62\n",
      "Episode length: 5.20 +/- 6.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.2         |\n",
      "|    mean_reward          | 84.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008014014 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0316      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.0947      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.18     |\n",
      "|    ep_rew_mean      | -0.216   |\n",
      "|    ep_true_rew_mean | 92.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=85.20 +/- 36.74\n",
      "Episode length: 4.80 +/- 6.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 85.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017667169 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.0632      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | 59.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.74     |\n",
      "|    ep_rew_mean      | -0.249   |\n",
      "|    ep_true_rew_mean | 92.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=85.10 +/- 36.72\n",
      "Episode length: 4.90 +/- 6.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 85.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039782155 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.014      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.0618      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.3      |\n",
      "|    ep_rew_mean      | -0.209   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.40 +/- 1.02\n",
      "Episode length: 2.60 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.6         |\n",
      "|    mean_reward          | 97.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027383484 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0572     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.0323      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.1     |\n",
      "|    mean_reward     | 47.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.44     |\n",
      "|    ep_rew_mean      | -0.219   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.90 +/- 0.83\n",
      "Episode length: 3.10 +/- 0.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078355566 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.0304      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.28     |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 0.80\n",
      "Episode length: 3.60 +/- 0.80\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.6       |\n",
      "|    mean_reward          | 96.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1093609 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.301    |\n",
      "|    explained_variance   | 0.236     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0659   |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0461   |\n",
      "|    value_loss           | 0.0146    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.91     |\n",
      "|    ep_rew_mean      | -0.102   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=84.00 +/- 36.35\n",
      "Episode length: 6.00 +/- 6.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6          |\n",
      "|    mean_reward          | 84         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05625605 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.299     |\n",
      "|    explained_variance   | 0.528      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.0042     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=84.50 +/- 36.51\n",
      "Episode length: 5.50 +/- 6.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | 84.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.32     |\n",
      "|    ep_rew_mean      | -0.207   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=97.30 +/- 1.49\n",
      "Episode length: 2.70 +/- 1.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.7        |\n",
      "|    mean_reward          | 97.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03855373 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.314     |\n",
      "|    explained_variance   | 0.237      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0542    |\n",
      "|    value_loss           | 0.0186     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08551108 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.197     |\n",
      "|    explained_variance   | 0.306      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.0057     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "execution time: 229.15802788734436; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98306007777b401698740fcd1b1a4026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 9.8      |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011959759 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.017       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0892      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.79    |\n",
      "|    ep_true_rew_mean | -2.53    |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01204084 |\n",
      "|    clip_fraction        | 0.0913     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | -0.0427    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0626     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -11.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011953685 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0308      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0971      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.248       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -2.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009066526 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0987      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.292       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 12.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013244663 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0275      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.086       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 18.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012142137 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.0633      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0957      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.249       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=24.50 +/- 60.63\n",
      "Episode length: 15.50 +/- 11.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.5     |\n",
      "|    mean_reward     | 24.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 35.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.9        |\n",
      "|    mean_reward          | 24.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015380228 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0798      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 51.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=85.00 +/- 36.69\n",
      "Episode length: 5.00 +/- 6.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 85         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01758133 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0804     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    value_loss           | 0.222      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=35.00 +/- 60.00\n",
      "Episode length: 15.00 +/- 10.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | 35       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | -0.726   |\n",
      "|    ep_true_rew_mean | 77.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=59.30 +/- 55.19\n",
      "Episode length: 10.70 +/- 9.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.7        |\n",
      "|    mean_reward          | 59.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022152968 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0413      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.82     |\n",
      "|    ep_rew_mean      | -0.533   |\n",
      "|    ep_true_rew_mean | 88.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=83.60 +/- 36.21\n",
      "Episode length: 6.40 +/- 6.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | 83.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024700092 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=84.60 +/- 36.56\n",
      "Episode length: 5.40 +/- 6.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 84.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.85     |\n",
      "|    ep_rew_mean      | -0.364   |\n",
      "|    ep_true_rew_mean | 92.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025114676 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.864      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0561     |\n",
      "|    value_loss           | 0.0608      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.02     |\n",
      "|    ep_rew_mean      | -0.261   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.70 +/- 1.62\n",
      "Episode length: 4.30 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041809797 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 0.0266      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.71     |\n",
      "|    ep_rew_mean      | -0.168   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021601763 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0758     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.0116      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.90 +/- 1.51\n",
      "Episode length: 3.10 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.45\n",
      "Episode length: 3.90 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012855755 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.0069      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 1.62\n",
      "Episode length: 3.70 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.9      |\n",
      "|    ep_rew_mean      | -0.115   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.30 +/- 0.64\n",
      "Episode length: 3.70 +/- 0.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00793777 |\n",
      "|    clip_fraction        | 0.0502     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.321     |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0274    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    value_loss           | 0.00402    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.70 +/- 0.78\n",
      "Episode length: 3.30 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.1      |\n",
      "|    ep_rew_mean      | -0.117   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.70 +/- 1.55\n",
      "Episode length: 3.30 +/- 1.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053781304 |\n",
      "|    clip_fraction        | 0.0549       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.581        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0316      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0251      |\n",
      "|    value_loss           | 0.00292      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.87     |\n",
      "|    ep_rew_mean      | -0.096   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013377713 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.96     |\n",
      "|    ep_rew_mean      | -0.117   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081191584 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.218      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0552     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 0.00671     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.60 +/- 0.92\n",
      "Episode length: 3.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.101   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.00 +/- 1.61\n",
      "Episode length: 4.00 +/- 1.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033837683 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0f71c9ce134c3cb679d816c32191e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010571448 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.029       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.000362    |\n",
      "|    value_loss           | 0.0894      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -22      |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009731787 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -21.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013798807 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0899      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.000673    |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -18.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.8         |\n",
      "|    mean_reward          | -12.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072229705 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1           |\n",
      "|    explained_variance   | 0.118        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0706       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 0.187        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -12.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009048271 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | -10.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010439508 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00897    |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.41    |\n",
      "|    ep_true_rew_mean | 9.37     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=73.20 +/- 49.10\n",
      "Episode length: 6.80 +/- 9.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | 73.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009908145 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0795      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.1     |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    ep_true_rew_mean | 11.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037119286 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.998       |\n",
      "|    explained_variance   | 0.198        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0998       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 0.181        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.41    |\n",
      "|    ep_true_rew_mean | 8.15     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053831465 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.95        |\n",
      "|    explained_variance   | 0.262        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0906       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 6.95     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16         |\n",
      "|    mean_reward          | 24         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00685062 |\n",
      "|    clip_fraction        | 0.0411     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.965     |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | 0.000148   |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.52    |\n",
      "|    ep_true_rew_mean | 0.84     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16           |\n",
      "|    mean_reward          | 24           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057844175 |\n",
      "|    clip_fraction        | 0.0659       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.949       |\n",
      "|    explained_variance   | 0.187        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 0.185        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | -4.08    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=24.50 +/- 60.63\n",
      "Episode length: 15.50 +/- 11.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.5        |\n",
      "|    mean_reward          | 24.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007906311 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000326   |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "execution time: 181.3040897846222; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754327eb38144a2db41ea0b8aa2c2226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -0.356   |\n",
      "| time/               |          |\n",
      "|    fps              | 212      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008913612 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.057      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0238      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 0.778    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011807935 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0362     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0795      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -0.48    |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.90 +/- 48.20\n",
      "Episode length: 20.90 +/- 8.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.9        |\n",
      "|    mean_reward          | -0.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011671597 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0652      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0699      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=11.30 +/- 55.45\n",
      "Episode length: 18.70 +/- 9.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.7     |\n",
      "|    mean_reward     | 11.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | 3.79     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010397477 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0449      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 14.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.9         |\n",
      "|    mean_reward          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093713235 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0.122        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0981       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 0.253        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=35.50 +/- 60.50\n",
      "Episode length: 14.50 +/- 10.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.5     |\n",
      "|    mean_reward     | 35.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | -1.52    |\n",
      "|    ep_true_rew_mean | 19.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=36.30 +/- 61.31\n",
      "Episode length: 13.70 +/- 11.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013969269 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0225      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0701      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.34    |\n",
      "|    ep_true_rew_mean | 34.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=71.90 +/- 48.47\n",
      "Episode length: 8.10 +/- 8.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.1        |\n",
      "|    mean_reward          | 71.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01543959 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.167      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | -1.19    |\n",
      "|    ep_true_rew_mean | 47.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.20 +/- 55.79\n",
      "Episode length: 9.80 +/- 10.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016601225 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=72.60 +/- 48.82\n",
      "Episode length: 7.40 +/- 8.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | 72.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.5     |\n",
      "|    ep_rew_mean      | -0.854   |\n",
      "|    ep_true_rew_mean | 73.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019091088 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=95.20 +/- 0.75\n",
      "Episode length: 4.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.84     |\n",
      "|    ep_rew_mean      | -0.509   |\n",
      "|    ep_true_rew_mean | 85.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02574427 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0345     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 1.81\n",
      "Episode length: 3.90 +/- 1.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.66     |\n",
      "|    ep_rew_mean      | -0.333   |\n",
      "|    ep_true_rew_mean | 93.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028216314 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.849      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0538     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.07     |\n",
      "|    ep_rew_mean      | -0.215   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035086058 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0478     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0487     |\n",
      "|    value_loss           | 0.0237      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.173   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020651404 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0386     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.00885     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.105   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014472457 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.056      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=96.90 +/- 1.04\n",
      "Episode length: 3.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.89     |\n",
      "|    ep_rew_mean      | -0.113   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 0.81\n",
      "Episode length: 3.50 +/- 0.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071969572 |\n",
      "|    clip_fraction        | 0.0682       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0462      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    value_loss           | 0.00198      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.30 +/- 1.55\n",
      "Episode length: 3.70 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.67     |\n",
      "|    ep_rew_mean      | -0.0822  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.60 +/- 0.80\n",
      "Episode length: 3.40 +/- 0.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048820246 |\n",
      "|    clip_fraction        | 0.0574       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.196       |\n",
      "|    explained_variance   | 0.662        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0259      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.025       |\n",
      "|    value_loss           | 0.00177      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.97     |\n",
      "|    ep_rew_mean      | -0.101   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003370625 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.179      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.000812    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=97.00 +/- 1.48\n",
      "Episode length: 3.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.69     |\n",
      "|    ep_rew_mean      | -0.0832  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.50 +/- 1.36\n",
      "Episode length: 4.50 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.5          |\n",
      "|    mean_reward          | 95.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048884065 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.15        |\n",
      "|    explained_variance   | 0.713        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0199      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0151      |\n",
      "|    value_loss           | 0.0012       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.50 +/- 1.75\n",
      "Episode length: 3.50 +/- 1.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.61     |\n",
      "|    ep_rew_mean      | -0.092   |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.90 +/- 1.37\n",
      "Episode length: 3.10 +/- 1.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.1          |\n",
      "|    mean_reward          | 96.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031784258 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0359      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0196      |\n",
      "|    value_loss           | 0.000947     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=97.40 +/- 1.36\n",
      "Episode length: 2.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.6      |\n",
      "|    mean_reward     | 97.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.65     |\n",
      "|    ep_rew_mean      | -0.0835  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067590727 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0291      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.000581     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.20 +/- 1.72\n",
      "Episode length: 3.80 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94     |\n",
      "|    ep_rew_mean      | -0.0957  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003707316 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.127      |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0837  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.3          |\n",
      "|    mean_reward          | 96.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075151525 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0181       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00905     |\n",
      "|    value_loss           | 0.000421     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.50 +/- 0.92\n",
      "Episode length: 3.50 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.65     |\n",
      "|    ep_rew_mean      | -0.0739  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003865202 |\n",
      "|    clip_fraction        | 0.0427      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0908     |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00107     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.00027     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0779  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010031544 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0876     |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00694    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.000594    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674059d8d4a64dce9fb20c42d539bd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -19.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 210      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037796892 |\n",
      "|    clip_fraction        | 0.0853      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | -0.00382    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    value_loss           | 0.0932      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | -1.95    |\n",
      "|    ep_true_rew_mean | -20.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007990433 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -17.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 12          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014770019 |\n",
      "|    clip_fraction        | 0.0734      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.355      |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -12.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004461765 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000638   |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | -7.95    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 17.9         |\n",
      "|    mean_reward          | 12.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057667373 |\n",
      "|    clip_fraction        | 0.0868       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.0935       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 0.233        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | -4.08    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.7         |\n",
      "|    mean_reward          | -12.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038253942 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.0817       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.145        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 0.237        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.55    |\n",
      "|    ep_true_rew_mean | 4.41     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010614145 |\n",
      "|    clip_fraction        | 0.0786      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | -1.45    |\n",
      "|    ep_true_rew_mean | 10.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013174181 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.27    |\n",
      "|    ep_true_rew_mean | 27.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015003805 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | -1.04    |\n",
      "|    ep_true_rew_mean | 45.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.7       |\n",
      "|    mean_reward          | 24.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02035772 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.589     |\n",
      "|    explained_variance   | 0.195      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0967     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.229      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.9     |\n",
      "|    mean_reward     | 12.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.4      |\n",
      "|    ep_rew_mean      | -0.568   |\n",
      "|    ep_true_rew_mean | 79.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01658469 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.566     |\n",
      "|    explained_variance   | 0.19       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0739     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0465    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.38     |\n",
      "|    ep_rew_mean      | -0.456   |\n",
      "|    ep_true_rew_mean | 89.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.80 +/- 1.54\n",
      "Episode length: 3.20 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017595902 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=84.20 +/- 36.42\n",
      "Episode length: 5.80 +/- 6.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 84.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.69     |\n",
      "|    ep_rew_mean      | -0.236   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041159473 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.00544     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 0.0455      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=97.10 +/- 1.64\n",
      "Episode length: 2.90 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | -0.16    |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069691435 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0486     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0525     |\n",
      "|    value_loss           | 0.0193      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.80 +/- 2.04\n",
      "Episode length: 4.20 +/- 2.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.2        |\n",
      "|    mean_reward          | 95.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05653529 |\n",
      "|    clip_fraction        | 0.0782     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.147     |\n",
      "|    explained_variance   | 0.431      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.00497    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82     |\n",
      "|    ep_rew_mean      | -0.0946  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006216242 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.10 +/- 0.94\n",
      "Episode length: 2.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15     |\n",
      "|    ep_rew_mean      | -0.0975  |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.80 +/- 1.40\n",
      "Episode length: 3.20 +/- 1.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037676687 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0835      |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0275      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 0.00128      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.90 +/- 1.51\n",
      "Episode length: 3.10 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.96     |\n",
      "|    ep_rew_mean      | -0.0875  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.30 +/- 1.49\n",
      "Episode length: 4.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016602583 |\n",
      "|    clip_fraction        | 0.018       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0719     |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.000393    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0789  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.20 +/- 1.78\n",
      "Episode length: 3.80 +/- 1.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013373693 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0584      |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0225      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    value_loss           | 0.000561     |\n",
      "------------------------------------------\n",
      "execution time: 249.06504583358765; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bc95776b1341c69535aff4ea7475a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -0.25    |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076708733 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.24        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0227       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.101        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -2.62    |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010749405 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0326      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | 0.75     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010214635 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0213      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0972      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.7     |\n",
      "|    ep_true_rew_mean | 8.78     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010417606 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0631      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.238       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | 24       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.51    |\n",
      "|    ep_true_rew_mean | 18.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012471382 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.80 +/- 56.22\n",
      "Episode length: 18.20 +/- 10.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 30.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=71.00 +/- 48.00\n",
      "Episode length: 9.00 +/- 8.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | 71          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011437295 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | -1.13    |\n",
      "|    ep_true_rew_mean | 48.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.1        |\n",
      "|    mean_reward          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013105914 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0792      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=59.90 +/- 55.60\n",
      "Episode length: 10.10 +/- 9.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | -0.791   |\n",
      "|    ep_true_rew_mean | 73       |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.20 +/- 48.62\n",
      "Episode length: 7.80 +/- 8.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.8        |\n",
      "|    mean_reward          | 72.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01721147 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0.301      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0367     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0451    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.05     |\n",
      "|    ep_rew_mean      | -0.561   |\n",
      "|    ep_true_rew_mean | 87       |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=71.70 +/- 48.36\n",
      "Episode length: 8.30 +/- 8.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.3         |\n",
      "|    mean_reward          | 71.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023707563 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0058      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=84.40 +/- 36.48\n",
      "Episode length: 5.60 +/- 6.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.51     |\n",
      "|    ep_rew_mean      | -0.482   |\n",
      "|    ep_true_rew_mean | 89.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.90 +/- 0.94\n",
      "Episode length: 4.10 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021969326 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.0922      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=84.10 +/- 36.38\n",
      "Episode length: 5.90 +/- 6.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.9      |\n",
      "|    mean_reward     | 84.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.9      |\n",
      "|    ep_rew_mean      | -0.262   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029643796 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 0.0312      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=71.80 +/- 48.41\n",
      "Episode length: 8.20 +/- 8.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 71.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.28     |\n",
      "|    ep_rew_mean      | -0.226   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=84.60 +/- 36.55\n",
      "Episode length: 5.40 +/- 6.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | 84.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019055326 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.0164      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=84.70 +/- 36.60\n",
      "Episode length: 5.30 +/- 6.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 84.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01151196 |\n",
      "|    clip_fraction        | 0.0902     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.434     |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.069     |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    value_loss           | 0.00613    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.124   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.50 +/- 1.28\n",
      "Episode length: 3.50 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077604265 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0485      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0341      |\n",
      "|    value_loss           | 0.00476      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061856387 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0443      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0228      |\n",
      "|    value_loss           | 0.00249      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.20 +/- 0.75\n",
      "Episode length: 3.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.0966  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073637674 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0413      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0235      |\n",
      "|    value_loss           | 0.00122      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.00 +/- 1.41\n",
      "Episode length: 3.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.86     |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010536501 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.10 +/- 0.70\n",
      "Episode length: 3.90 +/- 0.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.102   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021626696 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.000963    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.68     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.30 +/- 0.78\n",
      "Episode length: 3.70 +/- 0.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077375166 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0587     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 0.00884     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0954  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27235875 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.184     |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0324    |\n",
      "|    value_loss           | 0.0022     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | -0.723   |\n",
      "|    ep_true_rew_mean | 79.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 118      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038377404 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.0651      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0489     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f830e74dfd4f938e43af0958c311c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -19.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 212      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087061785 |\n",
      "|    clip_fraction        | 0.161        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | -0.033       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0636       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00312      |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -20.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009065597 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -16.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013266898 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0953      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | -1.59    |\n",
      "|    ep_true_rew_mean | -2.86    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008446343 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00704    |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.57    |\n",
      "|    ep_true_rew_mean | -1.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.2         |\n",
      "|    mean_reward          | 11.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028791965 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.532       |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0849       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 0.173        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.56    |\n",
      "|    ep_true_rew_mean | -1.63    |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16.1         |\n",
      "|    mean_reward          | 23.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068776356 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0975       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 6.98     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007256135 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.066       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00321    |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 20.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009253388 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0995      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=36.50 +/- 61.50\n",
      "Episode length: 13.50 +/- 11.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.5     |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.7     |\n",
      "|    ep_rew_mean      | -1.21    |\n",
      "|    ep_true_rew_mean | 20.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.70 +/- 60.18\n",
      "Episode length: 11.30 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.3        |\n",
      "|    mean_reward          | 48.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005807547 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.207       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00744    |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | -0.99    |\n",
      "|    ep_true_rew_mean | 33.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.2          |\n",
      "|    mean_reward          | 60.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023967682 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.546       |\n",
      "|    explained_variance   | 0.333        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.082        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000858    |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.5     |\n",
      "|    ep_rew_mean      | -0.779   |\n",
      "|    ep_true_rew_mean | 48.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=48.40 +/- 59.93\n",
      "Episode length: 11.60 +/- 10.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004871684 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00423    |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.936   |\n",
      "|    ep_true_rew_mean | 38.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.7         |\n",
      "|    mean_reward          | 36.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037310321 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0763       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 0.153        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.3     |\n",
      "|    ep_rew_mean      | -0.847   |\n",
      "|    ep_true_rew_mean | 44.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.2          |\n",
      "|    mean_reward          | 60.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035848815 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.478       |\n",
      "|    explained_variance   | 0.433        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0737       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 0.161        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=61.00 +/- 56.30\n",
      "Episode length: 9.00 +/- 10.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 61       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | -0.698   |\n",
      "|    ep_true_rew_mean | 54.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=36.40 +/- 61.41\n",
      "Episode length: 13.60 +/- 11.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.6         |\n",
      "|    mean_reward          | 36.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071375957 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.339        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0652       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    value_loss           | 0.184        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=48.20 +/- 59.78\n",
      "Episode length: 11.80 +/- 10.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 48.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.85     |\n",
      "|    ep_rew_mean      | -0.629   |\n",
      "|    ep_true_rew_mean | 58.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=85.20 +/- 36.74\n",
      "Episode length: 4.80 +/- 6.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 85.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004578774 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0817      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00799    |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=84.80 +/- 36.62\n",
      "Episode length: 5.20 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.94     |\n",
      "|    ep_rew_mean      | -0.552   |\n",
      "|    ep_true_rew_mean | 64.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.1         |\n",
      "|    mean_reward          | 47.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025006793 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0646       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    value_loss           | 0.166        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=85.20 +/- 36.75\n",
      "Episode length: 4.80 +/- 6.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 85.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.82     |\n",
      "|    ep_rew_mean      | -0.612   |\n",
      "|    ep_true_rew_mean | 59.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=72.80 +/- 48.91\n",
      "Episode length: 7.20 +/- 8.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.2          |\n",
      "|    mean_reward          | 72.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015860235 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.49         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 0.156        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.5      |\n",
      "|    mean_reward     | 60.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.69     |\n",
      "|    ep_rew_mean      | -0.609   |\n",
      "|    ep_true_rew_mean | 60.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 72.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045124413 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.515        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=73.00 +/- 49.00\n",
      "Episode length: 7.00 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 73       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.13     |\n",
      "|    ep_rew_mean      | -0.559   |\n",
      "|    ep_true_rew_mean | 63.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.4         |\n",
      "|    mean_reward          | 60.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005604165 |\n",
      "|    clip_fraction        | 0.0258      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0942      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00269    |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=97.10 +/- 1.22\n",
      "Episode length: 2.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | -0.667   |\n",
      "|    ep_true_rew_mean | 55.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=72.90 +/- 48.96\n",
      "Episode length: 7.10 +/- 9.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.1        |\n",
      "|    mean_reward          | 72.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00670046 |\n",
      "|    clip_fraction        | 0.0408     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.421     |\n",
      "|    explained_variance   | 0.463      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0982     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=84.80 +/- 36.61\n",
      "Episode length: 5.20 +/- 6.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=60.20 +/- 55.78\n",
      "Episode length: 9.80 +/- 9.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.27     |\n",
      "|    ep_rew_mean      | -0.483   |\n",
      "|    ep_true_rew_mean | 68.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.9        |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004359735 |\n",
      "|    clip_fraction        | 0.0112      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.5      |\n",
      "|    ep_rew_mean      | -0.423   |\n",
      "|    ep_true_rew_mean | 72.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=48.80 +/- 60.26\n",
      "Episode length: 11.20 +/- 11.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.2         |\n",
      "|    mean_reward          | 48.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028914069 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.561        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00671     |\n",
      "|    value_loss           | 0.143        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=60.40 +/- 55.92\n",
      "Episode length: 9.60 +/- 10.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | -0.629   |\n",
      "|    ep_true_rew_mean | 57.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.7         |\n",
      "|    mean_reward          | 48.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061555635 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0951       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | 0.000876     |\n",
      "|    value_loss           | 0.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.32     |\n",
      "|    ep_rew_mean      | -0.563   |\n",
      "|    ep_true_rew_mean | 62.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006519907 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00533    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=72.70 +/- 48.85\n",
      "Episode length: 7.30 +/- 8.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "execution time: 267.9970979690552; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c2618f8c224d17b24a526c2a1da03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.67    |\n",
      "|    ep_true_rew_mean | 1.67     |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075453226 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.235       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00658     |\n",
      "|    value_loss           | 0.0922       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 4.85     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009623083 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0633      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 5.48     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009999621 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0416      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.135       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 3.28     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=72.40 +/- 48.72\n",
      "Episode length: 7.60 +/- 8.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | 72.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006985862 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.077       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0788      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-0.80 +/- 48.40\n",
      "Episode length: 20.80 +/- 8.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -0.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 11.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | 36.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009393169 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.36    |\n",
      "|    ep_true_rew_mean | 38.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012260274 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.08        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.222       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=48.80 +/- 60.26\n",
      "Episode length: 11.20 +/- 11.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | 48.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | -1.32    |\n",
      "|    ep_true_rew_mean | 39.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.9         |\n",
      "|    mean_reward          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0155623155 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.16         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0684       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    value_loss           | 0.202        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 50       |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015106568 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.6     |\n",
      "|    ep_rew_mean      | -0.838   |\n",
      "|    ep_true_rew_mean | 72.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018366948 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.41     |\n",
      "|    ep_rew_mean      | -0.569   |\n",
      "|    ep_true_rew_mean | 88.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=72.60 +/- 48.81\n",
      "Episode length: 7.40 +/- 8.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | 72.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02733015 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000991  |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    value_loss           | 0.0937     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=84.40 +/- 36.49\n",
      "Episode length: 5.60 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.29     |\n",
      "|    ep_rew_mean      | -0.317   |\n",
      "|    ep_true_rew_mean | 92.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024821654 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.0507      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.32     |\n",
      "|    ep_rew_mean      | -0.225   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.70 +/- 1.42\n",
      "Episode length: 4.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016016012 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.71       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.0153      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.70 +/- 1.42\n",
      "Episode length: 4.30 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.27     |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014156512 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0676     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.60 +/- 1.11\n",
      "Episode length: 4.40 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024518397 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0465     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.00868     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.20 +/- 0.98\n",
      "Episode length: 2.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010061303 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0671     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0939  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011674585 |\n",
      "|    clip_fraction        | 0.0776      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00966    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.70 +/- 0.78\n",
      "Episode length: 4.30 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.74     |\n",
      "|    ep_rew_mean      | -0.0937  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.90 +/- 0.94\n",
      "Episode length: 3.10 +/- 0.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006273205 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.70 +/- 0.78\n",
      "Episode length: 3.30 +/- 0.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0939  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 102      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004708143 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | -0.0859  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004386502 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.127      |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.10 +/- 1.30\n",
      "Episode length: 3.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.67     |\n",
      "|    ep_rew_mean      | -0.0914  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032614314 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0151      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00653     |\n",
      "|    value_loss           | 0.000467     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6c6d142bbb4bebae9e955536bf2697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -16.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 202      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00563607 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.82      |\n",
      "|    explained_variance   | 0.0563     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0288     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.00566    |\n",
      "|    value_loss           | 0.0889     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -17.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008906969 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.000994    |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -16.4    |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067499615 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.841       |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0923       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000419     |\n",
      "|    value_loss           | 0.178        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | -4.01    |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.5        |\n",
      "|    mean_reward          | -0.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009323565 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.823      |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0784      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00793    |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=24.30 +/- 60.38\n",
      "Episode length: 15.70 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.7     |\n",
      "|    mean_reward     | 24.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 5.79     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.60 +/- 61.60\n",
      "Episode length: 13.40 +/- 11.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.4         |\n",
      "|    mean_reward          | 36.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048076273 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.769       |\n",
      "|    explained_variance   | 0.176        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 0.189        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    ep_true_rew_mean | -0.65    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 12          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007419293 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | -4.31    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004897684 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0826      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00217     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 5.71     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.6         |\n",
      "|    mean_reward          | 24.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062349075 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.736       |\n",
      "|    explained_variance   | 0.234        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0589       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00696     |\n",
      "|    value_loss           | 0.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=36.50 +/- 61.50\n",
      "Episode length: 13.50 +/- 11.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.5     |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 8.17     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008047424 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.0004      |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | -1.49    |\n",
      "|    ep_true_rew_mean | 3.27     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004626963 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "execution time: 181.6361300945282; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-84\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56fa6653bee4f92afdeb79af3bdfa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -9.49    |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012685563 |\n",
      "|    clip_fraction        | 0.0533      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.168      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 2.74     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013997011 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.00301    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.144       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.8     |\n",
      "|    ep_true_rew_mean | 4.22     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009181462 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.00813     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -0.54    |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008060211 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0215      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0892      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.291       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | 5.92     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.1       |\n",
      "|    mean_reward          | 23.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01206127 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.52      |\n",
      "|    explained_variance   | 0.0225     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.152      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    value_loss           | 0.308      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.9     |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 22.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015359096 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0922      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 44.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015070701 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.40 +/- 48.72\n",
      "Episode length: 7.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | -0.967   |\n",
      "|    ep_true_rew_mean | 65.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.20 +/- 48.62\n",
      "Episode length: 7.80 +/- 8.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 72.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015593532 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.87     |\n",
      "|    ep_rew_mean      | -0.607   |\n",
      "|    ep_true_rew_mean | 83.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=84.30 +/- 36.46\n",
      "Episode length: 5.70 +/- 6.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016939878 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=60.10 +/- 55.73\n",
      "Episode length: 9.90 +/- 9.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.11     |\n",
      "|    ep_rew_mean      | -0.382   |\n",
      "|    ep_true_rew_mean | 91.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.60 +/- 1.11\n",
      "Episode length: 3.40 +/- 1.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | 96.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02381567 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.969     |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0484    |\n",
      "|    value_loss           | 0.0778     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=60.10 +/- 55.71\n",
      "Episode length: 9.90 +/- 9.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.6      |\n",
      "|    ep_rew_mean      | -0.247   |\n",
      "|    ep_true_rew_mean | 93.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025443964 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 0.0426      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.01     |\n",
      "|    ep_rew_mean      | -0.192   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.58\n",
      "Episode length: 4.10 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016020205 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.651      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.0228      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 0.63\n",
      "Episode length: 4.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89     |\n",
      "|    ep_rew_mean      | -0.189   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.00 +/- 0.77\n",
      "Episode length: 4.00 +/- 0.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016928744 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0672     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0496     |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.118   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.35\n",
      "Episode length: 3.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008454461 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0436     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0951  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041512037 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.235       |\n",
      "|    explained_variance   | 0.598        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0244      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0222      |\n",
      "|    value_loss           | 0.00269      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.70 +/- 1.10\n",
      "Episode length: 3.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.0867  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031446046 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00149      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    value_loss           | 0.00177      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0872  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=97.00 +/- 1.84\n",
      "Episode length: 3.00 +/- 1.84\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3          |\n",
      "|    mean_reward          | 97         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00404956 |\n",
      "|    clip_fraction        | 0.0145     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.138     |\n",
      "|    explained_variance   | 0.795      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0185    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    value_loss           | 0.000834   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=95.80 +/- 0.87\n",
      "Episode length: 4.20 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0768  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019865877 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0198      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0137      |\n",
      "|    value_loss           | 0.000963     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=97.00 +/- 1.34\n",
      "Episode length: 3.00 +/- 1.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.82     |\n",
      "|    ep_rew_mean      | -0.0864  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031522429 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0124      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 0.000671     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | -0.079   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.8        |\n",
      "|    mean_reward          | 96.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00446797 |\n",
      "|    clip_fraction        | 0.0406     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00359   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 0.000578   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=97.10 +/- 0.94\n",
      "Episode length: 2.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=21500, episode_reward=97.00 +/- 1.67\n",
      "Episode length: 3.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.62     |\n",
      "|    ep_rew_mean      | -0.0741  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.9        |\n",
      "|    mean_reward          | 36.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101753384 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0564     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.000358    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=35.90 +/- 60.90\n",
      "Episode length: 14.10 +/- 10.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.1     |\n",
      "|    mean_reward     | 35.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.43     |\n",
      "|    ep_rew_mean      | -0.204   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.7        |\n",
      "|    mean_reward          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11898331 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.306     |\n",
      "|    explained_variance   | 0.0529     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0619    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.0189     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.102   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33533472 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.263     |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0576    |\n",
      "|    value_loss           | 0.00237    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | -0.781   |\n",
      "|    ep_true_rew_mean | 80.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02645668 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.453     |\n",
      "|    explained_variance   | 0.0201     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.7      |\n",
      "|    ep_rew_mean      | -0.454   |\n",
      "|    ep_true_rew_mean | 91.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016064001 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.0448      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0094      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112edfc6e1974f529c2b8ec480dab035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.01    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 208      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027942338 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.308       |\n",
      "|    explained_variance   | -0.0682      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0749       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 0.158        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | -1.95    |\n",
      "|    ep_true_rew_mean | -20.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003922286 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -15.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004202272 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.0899      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00672    |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -10.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004209937 |\n",
      "|    clip_fraction        | 0.0504      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.0891      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0991      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    value_loss           | 0.222       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | -7.77    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.3         |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041146046 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.272       |\n",
      "|    explained_variance   | 0.0564       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0775       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00723     |\n",
      "|    value_loss           | 0.234        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | 24.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 0.76     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.3         |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041168947 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.0818       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.132        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.35    |\n",
      "|    ep_true_rew_mean | 17.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | 12          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006372772 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    value_loss           | 0.224       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | -1.18    |\n",
      "|    ep_true_rew_mean | 28.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18           |\n",
      "|    mean_reward          | 12           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041706637 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.211        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00529     |\n",
      "|    value_loss           | 0.206        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    ep_true_rew_mean | 29.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.6         |\n",
      "|    mean_reward          | 36.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063066576 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.294       |\n",
      "|    explained_variance   | 0.209        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0987       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00848     |\n",
      "|    value_loss           | 0.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.914   |\n",
      "|    ep_true_rew_mean | 47       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=36.30 +/- 61.30\n",
      "Episode length: 13.70 +/- 11.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.7        |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027739363 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0701      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=48.80 +/- 60.26\n",
      "Episode length: 11.20 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | 48.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | -0.894   |\n",
      "|    ep_true_rew_mean | 47.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 23.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012128795 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.742   |\n",
      "|    ep_true_rew_mean | 60.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=36.20 +/- 61.20\n",
      "Episode length: 13.80 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026827067 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=60.60 +/- 56.04\n",
      "Episode length: 9.40 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.14     |\n",
      "|    ep_rew_mean      | -0.369   |\n",
      "|    ep_true_rew_mean | 87.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=24.00 +/- 60.01\n",
      "Episode length: 16.00 +/- 11.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | 24          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036042206 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0078      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=60.90 +/- 56.24\n",
      "Episode length: 9.10 +/- 10.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.1      |\n",
      "|    mean_reward     | 60.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.75     |\n",
      "|    ep_rew_mean      | -0.496   |\n",
      "|    ep_true_rew_mean | 85.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | 60.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013096628 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.1     |\n",
      "|    mean_reward     | 23.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.61     |\n",
      "|    ep_rew_mean      | -0.396   |\n",
      "|    ep_true_rew_mean | 91.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02850667 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.195      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0582    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=97.10 +/- 1.51\n",
      "Episode length: 2.90 +/- 1.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.3      |\n",
      "|    ep_rew_mean      | -0.203   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.1        |\n",
      "|    mean_reward          | 96.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05011835 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.33      |\n",
      "|    explained_variance   | 0.137      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0473    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0513    |\n",
      "|    value_loss           | 0.0384     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94     |\n",
      "|    ep_rew_mean      | -0.104   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02179585 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | -0.147     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0522    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.00663    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0866  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23352101 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0.438      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.00317    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.26     |\n",
      "|    ep_rew_mean      | -0.407   |\n",
      "|    ep_true_rew_mean | 91.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.20 +/- 1.33\n",
      "Episode length: 3.80 +/- 1.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019497713 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | -0.00547    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00801    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.075       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.05     |\n",
      "|    ep_rew_mean      | -0.226   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13035998 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.193     |\n",
      "|    explained_variance   | 0.0182     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.046      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "execution time: 265.202406167984; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7959b21fdb47b5907490765b85f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | -16.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 228      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00931151 |\n",
      "|    clip_fraction        | 0.0398     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -0.224     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.09       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00496   |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -8.37    |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008183045 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0114      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0874      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00922    |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -0.44    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013360145 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0377      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 7.64     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011335324 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0774      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 14.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=59.90 +/- 55.58\n",
      "Episode length: 10.10 +/- 9.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.1        |\n",
      "|    mean_reward          | 59.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009981716 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0805      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=73.00 +/- 49.00\n",
      "Episode length: 7.00 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | 73       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.47    |\n",
      "|    ep_true_rew_mean | 29.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.4        |\n",
      "|    mean_reward          | 48.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015591531 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | 23.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | -1.2     |\n",
      "|    ep_true_rew_mean | 43.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014729475 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0315      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | -0.989   |\n",
      "|    ep_true_rew_mean | 60.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.9        |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018174093 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=36.00 +/- 61.01\n",
      "Episode length: 14.00 +/- 11.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | 36       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.54     |\n",
      "|    ep_rew_mean      | -0.499   |\n",
      "|    ep_true_rew_mean | 89.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=59.80 +/- 55.52\n",
      "Episode length: 10.20 +/- 9.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | 59.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019616755 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00817     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=60.30 +/- 55.86\n",
      "Episode length: 9.70 +/- 10.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.18     |\n",
      "|    ep_rew_mean      | -0.45    |\n",
      "|    ep_true_rew_mean | 90.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021484682 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 0.0835      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=84.80 +/- 36.61\n",
      "Episode length: 5.20 +/- 6.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 84.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.87     |\n",
      "|    ep_rew_mean      | -0.273   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03140165 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.796     |\n",
      "|    explained_variance   | 0.272      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0637    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.0363     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.1      |\n",
      "|    ep_rew_mean      | -0.202   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025627472 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0708     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.0132      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4      |\n",
      "|    ep_rew_mean      | -0.145   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020561537 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.0068      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.123   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010433526 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=97.00 +/- 1.61\n",
      "Episode length: 3.00 +/- 1.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.94     |\n",
      "|    ep_rew_mean      | -0.101   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.70 +/- 0.64\n",
      "Episode length: 4.30 +/- 0.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067691584 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.618        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0105      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.019       |\n",
      "|    value_loss           | 0.00203      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.50 +/- 0.92\n",
      "Episode length: 3.50 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75     |\n",
      "|    ep_rew_mean      | -0.101   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019981922 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0172      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.016       |\n",
      "|    value_loss           | 0.0013       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.104   |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.10 +/- 0.94\n",
      "Episode length: 3.90 +/- 0.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030053137 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.032       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0123      |\n",
      "|    value_loss           | 0.000483     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.7      |\n",
      "|    ep_rew_mean      | -0.0829  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024707709 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.71         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00939     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 0.00107      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.103   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.50 +/- 0.92\n",
      "Episode length: 3.50 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008380618 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0994     |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00434    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.10 +/- 0.83\n",
      "Episode length: 3.90 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.65     |\n",
      "|    ep_rew_mean      | -0.0821  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=83.70 +/- 36.25\n",
      "Episode length: 6.30 +/- 6.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.3         |\n",
      "|    mean_reward          | 83.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058547154 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.000347    |\n",
      "-----------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d6642741a84e2f9e37426115766a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -4.65    |\n",
      "| time/               |          |\n",
      "|    fps              | 216      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015242853 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | -0.00553    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.46    |\n",
      "|    ep_true_rew_mean | 24.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049892213 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.00182     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.096       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | -1.07    |\n",
      "|    ep_true_rew_mean | 56.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01529398 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.584     |\n",
      "|    explained_variance   | 0.17       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0913     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | -0.798   |\n",
      "|    ep_true_rew_mean | 77.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026283752 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.1      |\n",
      "|    ep_rew_mean      | -0.457   |\n",
      "|    ep_true_rew_mean | 91.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034171596 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 0.0801      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.22     |\n",
      "|    ep_rew_mean      | -0.309   |\n",
      "|    ep_true_rew_mean | 89.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.65\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.8        |\n",
      "|    mean_reward          | 72.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07041241 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.442     |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0135    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.0752     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=95.60 +/- 1.85\n",
      "Episode length: 4.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.3      |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=47.50 +/- 59.22\n",
      "Episode length: 12.50 +/- 10.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.5        |\n",
      "|    mean_reward          | 47.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012897126 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.368      |\n",
      "|    explained_variance   | 0.0398      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.0233      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=60.10 +/- 55.71\n",
      "Episode length: 9.90 +/- 9.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.10 +/- 1.70\n",
      "Episode length: 3.90 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013257982 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0457     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.00861     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.32     |\n",
      "|    ep_rew_mean      | -0.128   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016847165 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0761     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=95.50 +/- 1.75\n",
      "Episode length: 4.50 +/- 1.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17     |\n",
      "|    ep_rew_mean      | -0.116   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015301527 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.163      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0469     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.98     |\n",
      "|    ep_rew_mean      | -0.0963  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004887077 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | -0.089   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.90 +/- 1.92\n",
      "Episode length: 3.10 +/- 1.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005277189 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.000682    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 1.10\n",
      "Episode length: 4.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0785  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.60 +/- 0.92\n",
      "Episode length: 3.40 +/- 0.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010179414 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00305    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.71     |\n",
      "|    ep_rew_mean      | -0.0923  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.70 +/- 1.95\n",
      "Episode length: 3.30 +/- 1.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008742461 |\n",
      "|    clip_fraction        | 0.0506      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0867     |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.000714    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.42     |\n",
      "|    ep_rew_mean      | -0.074   |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.90 +/- 1.70\n",
      "Episode length: 3.10 +/- 1.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012928445 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0718     |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00752     |\n",
      "|    value_loss           | 0.00024     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.10 +/- 1.92\n",
      "Episode length: 3.90 +/- 1.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.1      |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=97.00 +/- 0.77\n",
      "Episode length: 3.00 +/- 0.77\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3         |\n",
      "|    mean_reward          | 97        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 16500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1889509 |\n",
      "|    clip_fraction        | 0.231     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.133    |\n",
      "|    explained_variance   | 0.294     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0527   |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    value_loss           | 0.00664   |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.1     |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20627141 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.21      |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.101     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | -0.663   |\n",
      "|    ep_true_rew_mean | 85.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019552458 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.00573     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0458     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.4      |\n",
      "|    ep_rew_mean      | -0.339   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037324738 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.0505      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0542     |\n",
      "|    value_loss           | 0.0749      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.96     |\n",
      "|    ep_rew_mean      | -0.205   |\n",
      "|    ep_true_rew_mean | 95       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.50 +/- 1.86\n",
      "Episode length: 4.50 +/- 1.86\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23190925 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.155     |\n",
      "|    explained_variance   | 8.08e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0623    |\n",
      "|    value_loss           | 0.03       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.26     |\n",
      "|    ep_rew_mean      | -0.132   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.30 +/- 1.49\n",
      "Episode length: 4.70 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057229124 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.183      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0468     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 0.0083      |\n",
      "-----------------------------------------\n",
      "execution time: 232.87244486808777; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b417cf30c344d291b9bbbf5a4fa1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 215      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009450117 |\n",
      "|    clip_fraction        | 0.0834      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.103      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -4.74    |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097129075 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | 0.0112       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0911       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    value_loss           | 0.229        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | 0.42     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008100269 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0105     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.253       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -2.35    |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015251609 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0398      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.119       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.263       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 8.24     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011329408 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.275       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | -1.64    |\n",
      "|    ep_true_rew_mean | 10.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15.9         |\n",
      "|    mean_reward          | 24.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127506815 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.0154       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0979       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0188      |\n",
      "|    value_loss           | 0.257        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20       |\n",
      "|    ep_rew_mean      | -1.58    |\n",
      "|    ep_true_rew_mean | 11       |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.1         |\n",
      "|    mean_reward          | 11.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078890845 |\n",
      "|    clip_fraction        | 0.0697       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00882     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 15.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18           |\n",
      "|    mean_reward          | 12           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078121847 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.0889       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 0.239        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 25.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012494966 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0749      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | -1.22    |\n",
      "|    ep_true_rew_mean | 41.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.4         |\n",
      "|    mean_reward          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126248915 |\n",
      "|    clip_fraction        | 0.167        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.267        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0236      |\n",
      "|    value_loss           | 0.182        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | -1.05    |\n",
      "|    ep_true_rew_mean | 51.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011212132 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0682      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -0.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.4     |\n",
      "|    ep_rew_mean      | -0.963   |\n",
      "|    ep_true_rew_mean | 55.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18         |\n",
      "|    mean_reward          | 12         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01592863 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.325      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0534     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | -0.823   |\n",
      "|    ep_true_rew_mean | 69       |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=48.30 +/- 59.86\n",
      "Episode length: 11.70 +/- 10.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.7        |\n",
      "|    mean_reward          | 48.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017828347 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=71.90 +/- 48.47\n",
      "Episode length: 8.10 +/- 8.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | 71.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | -0.717   |\n",
      "|    ep_true_rew_mean | 75.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=84.00 +/- 36.35\n",
      "Episode length: 6.00 +/- 6.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017190628 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=72.50 +/- 48.76\n",
      "Episode length: 7.50 +/- 8.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.9      |\n",
      "|    ep_rew_mean      | -0.447   |\n",
      "|    ep_true_rew_mean | 90.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014428878 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    value_loss           | 0.0918      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=95.80 +/- 1.54\n",
      "Episode length: 4.20 +/- 1.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.28     |\n",
      "|    ep_rew_mean      | -0.373   |\n",
      "|    ep_true_rew_mean | 91.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016284157 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.892      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.0721      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.46     |\n",
      "|    ep_rew_mean      | -0.227   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.20 +/- 1.72\n",
      "Episode length: 3.80 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018964274 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0507     |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.50 +/- 1.57\n",
      "Episode length: 3.50 +/- 1.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.08     |\n",
      "|    ep_rew_mean      | -0.194   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.70 +/- 1.62\n",
      "Episode length: 4.30 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028468467 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0552     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 0.0115      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.00 +/- 1.34\n",
      "Episode length: 4.00 +/- 1.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.80 +/- 0.98\n",
      "Episode length: 4.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032180794 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0529     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.99     |\n",
      "|    ep_rew_mean      | -0.11    |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008738729 |\n",
      "|    clip_fraction        | 0.0649      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0604     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=97.20 +/- 0.87\n",
      "Episode length: 2.80 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.8      |\n",
      "|    ep_rew_mean      | -0.108   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025574216 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0268      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.00159      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 96           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059218784 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0385      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 0.00175      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.60 +/- 1.56\n",
      "Episode length: 3.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.77     |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025884826 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 0.00115      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=95.40 +/- 1.02\n",
      "Episode length: 4.60 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.96     |\n",
      "|    ep_rew_mean      | -0.0972  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 138      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021849065 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0307      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.00184      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4        |\n",
      "|    ep_rew_mean      | -0.106   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 143      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010623076 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.0772  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007948367 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.127      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.00609     |\n",
      "|    value_loss           | 0.000478    |\n",
      "-----------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed5bc78116e479fa0445623606a3980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | -1.87    |\n",
      "|    ep_true_rew_mean | -19.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 219      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014745185 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.0693      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.000656    |\n",
      "|    value_loss           | 0.0865      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.72    |\n",
      "|    ep_true_rew_mean | -10.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006652784 |\n",
      "|    clip_fraction        | 0.0769      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | 0.0343      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0771      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00808    |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | -1.43    |\n",
      "|    ep_true_rew_mean | 7.88     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=35.90 +/- 60.91\n",
      "Episode length: 14.10 +/- 10.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.1        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011866989 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0883      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=35.70 +/- 60.70\n",
      "Episode length: 14.30 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.3     |\n",
      "|    mean_reward     | 35.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.5     |\n",
      "|    ep_rew_mean      | -1.28    |\n",
      "|    ep_true_rew_mean | 18.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.3         |\n",
      "|    mean_reward          | 11.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072699757 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0675       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 0.203        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=60.60 +/- 56.05\n",
      "Episode length: 9.40 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 60.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.06    |\n",
      "|    ep_true_rew_mean | 33.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=73.10 +/- 49.06\n",
      "Episode length: 6.90 +/- 9.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.9         |\n",
      "|    mean_reward          | 73.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010318881 |\n",
      "|    clip_fraction        | 0.0891      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0717      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.887   |\n",
      "|    ep_true_rew_mean | 46       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.10 +/- 48.56\n",
      "Episode length: 7.90 +/- 8.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.9          |\n",
      "|    mean_reward          | 72.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074286624 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0766       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 0.184        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=72.70 +/- 48.86\n",
      "Episode length: 7.30 +/- 8.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.3      |\n",
      "|    mean_reward     | 72.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | -0.796   |\n",
      "|    ep_true_rew_mean | 52       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=35.70 +/- 60.70\n",
      "Episode length: 14.30 +/- 10.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006355799 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=59.90 +/- 55.58\n",
      "Episode length: 10.10 +/- 9.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.87     |\n",
      "|    ep_rew_mean      | -0.617   |\n",
      "|    ep_true_rew_mean | 62.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=72.10 +/- 48.56\n",
      "Episode length: 7.90 +/- 8.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.9          |\n",
      "|    mean_reward          | 72.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073418785 |\n",
      "|    clip_fraction        | 0.0898       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.533       |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=84.20 +/- 36.41\n",
      "Episode length: 5.80 +/- 6.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.8      |\n",
      "|    mean_reward     | 84.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.56     |\n",
      "|    ep_rew_mean      | -0.492   |\n",
      "|    ep_true_rew_mean | 75.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | 60          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005970032 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.30 +/- 48.66\n",
      "Episode length: 7.70 +/- 8.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.7      |\n",
      "|    mean_reward     | 72.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.98     |\n",
      "|    ep_rew_mean      | -0.513   |\n",
      "|    ep_true_rew_mean | 73       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.30 +/- 55.85\n",
      "Episode length: 9.70 +/- 10.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.7         |\n",
      "|    mean_reward          | 60.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024253719 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=60.30 +/- 55.84\n",
      "Episode length: 9.70 +/- 10.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.7      |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.26     |\n",
      "|    ep_rew_mean      | -0.288   |\n",
      "|    ep_true_rew_mean | 88.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | 72.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010767709 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=48.50 +/- 60.01\n",
      "Episode length: 11.50 +/- 11.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.5     |\n",
      "|    mean_reward     | 48.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5        |\n",
      "|    ep_rew_mean      | -0.182   |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008152737 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00851     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.0575      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.41     |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 94.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=72.50 +/- 48.75\n",
      "Episode length: 7.50 +/- 8.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.5          |\n",
      "|    mean_reward          | 72.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070503047 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.413       |\n",
      "|    explained_variance   | 0.443        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0193      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0215      |\n",
      "|    value_loss           | 0.0348       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.148   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=72.20 +/- 48.60\n",
      "Episode length: 7.80 +/- 8.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 7.8          |\n",
      "|    mean_reward          | 72.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052880314 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.335        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00335     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    value_loss           | 0.0149       |\n",
      "------------------------------------------\n",
      "execution time: 232.495432138443; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n",
      "training agent for task d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873f3ac7fa0b46728dda36fd1acf56cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | -2.03    |\n",
      "|    ep_true_rew_mean | -11.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 223      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011751462 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0455     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | -6.26    |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008383975 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.00126     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | 12.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | -1.37    |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008810021 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.000572   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0809      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -1.75    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012592681 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.00439     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.272       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.57    |\n",
      "|    ep_true_rew_mean | 17.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=23.30 +/- 59.16\n",
      "Episode length: 16.70 +/- 10.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.7        |\n",
      "|    mean_reward          | 23.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010037615 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0865      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.269       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=72.40 +/- 48.72\n",
      "Episode length: 7.60 +/- 8.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 28.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=60.10 +/- 55.73\n",
      "Episode length: 9.90 +/- 9.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.9         |\n",
      "|    mean_reward          | 60.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011485405 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.0573      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=36.10 +/- 61.11\n",
      "Episode length: 13.90 +/- 11.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.9     |\n",
      "|    mean_reward     | 36.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | -1.44    |\n",
      "|    ep_true_rew_mean | 27.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 23.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014126733 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=72.10 +/- 48.56\n",
      "Episode length: 7.90 +/- 8.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.9      |\n",
      "|    mean_reward     | 72.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | -1.03    |\n",
      "|    ep_true_rew_mean | 57       |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=24.00 +/- 60.02\n",
      "Episode length: 16.00 +/- 11.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16         |\n",
      "|    mean_reward          | 24         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01777313 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.22       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.102      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -0.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.799   |\n",
      "|    ep_true_rew_mean | 76.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=35.60 +/- 60.60\n",
      "Episode length: 14.40 +/- 10.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017656973 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.12     |\n",
      "|    ep_rew_mean      | -0.552   |\n",
      "|    ep_true_rew_mean | 84.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020941587 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.46     |\n",
      "|    ep_rew_mean      | -0.316   |\n",
      "|    ep_true_rew_mean | 93.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.10 +/- 1.51\n",
      "Episode length: 3.90 +/- 1.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02706796 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.899     |\n",
      "|    explained_variance   | 0.238      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 0.0643     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.99     |\n",
      "|    ep_rew_mean      | -0.285   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021613527 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.703      |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0477     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0529     |\n",
      "|    value_loss           | 0.0343      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.60 +/- 1.02\n",
      "Episode length: 3.40 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.82     |\n",
      "|    ep_rew_mean      | -0.178   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018976433 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0584     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    value_loss           | 0.0148      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.59     |\n",
      "|    ep_rew_mean      | -0.157   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028633118 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0649     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.00792     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.85     |\n",
      "|    ep_rew_mean      | -0.107   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010424912 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.84     |\n",
      "|    ep_rew_mean      | -0.0828  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.40 +/- 1.56\n",
      "Episode length: 3.60 +/- 1.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072710207 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.795        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.034       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0191      |\n",
      "|    value_loss           | 0.00082      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.90 +/- 1.14\n",
      "Episode length: 4.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.0951  |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043792427 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.70 +/- 1.49\n",
      "Episode length: 3.30 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0738  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.60 +/- 0.80\n",
      "Episode length: 4.40 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066835776 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.000917    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=59.90 +/- 55.58\n",
      "Episode length: 10.10 +/- 9.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.18     |\n",
      "|    ep_rew_mean      | -0.209   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.5        |\n",
      "|    mean_reward          | 96.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13634238 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.19      |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0603    |\n",
      "|    value_loss           | 0.0258     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.40 +/- 1.50\n",
      "Episode length: 3.60 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.79     |\n",
      "|    ep_rew_mean      | -0.112   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.70 +/- 1.68\n",
      "Episode length: 3.30 +/- 1.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05463191 |\n",
      "|    clip_fraction        | 0.0963     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.13      |\n",
      "|    explained_variance   | 0.426      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0285    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.00486    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.30 +/- 0.90\n",
      "Episode length: 3.70 +/- 0.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.20 +/- 1.47\n",
      "Episode length: 3.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loaded agent for task 8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126/logs/tensorboard/8caa02ec824d4dd40d1f336941f3cc867029d31cc50f3690e28c241723778494_transfer_from_d5bc99e4031d4c20a4b13293c5bbba47755bf3ef4fe34f0646506f5212e2854c_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1f6f0c779e4cdb936955a370b5fa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | -1.94    |\n",
      "|    ep_true_rew_mean | -12      |\n",
      "| time/               |          |\n",
      "|    fps              | 218      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012852216 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | -0.0733     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00766    |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | -1.84    |\n",
      "|    ep_true_rew_mean | -8.42    |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016410861 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 1.52     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013627693 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.0185      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.11    |\n",
      "|    ep_true_rew_mean | 39.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.6        |\n",
      "|    mean_reward          | 72.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04109812 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.687     |\n",
      "|    explained_variance   | -0.122     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.133      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0304    |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=72.50 +/- 48.75\n",
      "Episode length: 7.50 +/- 8.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.5      |\n",
      "|    mean_reward     | 72.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.815   |\n",
      "|    ep_true_rew_mean | 54.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=72.60 +/- 48.81\n",
      "Episode length: 7.40 +/- 8.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.4         |\n",
      "|    mean_reward          | 72.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016474463 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=72.00 +/- 48.51\n",
      "Episode length: 8.00 +/- 8.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | 72       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.59     |\n",
      "|    ep_rew_mean      | -0.591   |\n",
      "|    ep_true_rew_mean | 70.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=72.90 +/- 48.97\n",
      "Episode length: 7.10 +/- 9.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.1         |\n",
      "|    mean_reward          | 72.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018544093 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=85.10 +/- 36.72\n",
      "Episode length: 4.90 +/- 6.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 85.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.9      |\n",
      "|    ep_rew_mean      | -0.424   |\n",
      "|    ep_true_rew_mean | 82.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=96.10 +/- 0.83\n",
      "Episode length: 3.90 +/- 0.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018272178 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=71.90 +/- 48.46\n",
      "Episode length: 8.10 +/- 8.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.1      |\n",
      "|    mean_reward     | 71.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.71     |\n",
      "|    ep_rew_mean      | -0.385   |\n",
      "|    ep_true_rew_mean | 85.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.10 +/- 55.71\n",
      "Episode length: 9.90 +/- 9.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.9         |\n",
      "|    mean_reward          | 60.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022909256 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=59.90 +/- 55.59\n",
      "Episode length: 10.10 +/- 9.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.1     |\n",
      "|    mean_reward     | 59.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94     |\n",
      "|    ep_rew_mean      | -0.178   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=48.20 +/- 59.77\n",
      "Episode length: 11.80 +/- 10.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014852148 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.039       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.168   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009226602 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00535    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.0157      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.9      |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33     |\n",
      "|    ep_rew_mean      | -0.144   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=97.20 +/- 1.08\n",
      "Episode length: 2.80 +/- 1.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010207552 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17     |\n",
      "|    ep_rew_mean      | -0.111   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007912679 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.051      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.70 +/- 2.00\n",
      "Episode length: 3.30 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.40 +/- 1.69\n",
      "Episode length: 3.60 +/- 1.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012448968 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.04       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.00521     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.03     |\n",
      "|    ep_rew_mean      | -0.0947  |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01625345 |\n",
      "|    clip_fraction        | 0.0946     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.157     |\n",
      "|    explained_variance   | 0.663      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00479   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.00215    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.78     |\n",
      "|    ep_rew_mean      | -0.0927  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02806269 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.00156    |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.00 +/- 0.89\n",
      "Episode length: 4.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.12     |\n",
      "|    ep_rew_mean      | -0.119   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029090472 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.00978     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "execution time: 210.9346661567688; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-126\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(experiments) * len(context_pairs))\n",
    "for c_src, c_tgt in context_pairs:\n",
    "    for exp in experiments:\n",
    "        exp.run(c_src, c_tgt)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6370cc41-5e48-489a-9738-4fac73f8b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a9c83-dc55-4ee6-80d3-2537e14d57e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7a38a8-7955-4ad9-a07a-56ab3b4c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e798408ea3994a4abdc83be7f3a1a161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | -1.96    |\n",
      "|    ep_true_rew_mean | -12      |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010923559 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.494      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.0839      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -9.45    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011334445 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0225      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.075       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.81    |\n",
      "|    ep_true_rew_mean | -2.02    |\n",
      "| time/               |          |\n",
      "|    fps              | 159      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010181973 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0365     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 1.5      |\n",
      "| time/               |          |\n",
      "|    fps              | 159      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.3        |\n",
      "|    mean_reward          | 11.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012066767 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.0243     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | 5.48     |\n",
      "| time/               |          |\n",
      "|    fps              | 161      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009454833 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.0557      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.238       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.5     |\n",
      "|    ep_true_rew_mean | 21.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 161      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=24.50 +/- 60.63\n",
      "Episode length: 15.50 +/- 11.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.5        |\n",
      "|    mean_reward          | 24.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009608313 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0829      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.41    |\n",
      "|    ep_true_rew_mean | 26.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 162      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009958871 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.90 +/- 60.34\n",
      "Episode length: 11.10 +/- 11.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.1     |\n",
      "|    mean_reward     | 48.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.6     |\n",
      "|    ep_rew_mean      | -1.33    |\n",
      "|    ep_true_rew_mean | 34.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 164      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 60.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016481005 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=23.70 +/- 59.65\n",
      "Episode length: 16.30 +/- 10.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.3     |\n",
      "|    mean_reward     | 23.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -0.933   |\n",
      "|    ep_true_rew_mean | 60.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 165      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.20 +/- 55.79\n",
      "Episode length: 9.80 +/- 10.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | 60.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016293285 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=84.50 +/- 36.52\n",
      "Episode length: 5.50 +/- 6.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | 84.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.736   |\n",
      "|    ep_true_rew_mean | 76.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=95.70 +/- 1.10\n",
      "Episode length: 4.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020414904 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00745     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=83.50 +/- 36.19\n",
      "Episode length: 6.50 +/- 6.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.5      |\n",
      "|    mean_reward     | 83.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.86     |\n",
      "|    ep_rew_mean      | -0.517   |\n",
      "|    ep_true_rew_mean | 86.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=97.10 +/- 1.37\n",
      "Episode length: 2.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021110598 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0379     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0598     |\n",
      "|    value_loss           | 0.0945      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.40 +/- 0.92\n",
      "Episode length: 3.60 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.56     |\n",
      "|    ep_rew_mean      | -0.323   |\n",
      "|    ep_true_rew_mean | 92.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.10 +/- 1.04\n",
      "Episode length: 3.90 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023980096 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.929      |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0669     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0562     |\n",
      "|    value_loss           | 0.0496      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.93     |\n",
      "|    ep_rew_mean      | -0.193   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035166785 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.079      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 0.0239      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.93     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.70 +/- 0.90\n",
      "Episode length: 3.30 +/- 0.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02979935 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.465     |\n",
      "|    explained_variance   | 0.154      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0604    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.00676    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.90 +/- 1.45\n",
      "Episode length: 4.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.02     |\n",
      "|    ep_rew_mean      | -0.107   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018904064 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0532     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.0805  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010186371 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=97.00 +/- 0.89\n",
      "Episode length: 3.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0744  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059532914 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0221      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 0.00129      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.70 +/- 1.85\n",
      "Episode length: 3.30 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0585  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005400585 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.00079     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.42     |\n",
      "|    ep_rew_mean      | -0.0636  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046063755 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00535     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.000534     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.70 +/- 1.79\n",
      "Episode length: 3.30 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.61     |\n",
      "|    ep_rew_mean      | -0.0754  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=96.50 +/- 1.50\n",
      "Episode length: 3.50 +/- 1.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039190715 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0305      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.000464     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=97.30 +/- 1.42\n",
      "Episode length: 2.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.43     |\n",
      "|    ep_rew_mean      | -0.0741  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=97.00 +/- 1.41\n",
      "Episode length: 3.00 +/- 1.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3            |\n",
      "|    mean_reward          | 97           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056949113 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0895      |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    value_loss           | 0.000441     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | -0.0768  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=96.50 +/- 1.57\n",
      "Episode length: 3.50 +/- 1.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067990827 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0778      |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00447     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    value_loss           | 0.000146     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.62     |\n",
      "|    ep_rew_mean      | -0.0751  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002999096 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.07       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00245    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 0.000199    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=95.90 +/- 0.83\n",
      "Episode length: 4.10 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.63     |\n",
      "|    ep_rew_mean      | -0.0731  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=95.90 +/- 1.30\n",
      "Episode length: 4.10 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018158168 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.106      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.000478    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=97.20 +/- 1.25\n",
      "Episode length: 2.80 +/- 1.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0821  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=97.10 +/- 1.04\n",
      "Episode length: 2.90 +/- 1.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.9         |\n",
      "|    mean_reward          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031841613 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.00063     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=96.50 +/- 1.43\n",
      "Episode length: 3.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.57     |\n",
      "|    ep_rew_mean      | -0.0711  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034952633 |\n",
      "|    clip_fraction        | 0.0632      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0736     |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.000582    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=97.50 +/- 1.02\n",
      "Episode length: 2.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.5      |\n",
      "|    mean_reward     | 97.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.62     |\n",
      "|    ep_rew_mean      | -0.0768  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016683273 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.088      |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 0.000549    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=96.30 +/- 1.10\n",
      "Episode length: 3.70 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.66     |\n",
      "|    ep_rew_mean      | -0.0733  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 28672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02103857 |\n",
      "|    clip_fraction        | 0.0272     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0624    |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 0.000308   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=96.70 +/- 1.19\n",
      "Episode length: 3.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.53     |\n",
      "|    ep_rew_mean      | -0.078   |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 29       |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 29696    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=97.40 +/- 1.11\n",
      "Episode length: 2.60 +/- 1.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 2.6          |\n",
      "|    mean_reward          | 97.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011377576 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0549      |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 0.000171     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.6      |\n",
      "|    ep_rew_mean      | -0.0676  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 30       |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 30720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.5          |\n",
      "|    mean_reward          | 96.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018654362 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0622      |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00107     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00662     |\n",
      "|    value_loss           | 8.68e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=97.00 +/- 1.73\n",
      "Episode length: 3.00 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.45     |\n",
      "|    ep_rew_mean      | -0.0599  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 31       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 31744    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029740199 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0471      |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0167      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00825     |\n",
      "|    value_loss           | 0.000203     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.57     |\n",
      "|    ep_rew_mean      | -0.0759  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 32       |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total_timesteps  | 32768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78836817 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0776    |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0559    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    value_loss           | 2.82e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | -0.921   |\n",
      "|    ep_true_rew_mean | 54       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 33       |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total_timesteps  | 33792    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042920247 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.00924     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 61.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 34       |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total_timesteps  | 34816    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03932748 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.369     |\n",
      "|    explained_variance   | 0.0103     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0706     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.24       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.47     |\n",
      "|    ep_rew_mean      | -0.55    |\n",
      "|    ep_true_rew_mean | 86.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 35       |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total_timesteps  | 35840    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03644567 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.35      |\n",
      "|    explained_variance   | -0.0171    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0609     |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.13     |\n",
      "|    ep_rew_mean      | -0.397   |\n",
      "|    ep_true_rew_mean | 92.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 36       |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 36864    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=96.90 +/- 1.58\n",
      "Episode length: 3.10 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015715878 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.00657     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0541     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=96.80 +/- 1.66\n",
      "Episode length: 3.20 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.17     |\n",
      "|    ep_rew_mean      | -0.215   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 37       |\n",
      "|    time_elapsed     | 210      |\n",
      "|    total_timesteps  | 37888    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.4       |\n",
      "|    mean_reward          | 96.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 38000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1997997 |\n",
      "|    clip_fraction        | 0.35      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | 0.0297    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0441   |\n",
      "|    n_updates            | 370       |\n",
      "|    policy_gradient_loss | -0.0573   |\n",
      "|    value_loss           | 0.0404    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=96.70 +/- 1.62\n",
      "Episode length: 3.30 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.3      |\n",
      "|    mean_reward     | 96.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.75     |\n",
      "|    ep_rew_mean      | -0.078   |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 38       |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 38912    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=97.00 +/- 1.41\n",
      "Episode length: 3.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015947351 |\n",
      "|    clip_fraction        | 0.0198      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0576     |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00708    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=96.30 +/- 1.49\n",
      "Episode length: 3.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.64     |\n",
      "|    ep_rew_mean      | -0.0664  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 39       |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total_timesteps  | 39936    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=95.90 +/- 0.83\n",
      "Episode length: 4.10 +/- 0.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002307396 |\n",
      "|    clip_fraction        | 0.0184      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0482     |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.228       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 0.000225    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=96.50 +/- 1.02\n",
      "Episode length: 3.50 +/- 1.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.58     |\n",
      "|    ep_rew_mean      | -0.079   |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 40       |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 40960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=96.40 +/- 1.02\n",
      "Episode length: 3.60 +/- 1.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016902011 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.043       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000162     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    value_loss           | 0.000527     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=95.90 +/- 0.94\n",
      "Episode length: 4.10 +/- 0.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.36     |\n",
      "|    ep_rew_mean      | -0.0668  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 41       |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 41984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=96.90 +/- 0.94\n",
      "Episode length: 3.10 +/- 0.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.1          |\n",
      "|    mean_reward          | 96.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019493821 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0455      |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000565     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00779     |\n",
      "|    value_loss           | 0.000606     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=96.40 +/- 1.11\n",
      "Episode length: 3.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.73     |\n",
      "|    ep_rew_mean      | -0.0689  |\n",
      "|    ep_true_rew_mean | 96.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 42       |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 43008    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=84.90 +/- 36.66\n",
      "Episode length: 5.10 +/- 6.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.1         |\n",
      "|    mean_reward          | 84.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019172482 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0493     |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000719    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    value_loss           | 8.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=72.40 +/- 48.71\n",
      "Episode length: 7.60 +/- 8.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | 72.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.83     |\n",
      "|    ep_rew_mean      | -0.0905  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 43       |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 44032    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059725236 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0828     |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0469     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=96.80 +/- 0.98\n",
      "Episode length: 3.20 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.55     |\n",
      "|    ep_rew_mean      | -0.0714  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 44       |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total_timesteps  | 45056    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=97.20 +/- 1.17\n",
      "Episode length: 2.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 97.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006543441 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0462     |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    value_loss           | 0.000379    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.57     |\n",
      "|    ep_rew_mean      | -0.0804  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 45       |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 46080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=96.70 +/- 1.00\n",
      "Episode length: 3.30 +/- 1.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156902 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0775     |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.000384    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=97.00 +/- 1.10\n",
      "Episode length: 3.00 +/- 1.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.56     |\n",
      "|    ep_rew_mean      | -0.0834  |\n",
      "|    ep_true_rew_mean | 96.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 46       |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total_timesteps  | 47104    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=97.20 +/- 1.25\n",
      "Episode length: 2.80 +/- 1.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 2.8        |\n",
      "|    mean_reward          | 97.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04700093 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0803    |\n",
      "|    explained_variance   | 0.814      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.4      |\n",
      "|    ep_rew_mean      | -0.0747  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 47       |\n",
      "|    time_elapsed     | 265      |\n",
      "|    total_timesteps  | 48128    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=96.70 +/- 1.10\n",
      "Episode length: 3.30 +/- 1.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025772437 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0375     |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.000425    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=96.30 +/- 1.27\n",
      "Episode length: 3.70 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | -0.0686  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 48       |\n",
      "|    time_elapsed     | 271      |\n",
      "|    total_timesteps  | 49152    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | -25       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 49500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0612912 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0721   |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0287   |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.00809  |\n",
      "|    value_loss           | 3.25e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12       |\n",
      "|    ep_rew_mean      | -0.931   |\n",
      "|    ep_true_rew_mean | 54       |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 49       |\n",
      "|    time_elapsed     | 278      |\n",
      "|    total_timesteps  | 50176    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049646017 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.0104       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 0.137        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | 8.24     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 50       |\n",
      "|    time_elapsed     | 285      |\n",
      "|    total_timesteps  | 51200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006119944 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.0607      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    value_loss           | 0.241       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.97    |\n",
      "|    ep_true_rew_mean | 3.02     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 51       |\n",
      "|    time_elapsed     | 291      |\n",
      "|    total_timesteps  | 52224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120646045 |\n",
      "|    clip_fraction        | 0.0704      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.0601      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0088     |\n",
      "|    value_loss           | 0.283       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | -0.76    |\n",
      "|    ep_true_rew_mean | 75.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 52       |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 53248    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068055645 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    value_loss           | 0.265       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.32     |\n",
      "|    ep_rew_mean      | -0.335   |\n",
      "|    ep_true_rew_mean | 93.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 53       |\n",
      "|    time_elapsed     | 303      |\n",
      "|    total_timesteps  | 54272    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023228176 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.0618      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0638     |\n",
      "|    value_loss           | 0.0714      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.22     |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 54       |\n",
      "|    time_elapsed     | 309      |\n",
      "|    total_timesteps  | 55296    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042567603 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.069      |\n",
      "|    value_loss           | 0.0268      |\n",
      "-----------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace6fca6cfc04b4a83d809d4b6afca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | -1.93    |\n",
      "|    ep_true_rew_mean | -7.02    |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011726371 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.513      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0178      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    value_loss           | 0.0779      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -1.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010820968 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0149     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0662      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 5.23     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -12.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012806285 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0258      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 0.215       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | -1.69    |\n",
      "|    ep_true_rew_mean | 6.77     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.7        |\n",
      "|    mean_reward          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010840327 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0662      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=11.80 +/- 56.21\n",
      "Episode length: 18.20 +/- 10.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | -1.53    |\n",
      "|    ep_true_rew_mean | 22.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011292292 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0754      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 0.222       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | -1.31    |\n",
      "|    ep_true_rew_mean | 38.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.30 +/- 59.16\n",
      "Episode length: 16.70 +/- 10.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.7        |\n",
      "|    mean_reward          | 23.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013515709 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.7     |\n",
      "|    mean_reward     | 48.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | -0.954   |\n",
      "|    ep_true_rew_mean | 64.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.2       |\n",
      "|    mean_reward          | 23.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01799132 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.42       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0225     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=24.10 +/- 60.14\n",
      "Episode length: 15.90 +/- 11.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.9     |\n",
      "|    mean_reward     | 24.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | -0.797   |\n",
      "|    ep_true_rew_mean | 72.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=59.60 +/- 55.39\n",
      "Episode length: 10.40 +/- 9.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 10.4       |\n",
      "|    mean_reward          | 59.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01784281 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.035      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=47.60 +/- 59.28\n",
      "Episode length: 12.40 +/- 10.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | 47.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | -0.646   |\n",
      "|    ep_true_rew_mean | 77.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=47.70 +/- 59.37\n",
      "Episode length: 12.30 +/- 10.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.3        |\n",
      "|    mean_reward          | 47.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018653592 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=60.00 +/- 55.65\n",
      "Episode length: 10.00 +/- 9.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.06     |\n",
      "|    ep_rew_mean      | -0.554   |\n",
      "|    ep_true_rew_mean | 83.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=35.70 +/- 60.70\n",
      "Episode length: 14.30 +/- 10.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.3        |\n",
      "|    mean_reward          | 35.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026318148 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=60.70 +/- 56.12\n",
      "Episode length: 9.30 +/- 10.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.3      |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.73     |\n",
      "|    ep_rew_mean      | -0.333   |\n",
      "|    ep_true_rew_mean | 92.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.9         |\n",
      "|    mean_reward          | 72.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016242713 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.78       |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    value_loss           | 0.043       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.06     |\n",
      "|    ep_rew_mean      | -0.282   |\n",
      "|    ep_true_rew_mean | 93.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.57\n",
      "Episode length: 3.50 +/- 1.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014993582 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=95.90 +/- 1.64\n",
      "Episode length: 4.10 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06     |\n",
      "|    ep_rew_mean      | -0.216   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.10 +/- 1.30\n",
      "Episode length: 4.90 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 95.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018491566 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0706     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.30 +/- 1.19\n",
      "Episode length: 4.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.187   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.40 +/- 1.11\n",
      "Episode length: 4.60 +/- 1.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987479 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.0068      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.80 +/- 1.99\n",
      "Episode length: 4.20 +/- 1.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009570849 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=94.60 +/- 0.92\n",
      "Episode length: 5.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | 94.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.6      |\n",
      "|    ep_rew_mean      | -0.155   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.30 +/- 1.90\n",
      "Episode length: 4.70 +/- 1.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005198531 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.177      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.14     |\n",
      "|    ep_rew_mean      | -0.139   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.90 +/- 1.45\n",
      "Episode length: 4.10 +/- 1.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032172191 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00703     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.00136      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.00 +/- 1.34\n",
      "Episode length: 5.00 +/- 1.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.40 +/- 1.43\n",
      "Episode length: 3.60 +/- 1.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.6          |\n",
      "|    mean_reward          | 96.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022652135 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0369       |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00861     |\n",
      "|    value_loss           | 0.000552     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=96.20 +/- 1.40\n",
      "Episode length: 3.80 +/- 1.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.95     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.00 +/- 1.48\n",
      "Episode length: 4.00 +/- 1.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4            |\n",
      "|    mean_reward          | 96           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022764804 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0982      |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0028      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 0.00079      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.90 +/- 1.58\n",
      "Episode length: 4.10 +/- 1.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020200652 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0812      |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0328      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    value_loss           | 0.000684     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=94.90 +/- 1.04\n",
      "Episode length: 5.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 94.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=96.20 +/- 1.25\n",
      "Episode length: 3.80 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001921457 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0698     |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    value_loss           | 0.000829    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=95.20 +/- 1.83\n",
      "Episode length: 4.80 +/- 1.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002215873 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0621     |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00538    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    value_loss           | 0.000305    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=95.80 +/- 1.72\n",
      "Episode length: 4.20 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=96.30 +/- 1.42\n",
      "Episode length: 3.70 +/- 1.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009251389 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0449      |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000817     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00719     |\n",
      "|    value_loss           | 0.000331     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=95.50 +/- 1.43\n",
      "Episode length: 4.50 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.56     |\n",
      "|    ep_rew_mean      | -0.152   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=23.90 +/- 59.89\n",
      "Episode length: 16.10 +/- 10.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.1       |\n",
      "|    mean_reward          | 23.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15257016 |\n",
      "|    clip_fraction        | 0.0792     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0994    |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00829   |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0069    |\n",
      "|    value_loss           | 0.000164   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.71     |\n",
      "|    ep_rew_mean      | -0.329   |\n",
      "|    ep_true_rew_mean | 93.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=96.20 +/- 1.83\n",
      "Episode length: 3.80 +/- 1.83\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 3.8       |\n",
      "|    mean_reward          | 96.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0658658 |\n",
      "|    clip_fraction        | 0.35      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.306    |\n",
      "|    explained_variance   | 0.219     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0256   |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.0434   |\n",
      "|    value_loss           | 0.0298    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=95.20 +/- 1.66\n",
      "Episode length: 4.80 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09     |\n",
      "|    ep_rew_mean      | -0.202   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=95.10 +/- 2.21\n",
      "Episode length: 4.90 +/- 2.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.9        |\n",
      "|    mean_reward          | 95.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03761806 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.0218     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=96.00 +/- 1.79\n",
      "Episode length: 4.00 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94     |\n",
      "|    ep_rew_mean      | -0.194   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=95.10 +/- 1.30\n",
      "Episode length: 4.90 +/- 1.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 95.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027705641 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0906     |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=95.10 +/- 2.07\n",
      "Episode length: 4.90 +/- 2.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65     |\n",
      "|    ep_rew_mean      | -0.169   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 28672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=95.00 +/- 1.61\n",
      "Episode length: 5.00 +/- 1.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 95          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011409672 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.052      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 0.000976    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.91     |\n",
      "|    ep_rew_mean      | -0.176   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 29       |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 29696    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61475813 |\n",
      "|    clip_fraction        | 0.0863     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0843    |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0372    |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | -0.976   |\n",
      "|    ep_true_rew_mean | 50       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 30       |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 30720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 31000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00973401 |\n",
      "|    clip_fraction        | 0.0526     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.332     |\n",
      "|    explained_variance   | 0.0543     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0696     |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.00276   |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 8.23     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 31       |\n",
      "|    time_elapsed     | 163      |\n",
      "|    total_timesteps  | 31744    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008731348 |\n",
      "|    clip_fraction        | 0.077       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.0762      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | -1.38    |\n",
      "|    ep_true_rew_mean | 40.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 32       |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total_timesteps  | 32768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110209435 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.0719       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.152        |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    value_loss           | 0.283        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.68     |\n",
      "|    ep_rew_mean      | -0.633   |\n",
      "|    ep_true_rew_mean | 86.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 33       |\n",
      "|    time_elapsed     | 175      |\n",
      "|    total_timesteps  | 33792    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.4       |\n",
      "|    mean_reward          | -0.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 34000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01806503 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.494     |\n",
      "|    explained_variance   | 0.0954     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00411    |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.63     |\n",
      "|    ep_rew_mean      | -0.339   |\n",
      "|    ep_true_rew_mean | 93.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 34       |\n",
      "|    time_elapsed     | 181      |\n",
      "|    total_timesteps  | 34816    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=96.00 +/- 1.95\n",
      "Episode length: 4.00 +/- 1.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011168938 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.0486      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=95.70 +/- 1.68\n",
      "Episode length: 4.30 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.87     |\n",
      "|    ep_rew_mean      | -0.27    |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 35       |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 35840    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=95.30 +/- 1.68\n",
      "Episode length: 4.70 +/- 1.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011423048 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=94.90 +/- 1.87\n",
      "Episode length: 5.10 +/- 1.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 94.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.9      |\n",
      "|    ep_rew_mean      | -0.188   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 36       |\n",
      "|    time_elapsed     | 191      |\n",
      "|    total_timesteps  | 36864    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=94.70 +/- 2.10\n",
      "Episode length: 5.30 +/- 2.10\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5.3       |\n",
      "|    mean_reward          | 94.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 37000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0276849 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.177    |\n",
      "|    explained_variance   | 0.576     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0316   |\n",
      "|    n_updates            | 360       |\n",
      "|    policy_gradient_loss | -0.0427   |\n",
      "|    value_loss           | 0.00607   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=94.50 +/- 1.86\n",
      "Episode length: 5.50 +/- 1.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.5      |\n",
      "|    mean_reward     | 94.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.76     |\n",
      "|    ep_rew_mean      | -0.177   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 37       |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 37888    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=95.00 +/- 1.41\n",
      "Episode length: 5.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 95          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087105125 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.077      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0397     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=94.80 +/- 1.54\n",
      "Episode length: 5.20 +/- 1.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 94.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.6      |\n",
      "|    ep_rew_mean      | -0.161   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 38       |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total_timesteps  | 38912    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=95.30 +/- 1.62\n",
      "Episode length: 4.70 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002172702 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0816     |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    value_loss           | 0.00164     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=95.40 +/- 1.62\n",
      "Episode length: 4.60 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.152   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 39       |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 39936    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=96.60 +/- 1.50\n",
      "Episode length: 3.40 +/- 1.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029988674 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0527     |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=95.10 +/- 2.12\n",
      "Episode length: 4.90 +/- 2.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 40       |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total_timesteps  | 40960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=96.40 +/- 1.36\n",
      "Episode length: 3.60 +/- 1.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.6        |\n",
      "|    mean_reward          | 96.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 41000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10865852 |\n",
      "|    clip_fraction        | 0.0512     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0471    |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.000198   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=96.20 +/- 1.83\n",
      "Episode length: 3.80 +/- 1.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.139   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 41       |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total_timesteps  | 41984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=96.40 +/- 1.91\n",
      "Episode length: 3.60 +/- 1.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013137981 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0492     |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00839    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 0.000828    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=96.60 +/- 1.36\n",
      "Episode length: 3.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 42       |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total_timesteps  | 43008    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=96.60 +/- 1.62\n",
      "Episode length: 3.40 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.4         |\n",
      "|    mean_reward          | 96.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002317579 |\n",
      "|    clip_fraction        | 0.024       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.04       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00977    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 0.000243    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=94.80 +/- 1.17\n",
      "Episode length: 5.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | 94.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.88     |\n",
      "|    ep_rew_mean      | -0.12    |\n",
      "|    ep_true_rew_mean | 96.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 43       |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 44032    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=95.60 +/- 1.50\n",
      "Episode length: 4.40 +/- 1.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013560257 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0316      |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000103     |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 0.00024      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=95.30 +/- 1.73\n",
      "Episode length: 4.70 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 44       |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 45056    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | -25       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 45500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8254735 |\n",
      "|    clip_fraction        | 0.049     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0494   |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0537   |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.00775  |\n",
      "|    value_loss           | 3.73e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 46.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 45       |\n",
      "|    time_elapsed     | 237      |\n",
      "|    total_timesteps  | 46080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013999696 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.0431      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00912     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -2.11    |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 46       |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 47104    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009284267 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0836      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00337     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.21    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 47       |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total_timesteps  | 48128    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039560837 |\n",
      "|    clip_fraction        | 0.0808       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.486       |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.125        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.00528      |\n",
      "|    value_loss           | 0.244        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 48       |\n",
      "|    time_elapsed     | 255      |\n",
      "|    total_timesteps  | 49152    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 49500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007756155 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000746   |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 49       |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 50176    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008183602 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.153       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    value_loss           | 0.264       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 50       |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 51200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 51500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074960403 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.147        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | 0.000765     |\n",
      "|    value_loss           | 0.271        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 51       |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 52224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010227781 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.391       |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.141        |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.000199    |\n",
      "|    value_loss           | 0.263        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -23.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 52       |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total_timesteps  | 53248    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 53500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045993114 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 0.257        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 53       |\n",
      "|    time_elapsed     | 285      |\n",
      "|    total_timesteps  | 54272    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021200401 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 0.00162      |\n",
      "|    value_loss           | 0.261        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 54       |\n",
      "|    time_elapsed     | 291      |\n",
      "|    total_timesteps  | 55296    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034446577 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.413       |\n",
      "|    explained_variance   | 0.151        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.118        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 0.256        |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7754a74b134f09a111c4f99c690b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -25      |\n",
      "| time/               |          |\n",
      "|    fps              | 229      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028372612 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0432      |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0448       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | -2.12    |\n",
      "|    ep_true_rew_mean | -22.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.3        |\n",
      "|    mean_reward          | -0.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002254189 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0658     |\n",
      "|    explained_variance   | 0.0838      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -6.79e-05   |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | -2.05    |\n",
      "|    ep_true_rew_mean | -19.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.3         |\n",
      "|    mean_reward          | -0.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027799993 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0613      |\n",
      "|    explained_variance   | 0.0987       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.13         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 0.231        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | -1.91    |\n",
      "|    ep_true_rew_mean | -11.8    |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.7         |\n",
      "|    mean_reward          | -12.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009976506 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0485      |\n",
      "|    explained_variance   | 0.156        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.227        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.86    |\n",
      "|    ep_true_rew_mean | -7.88    |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.6         |\n",
      "|    mean_reward          | -12.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018281409 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0522      |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    value_loss           | 0.229        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=12.00 +/- 56.52\n",
      "Episode length: 18.00 +/- 10.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | 12       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | -1.89    |\n",
      "|    ep_true_rew_mean | -8.21    |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.4         |\n",
      "|    mean_reward          | -0.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027182775 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.053       |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.111        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 0.242        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | -1.82    |\n",
      "|    ep_true_rew_mean | -4.38    |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.1        |\n",
      "|    mean_reward          | 11.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003112656 |\n",
      "|    clip_fraction        | 0.0181      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0904     |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.118       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000721   |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.74    |\n",
      "|    ep_true_rew_mean | 6.2      |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09199804 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00906   |\n",
      "|    value_loss           | 0.244      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.77    |\n",
      "|    ep_true_rew_mean | 66.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 20.4      |\n",
      "|    mean_reward          | -0.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0760754 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.454    |\n",
      "|    explained_variance   | 0.0177    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0182    |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 0.174     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | -0.776   |\n",
      "|    ep_true_rew_mean | 81.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063083775 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.03     |\n",
      "|    ep_rew_mean      | -0.385   |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053008996 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0552     |\n",
      "|    value_loss           | 0.0554      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.48     |\n",
      "|    ep_rew_mean      | -0.246   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.80 +/- 1.54\n",
      "Episode length: 4.20 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.2         |\n",
      "|    mean_reward          | 95.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057853006 |\n",
      "|    clip_fraction        | 0.44        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0828     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.0172      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.30 +/- 1.79\n",
      "Episode length: 3.70 +/- 1.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.47     |\n",
      "|    ep_rew_mean      | -0.247   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.20 +/- 1.54\n",
      "Episode length: 4.80 +/- 1.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025334291 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0493     |\n",
      "|    value_loss           | 0.0232      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.00 +/- 1.84\n",
      "Episode length: 5.00 +/- 1.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.16     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.4        |\n",
      "|    mean_reward          | 95.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07553973 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.222     |\n",
      "|    explained_variance   | 0.397      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0531    |\n",
      "|    value_loss           | 0.0125     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.40 +/- 1.56\n",
      "Episode length: 3.60 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4      |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | 95.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13484666 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0774    |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    value_loss           | 0.00257    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.50 +/- 1.57\n",
      "Episode length: 4.50 +/- 1.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.41     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44038004 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0923    |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.000431   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | -0.932   |\n",
      "|    ep_true_rew_mean | 61.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045409877 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.0339      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0376      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.82     |\n",
      "|    ep_rew_mean      | -0.654   |\n",
      "|    ep_true_rew_mean | 88.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034430973 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.0379      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.63     |\n",
      "|    ep_rew_mean      | -0.452   |\n",
      "|    ep_true_rew_mean | 92.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.1        |\n",
      "|    mean_reward          | 95.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04393273 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.286     |\n",
      "|    explained_variance   | 0.0693     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.052     |\n",
      "|    value_loss           | 0.096      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.50 +/- 2.29\n",
      "Episode length: 4.50 +/- 2.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.53     |\n",
      "|    ep_rew_mean      | -0.337   |\n",
      "|    ep_true_rew_mean | 93.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.30 +/- 1.79\n",
      "Episode length: 4.70 +/- 1.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08945826 |\n",
      "|    clip_fraction        | 0.319      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | 0.0545     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00897   |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0519    |\n",
      "|    value_loss           | 0.0595     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=96.40 +/- 1.62\n",
      "Episode length: 3.60 +/- 1.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.10 +/- 2.02\n",
      "Episode length: 3.90 +/- 2.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09     |\n",
      "|    ep_rew_mean      | -0.206   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.30 +/- 2.00\n",
      "Episode length: 4.70 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18981406 |\n",
      "|    clip_fraction        | 0.0608     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0586    |\n",
      "|    explained_variance   | 0.398      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    value_loss           | 0.0102     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=96.20 +/- 1.66\n",
      "Episode length: 3.80 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2      |\n",
      "|    ep_rew_mean      | -0.134   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 22       |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total_timesteps  | 22528    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | -25       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 23000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4738076 |\n",
      "|    clip_fraction        | 0.275     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.196    |\n",
      "|    explained_variance   | 0.959     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0896   |\n",
      "|    n_updates            | 220       |\n",
      "|    policy_gradient_loss | 0.339     |\n",
      "|    value_loss           | 0.000299  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | -0.935   |\n",
      "|    ep_true_rew_mean | 58.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 23       |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 23552    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038643874 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.0363      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0375      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.893   |\n",
      "|    ep_true_rew_mean | 74       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030655844 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | -0.0133     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.04     |\n",
      "|    ep_rew_mean      | -0.483   |\n",
      "|    ep_true_rew_mean | 91       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 25       |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total_timesteps  | 25600    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=95.40 +/- 1.28\n",
      "Episode length: 4.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035301503 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.0879      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0092      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0483     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=95.30 +/- 1.49\n",
      "Episode length: 4.70 +/- 1.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.16     |\n",
      "|    ep_rew_mean      | -0.315   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 26       |\n",
      "|    time_elapsed     | 145      |\n",
      "|    total_timesteps  | 26624    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.1        |\n",
      "|    mean_reward          | 95.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18128508 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.0601     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0319    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0529    |\n",
      "|    value_loss           | 0.0551     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=96.20 +/- 1.78\n",
      "Episode length: 3.80 +/- 1.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.54     |\n",
      "|    ep_rew_mean      | -0.157   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 27       |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total_timesteps  | 27648    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056820862 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0498     |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=95.80 +/- 1.78\n",
      "Episode length: 4.20 +/- 1.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total_timesteps  | 28672    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=95.70 +/- 1.79\n",
      "Episode length: 4.30 +/- 1.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.3         |\n",
      "|    mean_reward          | 95.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006283336 |\n",
      "|    clip_fraction        | 0.0116      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0328     |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00159    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 0.000541    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=96.00 +/- 1.00\n",
      "Episode length: 4.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.27     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 29       |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 29696    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=96.40 +/- 1.80\n",
      "Episode length: 3.60 +/- 1.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041564096 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0435     |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00356    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00544    |\n",
      "|    value_loss           | 0.000447    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=95.10 +/- 1.22\n",
      "Episode length: 4.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.6      |\n",
      "|    ep_rew_mean      | -0.175   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 30       |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total_timesteps  | 30720    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=95.10 +/- 1.22\n",
      "Episode length: 4.90 +/- 1.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.9         |\n",
      "|    mean_reward          | 95.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027926492 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0707     |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.00704     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=96.30 +/- 1.68\n",
      "Episode length: 3.70 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 31       |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 31744    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 25        |\n",
      "|    mean_reward          | -25       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 32000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3316306 |\n",
      "|    clip_fraction        | 0.255     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.205    |\n",
      "|    explained_variance   | 0.728     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0594   |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -0.0547   |\n",
      "|    value_loss           | 0.00173   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.854   |\n",
      "|    ep_true_rew_mean | 68.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 32       |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 32768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024533343 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 0.0154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.1     |\n",
      "|    ep_rew_mean      | -0.964   |\n",
      "|    ep_true_rew_mean | 71.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 33       |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total_timesteps  | 33792    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 34000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02231611 |\n",
      "|    clip_fraction        | 0.394      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.626     |\n",
      "|    explained_variance   | 0.0379     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0295     |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    value_loss           | 0.182      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.01     |\n",
      "|    ep_rew_mean      | -0.384   |\n",
      "|    ep_true_rew_mean | 93       |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 34       |\n",
      "|    time_elapsed     | 188      |\n",
      "|    total_timesteps  | 34816    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=96.00 +/- 1.18\n",
      "Episode length: 4.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04452496 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.435     |\n",
      "|    explained_variance   | 0.0785     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00624    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0495    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.54     |\n",
      "|    ep_rew_mean      | -0.266   |\n",
      "|    ep_true_rew_mean | 94.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 35       |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 35840    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=96.90 +/- 1.76\n",
      "Episode length: 3.10 +/- 1.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.1         |\n",
      "|    mean_reward          | 96.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098092355 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 0.0382      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36500, episode_reward=96.40 +/- 1.20\n",
      "Episode length: 3.60 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 36       |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total_timesteps  | 36864    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=95.10 +/- 1.45\n",
      "Episode length: 4.90 +/- 1.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.9        |\n",
      "|    mean_reward          | 95.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 37000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13409624 |\n",
      "|    clip_fraction        | 0.0493     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0695    |\n",
      "|    explained_variance   | 0.614      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    value_loss           | 0.00383    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=95.60 +/- 1.56\n",
      "Episode length: 4.40 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.142   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 37       |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 37888    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=96.40 +/- 2.15\n",
      "Episode length: 3.60 +/- 2.15\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 3.6           |\n",
      "|    mean_reward          | 96.4          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 38000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085054693 |\n",
      "|    clip_fraction        | 0.0108        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0566       |\n",
      "|    explained_variance   | 0.764         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00174       |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.00697      |\n",
      "|    value_loss           | 0.00139       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=95.50 +/- 1.50\n",
      "Episode length: 4.50 +/- 1.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 38       |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total_timesteps  | 38912    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=95.70 +/- 1.19\n",
      "Episode length: 4.30 +/- 1.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.3          |\n",
      "|    mean_reward          | 95.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016872929 |\n",
      "|    clip_fraction        | 0.0144       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0487      |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000572     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 0.000785     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=95.30 +/- 1.95\n",
      "Episode length: 4.70 +/- 1.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.29     |\n",
      "|    ep_rew_mean      | -0.134   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 39       |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total_timesteps  | 39936    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=95.40 +/- 1.62\n",
      "Episode length: 4.60 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005714942 |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0305     |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    value_loss           | 0.000613    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=95.90 +/- 1.64\n",
      "Episode length: 4.10 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.15     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 40       |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total_timesteps  | 40960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=95.80 +/- 2.04\n",
      "Episode length: 4.20 +/- 2.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.2          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001664834 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0258      |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00186     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000801    |\n",
      "|    value_loss           | 0.000295     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=97.10 +/- 1.45\n",
      "Episode length: 2.90 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.9      |\n",
      "|    mean_reward     | 97.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.38     |\n",
      "|    ep_rew_mean      | -0.146   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 41       |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total_timesteps  | 41984    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=95.90 +/- 1.92\n",
      "Episode length: 4.10 +/- 1.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.1          |\n",
      "|    mean_reward          | 95.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054147723 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0502      |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00882     |\n",
      "|    value_loss           | 0.000488     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=95.10 +/- 2.21\n",
      "Episode length: 4.90 +/- 2.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21     |\n",
      "|    ep_rew_mean      | -0.133   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 42       |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 43008    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=96.50 +/- 1.63\n",
      "Episode length: 3.50 +/- 1.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021926101 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0469     |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00839    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 0.000343    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=95.50 +/- 1.75\n",
      "Episode length: 4.50 +/- 1.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.53     |\n",
      "|    ep_rew_mean      | -0.156   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 43       |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total_timesteps  | 44032    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=96.00 +/- 1.41\n",
      "Episode length: 4.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044512063 |\n",
      "|    clip_fraction        | 0.0194      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.024      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00219    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=95.10 +/- 1.30\n",
      "Episode length: 4.90 +/- 1.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 189      |\n",
      "|    iterations       | 44       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 45056    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 25       |\n",
      "|    mean_reward          | -25      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 45500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.02488  |\n",
      "|    clip_fraction        | 0.212    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.115   |\n",
      "|    explained_variance   | 0.967    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0387  |\n",
      "|    n_updates            | 440      |\n",
      "|    policy_gradient_loss | -0.0308  |\n",
      "|    value_loss           | 0.000197 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | -0.975   |\n",
      "|    ep_true_rew_mean | 46.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 45       |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 46080    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 46500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02594351 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.508     |\n",
      "|    explained_variance   | 0.051      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0923     |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | 0.0109     |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | 1.97     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 46       |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total_timesteps  | 47104    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064134835 |\n",
      "|    clip_fraction        | 0.0816       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.543       |\n",
      "|    explained_variance   | 0.0873       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0924       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.204        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | 3.11     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 47       |\n",
      "|    time_elapsed     | 255      |\n",
      "|    total_timesteps  | 48128    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055525526 |\n",
      "|    clip_fraction        | 0.0903       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.557       |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00709     |\n",
      "|    value_loss           | 0.237        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | 9.85     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 48       |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 49152    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043680947 |\n",
      "|    clip_fraction        | 0.0821       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0993       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0124      |\n",
      "|    value_loss           | 0.263        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.8     |\n",
      "|    ep_rew_mean      | -1.53    |\n",
      "|    ep_true_rew_mean | 29.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 49       |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 50176    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033558287 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0819      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.246       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -0.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | -0.862   |\n",
      "|    ep_true_rew_mean | 71.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 50       |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 51200    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019013729 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.0435      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.244       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.87     |\n",
      "|    ep_rew_mean      | -0.382   |\n",
      "|    ep_true_rew_mean | 93.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 51       |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total_timesteps  | 52224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=96.70 +/- 1.55\n",
      "Episode length: 3.30 +/- 1.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018163389 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=95.10 +/- 0.83\n",
      "Episode length: 4.90 +/- 0.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.9      |\n",
      "|    mean_reward     | 95.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.61     |\n",
      "|    ep_rew_mean      | -0.259   |\n",
      "|    ep_true_rew_mean | 94.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 52       |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total_timesteps  | 53248    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008938502 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.0366      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=95.70 +/- 1.35\n",
      "Episode length: 4.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.22     |\n",
      "|    ep_rew_mean      | -0.233   |\n",
      "|    ep_true_rew_mean | 94.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 53       |\n",
      "|    time_elapsed     | 288      |\n",
      "|    total_timesteps  | 54272    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=95.20 +/- 1.72\n",
      "Episode length: 4.80 +/- 1.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025550023 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 0.0212      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=95.80 +/- 1.17\n",
      "Episode length: 4.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09     |\n",
      "|    ep_rew_mean      | -0.212   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 54       |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 55296    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=96.00 +/- 2.05\n",
      "Episode length: 4.00 +/- 2.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020480704 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0545     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.00792     |\n",
      "-----------------------------------------\n",
      "execution time: 902.093710899353; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "execution time 902.1072990894318\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "no_geco_exp.run(*context_pair)\n",
    "\n",
    "end = time.time()\n",
    "print(f'execution time {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00977ee-a1dd-46e2-b9fc-1ed751f7a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bde42e2aa848fd9af7ef9217d5f7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | -4.57    |\n",
      "| time/               |          |\n",
      "|    fps              | 216      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=23.60 +/- 59.52\n",
      "Episode length: 16.40 +/- 10.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | 23.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013716312 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.189      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0049     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.0842      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | 23.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | -1.77    |\n",
      "|    ep_true_rew_mean | -2.45    |\n",
      "| time/               |          |\n",
      "|    fps              | 197      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.7        |\n",
      "|    mean_reward          | -0.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011380397 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.0891     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0668      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00674    |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=24.40 +/- 60.50\n",
      "Episode length: 15.60 +/- 11.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | 24.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.68    |\n",
      "|    ep_true_rew_mean | 9.93     |\n",
      "| time/               |          |\n",
      "|    fps              | 191      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20.7       |\n",
      "|    mean_reward          | -0.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01244794 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.0454     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0892     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.70 +/- 56.06\n",
      "Episode length: 18.30 +/- 10.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.3     |\n",
      "|    mean_reward     | 11.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | -1.45    |\n",
      "|    ep_true_rew_mean | 30.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.60 +/- 48.80\n",
      "Episode length: 20.60 +/- 8.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -0.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012538508 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.50 +/- 49.00\n",
      "Episode length: 20.50 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.5     |\n",
      "|    mean_reward     | -0.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | -1.16    |\n",
      "|    ep_true_rew_mean | 51.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=36.00 +/- 61.00\n",
      "Episode length: 14.00 +/- 11.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014214918 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=48.60 +/- 60.10\n",
      "Episode length: 11.40 +/- 11.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | 48.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | -0.816   |\n",
      "|    ep_true_rew_mean | 76.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=23.80 +/- 59.77\n",
      "Episode length: 16.20 +/- 10.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.2       |\n",
      "|    mean_reward          | 23.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02331125 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00345    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0488    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=36.40 +/- 61.40\n",
      "Episode length: 13.60 +/- 11.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | 36.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.29     |\n",
      "|    ep_rew_mean      | -0.549   |\n",
      "|    ep_true_rew_mean | 86.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.40 +/- 59.94\n",
      "Episode length: 11.60 +/- 10.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | 48.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022350186 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00315    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0547     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.4      |\n",
      "|    ep_rew_mean      | -0.388   |\n",
      "|    ep_true_rew_mean | 91.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.90 +/- 1.51\n",
      "Episode length: 4.10 +/- 1.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034030855 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    value_loss           | 0.0668      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=97.00 +/- 1.00\n",
      "Episode length: 3.00 +/- 1.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.83     |\n",
      "|    ep_rew_mean      | -0.188   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.80 +/- 1.25\n",
      "Episode length: 3.20 +/- 1.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017361421 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.761      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0445     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 0.0279      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=97.30 +/- 1.42\n",
      "Episode length: 2.70 +/- 1.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.7      |\n",
      "|    mean_reward     | 97.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48     |\n",
      "|    ep_rew_mean      | -0.158   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=97.00 +/- 1.18\n",
      "Episode length: 3.00 +/- 1.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3           |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017585857 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0702     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0536     |\n",
      "|    value_loss           | 0.0108      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=96.80 +/- 1.33\n",
      "Episode length: 3.20 +/- 1.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.06     |\n",
      "|    ep_rew_mean      | -0.113   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.1        |\n",
      "|    mean_reward          | 95.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02847341 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.395     |\n",
      "|    explained_variance   | 0.418      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0482    |\n",
      "|    value_loss           | 0.0046     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.30 +/- 1.19\n",
      "Episode length: 3.70 +/- 1.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.7      |\n",
      "|    mean_reward     | 96.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.81     |\n",
      "|    ep_rew_mean      | -0.0938  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.70 +/- 1.35\n",
      "Episode length: 3.30 +/- 1.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020088963 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.60 +/- 1.28\n",
      "Episode length: 3.40 +/- 1.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0864  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.70 +/- 0.78\n",
      "Episode length: 3.30 +/- 0.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005119242 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.177      |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0475     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00175     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.46     |\n",
      "|    ep_rew_mean      | -0.0674  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029901152 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.142       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00934     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 0.000449     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.90 +/- 1.14\n",
      "Episode length: 3.10 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.44     |\n",
      "|    ep_rew_mean      | -0.0695  |\n",
      "|    ep_true_rew_mean | 96.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.20 +/- 0.98\n",
      "Episode length: 3.80 +/- 0.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005373997 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.42e-05    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 0.000905    |\n",
      "-----------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0713935a1a494ad1ba2816944ec1f3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | -1.94    |\n",
      "|    ep_true_rew_mean | -7.16    |\n",
      "| time/               |          |\n",
      "|    fps              | 205      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011136007 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.472      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00799    |\n",
      "|    value_loss           | 0.0789      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | -1.96    |\n",
      "|    ep_true_rew_mean | -12      |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008006715 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.052       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00691    |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | -1.88    |\n",
      "|    ep_true_rew_mean | -10.1    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.6       |\n",
      "|    mean_reward          | -12.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00804119 |\n",
      "|    clip_fraction        | 0.0432     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | -0.00433   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0878     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00901   |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=11.90 +/- 56.37\n",
      "Episode length: 18.10 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.1     |\n",
      "|    mean_reward     | 11.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.73    |\n",
      "|    ep_true_rew_mean | -0.58    |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=12.10 +/- 56.67\n",
      "Episode length: 17.90 +/- 10.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.9        |\n",
      "|    mean_reward          | 12.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008969119 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0977      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00958    |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | -1.76    |\n",
      "|    ep_true_rew_mean | -0.03    |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012760956 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.0745      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0768      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-12.80 +/- 36.60\n",
      "Episode length: 22.80 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -12.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.65    |\n",
      "|    ep_true_rew_mean | 10.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-0.20 +/- 49.60\n",
      "Episode length: 20.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014945274 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0742      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.264       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.30 +/- 49.40\n",
      "Episode length: 20.30 +/- 9.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.3     |\n",
      "|    mean_reward     | -0.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 22.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=12.20 +/- 56.82\n",
      "Episode length: 17.80 +/- 11.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.8       |\n",
      "|    mean_reward          | 12.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01058745 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.161      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0601     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | -1.42    |\n",
      "|    ep_true_rew_mean | 24.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012069819 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 0.242       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=24.20 +/- 60.26\n",
      "Episode length: 15.80 +/- 11.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | 24.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.27    |\n",
      "|    ep_true_rew_mean | 35.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=60.50 +/- 55.98\n",
      "Episode length: 9.50 +/- 10.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.5         |\n",
      "|    mean_reward          | 60.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016410839 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=60.40 +/- 55.91\n",
      "Episode length: 9.60 +/- 10.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.5     |\n",
      "|    ep_rew_mean      | -0.95    |\n",
      "|    ep_true_rew_mean | 60.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=60.80 +/- 56.17\n",
      "Episode length: 9.20 +/- 10.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | 60.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013738147 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.7     |\n",
      "|    mean_reward     | 48.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | -0.765   |\n",
      "|    ep_true_rew_mean | 71.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=60.10 +/- 55.72\n",
      "Episode length: 9.90 +/- 9.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.9        |\n",
      "|    mean_reward          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01723985 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.38       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0213     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=48.30 +/- 59.85\n",
      "Episode length: 11.70 +/- 10.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.7     |\n",
      "|    mean_reward     | 48.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.61     |\n",
      "|    ep_rew_mean      | -0.442   |\n",
      "|    ep_true_rew_mean | 91.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=72.10 +/- 48.57\n",
      "Episode length: 7.90 +/- 8.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.9         |\n",
      "|    mean_reward          | 72.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022891507 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.391       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00583    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.0718      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=72.80 +/- 48.91\n",
      "Episode length: 7.20 +/- 8.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | 72.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.28     |\n",
      "|    ep_rew_mean      | -0.401   |\n",
      "|    ep_true_rew_mean | 91.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.60 +/- 1.62\n",
      "Episode length: 4.40 +/- 1.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023319745 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.862      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00881    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    value_loss           | 0.0586      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=84.40 +/- 36.50\n",
      "Episode length: 5.60 +/- 6.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | 84.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.95     |\n",
      "|    ep_rew_mean      | -0.278   |\n",
      "|    ep_true_rew_mean | 94       |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.70 +/- 1.73\n",
      "Episode length: 3.30 +/- 1.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.3        |\n",
      "|    mean_reward          | 96.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01482351 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.654     |\n",
      "|    explained_variance   | 0.385      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.058     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    value_loss           | 0.0194     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=95.30 +/- 1.68\n",
      "Episode length: 4.70 +/- 1.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.34     |\n",
      "|    ep_rew_mean      | -0.23    |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013283327 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0744     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    value_loss           | 0.0139      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.30 +/- 1.95\n",
      "Episode length: 4.70 +/- 1.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.77     |\n",
      "|    ep_rew_mean      | -0.184   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.00 +/- 1.67\n",
      "Episode length: 5.00 +/- 1.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 95         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01609547 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.346     |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.00488    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.40 +/- 1.43\n",
      "Episode length: 4.60 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.65     |\n",
      "|    ep_rew_mean      | -0.165   |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 95       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.60 +/- 1.56\n",
      "Episode length: 4.40 +/- 1.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011775638 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0053      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.00 +/- 1.67\n",
      "Episode length: 5.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 95       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.147   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=96.10 +/- 1.37\n",
      "Episode length: 3.90 +/- 1.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003193725 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.80 +/- 1.83\n",
      "Episode length: 4.20 +/- 1.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16     |\n",
      "|    ep_rew_mean      | -0.136   |\n",
      "|    ep_true_rew_mean | 95.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=96.10 +/- 1.22\n",
      "Episode length: 3.90 +/- 1.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030460022 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0094      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0181      |\n",
      "|    value_loss           | 0.0016       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.70 +/- 2.05\n",
      "Episode length: 4.30 +/- 2.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e30e1da0624459a92a759dc9de3dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    ep_true_rew_mean | -22.3    |\n",
      "| time/               |          |\n",
      "|    fps              | 161      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110440105 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.932       |\n",
      "|    explained_variance   | 0.0143       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0727       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00215      |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | -2.2     |\n",
      "|    ep_true_rew_mean | -23.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 150      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079041235 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.935       |\n",
      "|    explained_variance   | 0.0918       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0827       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00583      |\n",
      "|    value_loss           | 0.204        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | -2.19    |\n",
      "|    ep_true_rew_mean | -21.9    |\n",
      "| time/               |          |\n",
      "|    fps              | 156      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072330316 |\n",
      "|    clip_fraction        | 0.0983       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.988       |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.107        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000962     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | -2.11    |\n",
      "|    ep_true_rew_mean | -15.2    |\n",
      "| time/               |          |\n",
      "|    fps              | 155      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | -25          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065816483 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0762       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.116        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00438     |\n",
      "|    value_loss           | 0.272        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | -2.01    |\n",
      "|    ep_true_rew_mean | -5.26    |\n",
      "| time/               |          |\n",
      "|    fps              | 156      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013907805 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0723      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | -1.85    |\n",
      "|    ep_true_rew_mean | 6.42     |\n",
      "| time/               |          |\n",
      "|    fps              | 157      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010977956 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | -1.39    |\n",
      "|    ep_true_rew_mean | 38.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 157      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25         |\n",
      "|    mean_reward          | -25        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01358513 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.975     |\n",
      "|    explained_variance   | 0.0145     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.147      |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | -0.982   |\n",
      "|    ep_true_rew_mean | 74.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 157      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=96.40 +/- 1.74\n",
      "Episode length: 3.60 +/- 1.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011207795 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.0828      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0442      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.229       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=95.60 +/- 0.80\n",
      "Episode length: 4.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.54     |\n",
      "|    ep_rew_mean      | -0.616   |\n",
      "|    ep_true_rew_mean | 85.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 161      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=95.30 +/- 1.10\n",
      "Episode length: 4.70 +/- 1.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.7        |\n",
      "|    mean_reward          | 95.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01472255 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.813     |\n",
      "|    explained_variance   | 0.0778     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0487    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=97.00 +/- 1.55\n",
      "Episode length: 3.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.82     |\n",
      "|    ep_rew_mean      | -0.473   |\n",
      "|    ep_true_rew_mean | 91.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 164      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.20 +/- 2.04\n",
      "Episode length: 3.80 +/- 2.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037031446 |\n",
      "|    clip_fraction        | 0.442       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0238     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 1.43\n",
      "Episode length: 4.40 +/- 1.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.66     |\n",
      "|    ep_rew_mean      | -0.274   |\n",
      "|    ep_true_rew_mean | 94.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 167      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.00 +/- 1.67\n",
      "Episode length: 4.00 +/- 1.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4           |\n",
      "|    mean_reward          | 96          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017627649 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 0.0325      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=95.80 +/- 1.60\n",
      "Episode length: 4.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.32     |\n",
      "|    ep_rew_mean      | -0.22    |\n",
      "|    ep_true_rew_mean | 94.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 168      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.40 +/- 1.43\n",
      "Episode length: 4.60 +/- 1.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.6         |\n",
      "|    mean_reward          | 95.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033030193 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.077      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.00956     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.90 +/- 1.22\n",
      "Episode length: 4.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52     |\n",
      "|    ep_rew_mean      | -0.174   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=94.90 +/- 1.45\n",
      "Episode length: 5.10 +/- 1.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.1         |\n",
      "|    mean_reward          | 94.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042372268 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.075      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    value_loss           | 0.00636     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.32     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=94.90 +/- 1.22\n",
      "Episode length: 5.10 +/- 1.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5.1        |\n",
      "|    mean_reward          | 94.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03827393 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.186     |\n",
      "|    explained_variance   | 0.789      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0746    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.0574     |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.20 +/- 1.78\n",
      "Episode length: 4.80 +/- 1.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.8      |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12     |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.50 +/- 1.43\n",
      "Episode length: 4.50 +/- 1.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.5        |\n",
      "|    mean_reward          | 95.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11156154 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0372    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.0111     |\n",
      "----------------------------------------\n",
      "execution time: 290.09781289100647; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECO)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "execution time 290.11234498023987\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with_geco_exp.run(*context_pair)\n",
    "\n",
    "end = time.time()\n",
    "print(f'execution time {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db26281-6f1f-4721-b1a4-4fd380001cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training agent for task 53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ba601f60ec42d89f568be48fab10cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | -1.83    |\n",
      "|    ep_true_rew_mean | -4.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 204      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014138963 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.632      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000518   |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.083       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | -1.75    |\n",
      "|    ep_true_rew_mean | 2.66     |\n",
      "| time/               |          |\n",
      "|    fps              | 181      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009476387 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0663     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00943     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | -1.71    |\n",
      "|    ep_true_rew_mean | 8.87     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011694605 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.00737    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -12.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | -1.62    |\n",
      "|    ep_true_rew_mean | 16.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-0.40 +/- 49.20\n",
      "Episode length: 20.40 +/- 9.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -0.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013181873 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0569      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.217       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=11.50 +/- 55.76\n",
      "Episode length: 18.50 +/- 9.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.5     |\n",
      "|    mean_reward     | 11.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | -1.38    |\n",
      "|    ep_true_rew_mean | 37.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 166      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=84.70 +/- 36.57\n",
      "Episode length: 5.30 +/- 6.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.3         |\n",
      "|    mean_reward          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011064107 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0715      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.246       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | 36.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | -1.12    |\n",
      "|    ep_true_rew_mean | 55.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 169      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.1        |\n",
      "|    mean_reward          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018833332 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=84.90 +/- 36.66\n",
      "Episode length: 5.10 +/- 6.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.1      |\n",
      "|    mean_reward     | 84.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | -0.777   |\n",
      "|    ep_true_rew_mean | 77.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 170      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=48.50 +/- 60.02\n",
      "Episode length: 11.50 +/- 11.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.5        |\n",
      "|    mean_reward          | 48.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023824748 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=84.70 +/- 36.59\n",
      "Episode length: 5.30 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.3      |\n",
      "|    mean_reward     | 84.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.52     |\n",
      "|    ep_rew_mean      | -0.485   |\n",
      "|    ep_true_rew_mean | 91.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 171      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.00 +/- 36.37\n",
      "Episode length: 6.00 +/- 6.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035753503 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00506     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 0.0982      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=60.40 +/- 55.92\n",
      "Episode length: 9.60 +/- 10.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.16     |\n",
      "|    ep_rew_mean      | -0.286   |\n",
      "|    ep_true_rew_mean | 93.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 172      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.70 +/- 1.42\n",
      "Episode length: 3.30 +/- 1.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032680072 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.856      |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0586     |\n",
      "|    value_loss           | 0.0502      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.20 +/- 0.87\n",
      "Episode length: 3.80 +/- 0.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.89     |\n",
      "|    ep_rew_mean      | -0.195   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.50 +/- 1.20\n",
      "Episode length: 3.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028035438 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0609     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.0148      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=97.00 +/- 1.61\n",
      "Episode length: 3.00 +/- 1.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3        |\n",
      "|    mean_reward     | 97       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.04     |\n",
      "|    ep_rew_mean      | -0.122   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=96.70 +/- 1.27\n",
      "Episode length: 3.30 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.3         |\n",
      "|    mean_reward          | 96.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033187434 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.80 +/- 1.08\n",
      "Episode length: 3.20 +/- 1.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.113   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.50 +/- 1.12\n",
      "Episode length: 3.50 +/- 1.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.5         |\n",
      "|    mean_reward          | 96.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011788767 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=96.50 +/- 1.36\n",
      "Episode length: 3.50 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.5      |\n",
      "|    mean_reward     | 96.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.49     |\n",
      "|    ep_rew_mean      | -0.0791  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.20 +/- 1.17\n",
      "Episode length: 3.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.8         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007742137 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.00184     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.54     |\n",
      "|    ep_rew_mean      | -0.081   |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.30 +/- 1.00\n",
      "Episode length: 3.70 +/- 1.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.7          |\n",
      "|    mean_reward          | 96.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030918699 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.028       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    value_loss           | 0.000445     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=96.80 +/- 1.17\n",
      "Episode length: 3.20 +/- 1.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.2      |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.76     |\n",
      "|    ep_rew_mean      | -0.0901  |\n",
      "|    ep_true_rew_mean | 96.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=96.40 +/- 1.28\n",
      "Episode length: 3.60 +/- 1.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.6         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373901 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0978     |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00935    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.60 +/- 1.20\n",
      "Episode length: 3.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | 96.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.46     |\n",
      "|    ep_rew_mean      | -0.0686  |\n",
      "|    ep_true_rew_mean | 96.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.60 +/- 1.43\n",
      "Episode length: 3.40 +/- 1.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011847345 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0741      |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.01        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    value_loss           | 0.000518     |\n",
      "------------------------------------------\n",
      "training agent for task 2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283de487d371480198e8f2b7b070d31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | -1.92    |\n",
      "|    ep_true_rew_mean | -2.37    |\n",
      "| time/               |          |\n",
      "|    fps              | 218      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008174786 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | -0.748      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 0.0779      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | -1.78    |\n",
      "|    ep_true_rew_mean | 4.88     |\n",
      "| time/               |          |\n",
      "|    fps              | 187      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008656021 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0024      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 9.25     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-12.60 +/- 37.20\n",
      "Episode length: 22.60 +/- 7.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010511254 |\n",
      "|    clip_fraction        | 0.0776      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0208      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0787      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.222       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-12.70 +/- 36.90\n",
      "Episode length: 22.70 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.7     |\n",
      "|    mean_reward     | -12.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    ep_true_rew_mean | 13.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=47.90 +/- 59.53\n",
      "Episode length: 12.10 +/- 10.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.1        |\n",
      "|    mean_reward          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012217405 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.082       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.118       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=11.00 +/- 54.99\n",
      "Episode length: 19.00 +/- 9.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | 11       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | -1.48    |\n",
      "|    ep_true_rew_mean | 28.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 173      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=22.80 +/- 58.54\n",
      "Episode length: 17.20 +/- 9.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.2        |\n",
      "|    mean_reward          | 22.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012054538 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0736      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=59.70 +/- 55.46\n",
      "Episode length: 10.30 +/- 9.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.3     |\n",
      "|    mean_reward     | 59.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | -1.29    |\n",
      "|    ep_true_rew_mean | 43.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 174      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=11.60 +/- 55.91\n",
      "Episode length: 18.40 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012432735 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-0.70 +/- 48.60\n",
      "Episode length: 20.70 +/- 8.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.7     |\n",
      "|    mean_reward     | -0.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | -1.02    |\n",
      "|    ep_true_rew_mean | 56.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 175      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=36.20 +/- 61.21\n",
      "Episode length: 13.80 +/- 11.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019563418 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=48.10 +/- 59.69\n",
      "Episode length: 11.90 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.9     |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.7     |\n",
      "|    ep_rew_mean      | -0.878   |\n",
      "|    ep_true_rew_mean | 65.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=84.30 +/- 36.45\n",
      "Episode length: 5.70 +/- 6.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.7         |\n",
      "|    mean_reward          | 84.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019089015 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00817     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=23.50 +/- 59.40\n",
      "Episode length: 16.50 +/- 10.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.5     |\n",
      "|    mean_reward     | 23.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.95     |\n",
      "|    ep_rew_mean      | -0.623   |\n",
      "|    ep_true_rew_mean | 84       |\n",
      "| time/               |          |\n",
      "|    fps              | 176      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.00 +/- 2.00\n",
      "Episode length: 4.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4          |\n",
      "|    mean_reward          | 96         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02569261 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00138   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 0.0965     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=72.20 +/- 48.61\n",
      "Episode length: 7.80 +/- 8.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | 72.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.08     |\n",
      "|    ep_rew_mean      | -0.538   |\n",
      "|    ep_true_rew_mean | 89.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=71.50 +/- 48.29\n",
      "Episode length: 8.50 +/- 8.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.5         |\n",
      "|    mean_reward          | 71.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023390459 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0502     |\n",
      "|    value_loss           | 0.0682      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=59.80 +/- 55.53\n",
      "Episode length: 10.20 +/- 9.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | 59.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.3      |\n",
      "|    ep_rew_mean      | -0.31    |\n",
      "|    ep_true_rew_mean | 93.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 177      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.20 +/- 1.89\n",
      "Episode length: 4.80 +/- 1.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034524746 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.0325      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12     |\n",
      "|    ep_rew_mean      | -0.216   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 178      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=95.20 +/- 1.17\n",
      "Episode length: 4.80 +/- 1.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.8         |\n",
      "|    mean_reward          | 95.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012611792 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    value_loss           | 0.0106      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.87     |\n",
      "|    ep_rew_mean      | -0.189   |\n",
      "|    ep_true_rew_mean | 95.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 179      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=96.80 +/- 1.47\n",
      "Episode length: 3.20 +/- 1.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.2         |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023219395 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0652     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    value_loss           | 0.0097      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=95.60 +/- 0.92\n",
      "Episode length: 4.40 +/- 0.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.57     |\n",
      "|    ep_rew_mean      | -0.171   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 180      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 79       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=95.90 +/- 2.21\n",
      "Episode length: 4.10 +/- 2.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010030448 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00761    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.00251     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.50 +/- 1.80\n",
      "Episode length: 4.50 +/- 1.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.5      |\n",
      "|    mean_reward     | 95.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.153   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.50 +/- 1.20\n",
      "Episode length: 4.50 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.5         |\n",
      "|    mean_reward          | 95.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006910992 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.179      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=95.30 +/- 1.35\n",
      "Episode length: 4.70 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.7      |\n",
      "|    mean_reward     | 95.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.01     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 182      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=96.10 +/- 1.51\n",
      "Episode length: 3.90 +/- 1.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.9          |\n",
      "|    mean_reward          | 96.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074602887 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.041       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0234      |\n",
      "|    value_loss           | 0.00117      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=96.90 +/- 1.22\n",
      "Episode length: 3.10 +/- 1.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=96.80 +/- 1.60\n",
      "Episode length: 3.20 +/- 1.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.2          |\n",
      "|    mean_reward          | 96.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017659836 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0955      |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 0.000823     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=95.70 +/- 1.35\n",
      "Episode length: 4.30 +/- 1.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 184      |\n",
      "|    iterations       | 18       |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total_timesteps  | 18432    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=95.60 +/- 1.28\n",
      "Episode length: 4.40 +/- 1.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088427905 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0906      |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0453      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0148      |\n",
      "|    value_loss           | 0.000383     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=95.70 +/- 1.27\n",
      "Episode length: 4.30 +/- 1.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09     |\n",
      "|    ep_rew_mean      | -0.129   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 19       |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 19456    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=95.60 +/- 2.11\n",
      "Episode length: 4.40 +/- 2.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022806399 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 0.000816    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=95.90 +/- 1.70\n",
      "Episode length: 4.10 +/- 1.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61     |\n",
      "|    ep_rew_mean      | -0.174   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 185      |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 20480    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=95.00 +/- 1.41\n",
      "Episode length: 5.00 +/- 1.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 95         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07254443 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.174     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0358    |\n",
      "|    value_loss           | 0.0146     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=95.90 +/- 1.04\n",
      "Episode length: 4.10 +/- 1.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=96.90 +/- 1.45\n",
      "Episode length: 3.10 +/- 1.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.1      |\n",
      "|    mean_reward     | 96.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.78     |\n",
      "|    ep_rew_mean      | -0.163   |\n",
      "|    ep_true_rew_mean | 95.2     |\n",
      "| time/               |          |\n",
      "|    fps              | 186      |\n",
      "|    iterations       | 21       |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total_timesteps  | 21504    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.3        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03770335 |\n",
      "|    clip_fraction        | 0.0266     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0605    |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.00227    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=95.40 +/- 1.56\n",
      "Episode length: 4.60 +/- 1.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to single_experiment_dumps/WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42/logs/tensorboard/2f7b4f8a8b91ff2d752157422029fe71a56a048e3f37a07602ae2e559bdf6a42_transfer_from_53ad9c64ee168199805b6d596c9cd230efcabf1a9640eb7454ae65d6b1d7e256_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6bbe7617e54fb9a2a6588aff0ced50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/100000.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | -2.11    |\n",
      "|    ep_true_rew_mean | -14.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 224      |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 1024     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057835918 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000781   |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.3     |\n",
      "|    ep_rew_mean      | -1.61    |\n",
      "|    ep_true_rew_mean | 28.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 2        |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 2048     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -25         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016020654 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0895      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 0.26        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-25.00 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -25      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | -0.753   |\n",
      "|    ep_true_rew_mean | 81.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 183      |\n",
      "|    iterations       | 3        |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 3072     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=95.60 +/- 1.56\n",
      "Episode length: 4.40 +/- 1.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019720385 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.721      |\n",
      "|    explained_variance   | 0.0668      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=95.90 +/- 1.37\n",
      "Episode length: 4.10 +/- 1.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.1      |\n",
      "|    mean_reward     | 95.9     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.47     |\n",
      "|    ep_rew_mean      | -0.432   |\n",
      "|    ep_true_rew_mean | 92.5     |\n",
      "| time/               |          |\n",
      "|    fps              | 188      |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 4096     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=95.90 +/- 1.97\n",
      "Episode length: 4.10 +/- 1.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.1         |\n",
      "|    mean_reward          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030699207 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.055      |\n",
      "|    value_loss           | 0.0914      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=95.70 +/- 1.55\n",
      "Episode length: 4.30 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.3      |\n",
      "|    mean_reward     | 95.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.94     |\n",
      "|    ep_rew_mean      | -0.267   |\n",
      "|    ep_true_rew_mean | 94.1     |\n",
      "| time/               |          |\n",
      "|    fps              | 190      |\n",
      "|    iterations       | 5        |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 5120     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=96.10 +/- 1.92\n",
      "Episode length: 3.90 +/- 1.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.9        |\n",
      "|    mean_reward          | 96.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04087795 |\n",
      "|    clip_fraction        | 0.402      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.394     |\n",
      "|    explained_variance   | 0.266      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0441    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 0.0254     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=96.10 +/- 1.87\n",
      "Episode length: 3.90 +/- 1.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.11     |\n",
      "|    ep_rew_mean      | -0.198   |\n",
      "|    ep_true_rew_mean | 94.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 192      |\n",
      "|    iterations       | 6        |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 6144     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=95.30 +/- 1.27\n",
      "Episode length: 4.70 +/- 1.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.7         |\n",
      "|    mean_reward          | 95.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026352048 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=95.60 +/- 1.20\n",
      "Episode length: 4.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.6      |\n",
      "|    ep_rew_mean      | -0.167   |\n",
      "|    ep_true_rew_mean | 95.4     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 7        |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 7168     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=95.60 +/- 1.36\n",
      "Episode length: 4.40 +/- 1.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 4.4         |\n",
      "|    mean_reward          | 95.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039626434 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00535    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.0481      |\n",
      "|    value_loss           | 0.00202     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.36     |\n",
      "|    ep_rew_mean      | -0.149   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=95.00 +/- 0.63\n",
      "Episode length: 5.00 +/- 0.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 95           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038451543 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.67         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0345      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.00281      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=96.00 +/- 1.84\n",
      "Episode length: 4.00 +/- 1.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05     |\n",
      "|    ep_rew_mean      | -0.131   |\n",
      "|    ep_true_rew_mean | 96       |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 9        |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 9216     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=96.30 +/- 2.05\n",
      "Episode length: 3.70 +/- 2.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.7         |\n",
      "|    mean_reward          | 96.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005438314 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0886     |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -3.43e-05   |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 0.00152     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=96.00 +/- 1.73\n",
      "Episode length: 4.00 +/- 1.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.11     |\n",
      "|    ep_rew_mean      | -0.138   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 10       |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 10240    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=96.20 +/- 1.54\n",
      "Episode length: 3.80 +/- 1.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.8          |\n",
      "|    mean_reward          | 96.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020053275 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0736      |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000246     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00673     |\n",
      "|    value_loss           | 0.000718     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=95.60 +/- 1.36\n",
      "Episode length: 4.40 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.4      |\n",
      "|    mean_reward     | 95.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.37     |\n",
      "|    ep_rew_mean      | -0.14    |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 196      |\n",
      "|    iterations       | 11       |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 11264    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=95.20 +/- 1.78\n",
      "Episode length: 4.80 +/- 1.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.8          |\n",
      "|    mean_reward          | 95.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022548598 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0675      |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0145      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 0.00911      |\n",
      "|    value_loss           | 0.000688     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=96.20 +/- 1.66\n",
      "Episode length: 3.80 +/- 1.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.8      |\n",
      "|    mean_reward     | 96.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.08     |\n",
      "|    ep_rew_mean      | -0.135   |\n",
      "|    ep_true_rew_mean | 95.9     |\n",
      "| time/               |          |\n",
      "|    fps              | 195      |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 12288    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=96.60 +/- 1.69\n",
      "Episode length: 3.40 +/- 1.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 3.4          |\n",
      "|    mean_reward          | 96.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012159946 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0594      |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00043      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    value_loss           | 0.000585     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=96.00 +/- 1.55\n",
      "Episode length: 4.00 +/- 1.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4        |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.34     |\n",
      "|    ep_rew_mean      | -0.134   |\n",
      "|    ep_true_rew_mean | 95.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 13       |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 13312    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=95.60 +/- 1.69\n",
      "Episode length: 4.40 +/- 1.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 4.4          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042938613 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0476      |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0252      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00881     |\n",
      "|    value_loss           | 0.000575     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=96.10 +/- 1.14\n",
      "Episode length: 3.90 +/- 1.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42     |\n",
      "|    ep_rew_mean      | -0.137   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 14       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 14336    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=96.10 +/- 1.58\n",
      "Episode length: 3.90 +/- 1.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 3.9         |\n",
      "|    mean_reward          | 96.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057663515 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00476    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 6.23e-05    |\n",
      "|    value_loss           | 0.000171    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=95.40 +/- 1.11\n",
      "Episode length: 4.60 +/- 1.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | 95.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.38     |\n",
      "|    ep_rew_mean      | -0.327   |\n",
      "|    ep_true_rew_mean | 93.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 15       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 15360    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=95.00 +/- 1.18\n",
      "Episode length: 5.00 +/- 1.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 95         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13705866 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.309     |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0455    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 0.0318     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=96.10 +/- 1.64\n",
      "Episode length: 3.90 +/- 1.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.9      |\n",
      "|    mean_reward     | 96.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.73     |\n",
      "|    ep_rew_mean      | -0.19    |\n",
      "|    ep_true_rew_mean | 95.3     |\n",
      "| time/               |          |\n",
      "|    fps              | 194      |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 84       |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=95.90 +/- 1.76\n",
      "Episode length: 4.10 +/- 1.76\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 4.1       |\n",
      "|    mean_reward          | 95.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 16500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1423634 |\n",
      "|    clip_fraction        | 0.258     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0952   |\n",
      "|    explained_variance   | 0.559     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0363   |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0469   |\n",
      "|    value_loss           | 0.00731   |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=95.80 +/- 1.47\n",
      "Episode length: 4.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.2      |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.44     |\n",
      "|    ep_rew_mean      | -0.143   |\n",
      "|    ep_true_rew_mean | 95.6     |\n",
      "| time/               |          |\n",
      "|    fps              | 193      |\n",
      "|    iterations       | 17       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 17408    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=95.40 +/- 1.02\n",
      "Episode length: 4.60 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 4.6        |\n",
      "|    mean_reward          | 95.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05491456 |\n",
      "|    clip_fraction        | 0.0484     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0936    |\n",
      "|    explained_variance   | 0.775      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=96.40 +/- 1.85\n",
      "Episode length: 3.60 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.6      |\n",
      "|    mean_reward     | 96.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "execution time: 307.01110196113586; experiment: WithTransferExperiment/env-small/cspace-fixed_entities/alg-PPO/mods-(AS,RS,GECOUPT)/rm_kwargs-((grid_resolution-(2,2)))/alg_kwargs-((n_steps-1024))/model_kwargs-((gnn_hidden_dims-(32,32)))/seed-42\n",
      "execution time 307.02650713920593\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "geco_upt_exp.run(*context_pair)\n",
    "\n",
    "end = time.time()\n",
    "print(f'execution time {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e0326-90d1-4da0-ac38-48a005ec0725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
