{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMs for RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tuple}[1]{\\left\\langle #1 \\right\\rangle}\n",
    "\\newcommand{\\StateSpace}[0]{\\mathcal{S}}\n",
    "\\newcommand{\\ActionSpace}[0]{\\mathcal{A}}\n",
    "\\newcommand{\\SAS}[0]{\\StateSpace\\times\\ActionSpace\\times\\StateSpace}\n",
    "\\newcommand{\\ContextSpace}[0]{\\mathcal{C}}\n",
    "\\newcommand{\\MDPFunc}[0]{\\mathcal{M}}\n",
    "\\newcommand{\\CMDP}[0]{\\tuple{\\ContextSpace, \\StateSpace, \\ActionSpace, \\MDPFunc}}\n",
    "\\newcommand{\\MDPInContext}[1]{\\tuple{\\StateSpace, \\ActionSpace, p^{#1}, r^{#1}, \\gamma}}\n",
    "\\newcommand{\\propsym}[0]{\\mathcal{P}}\n",
    "\\newcommand{\\RM}[0]{\\tuple{\\propsym, U, \\delta_u, \\delta_r}}\n",
    "\\newcommand{\\RMsym}[0]{\\mathcal{R}}\n",
    "\\newcommand{\\MDPRM}[0]{\\tuple{\\StateSpace , \\ActionSpace, p,\\gamma,\\propsym, L, U, \\delta_u, \\delta_r}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "$$\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Let $X = \\CMDP$ be a CMDP and let $\\Psi$ be the distribution over $\\ContextSpace$. Let $f$ be a parameterized function, called the _adaptation function_, that takes trajectories and outputs adapted parameters. Denote by $\\tau_c^{1:K}$ a collection of $K$ trajectories collected within context $c\\in\\ContextSpace$. We would like to find meta parameters $\\theta^*$ such that sampling few trajectories from parameterized policy $\\pi_\\theta $ and adapting $\\theta$ to $\\phi = f_\\theta(\\tau_c^{1:K})$ maximizes $\\pi_\\phi$'s return over $c\\sim\\Psi$. More formally, we would like to find:\n",
    "\\begin{equation}\n",
    "    \\theta^*\\in\\argmax{\\theta}\\mathbb{E}_{c\\sim\\Psi}\\left[J_c(\\pi_\\phi)\\middle|\\phi = f_\\theta(\\tau_c^{1:K}), \\tau_c^{1:K}\\sim\\pi_\\theta\\right]\n",
    "\\end{equation}\n",
    "with the smallest $K$ possible. We measure performance using the \"time to threshold\" metric, that measures the number of samples/trajectories collected in order to achieve some threshold accumulated rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "![GNN RM usage](images/GNN_RM_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and notebook utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/rmrl2/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from rmrl.reward_machines.rm_env import RMEnvWrapper\n",
    "from rmrl.envs.multitask_env import MultiTaskWrapper\n",
    "from rmrl.envs.mujoco.HalfCheetahV3 import velocity_env\n",
    "from rmrl.envs.mujoco.reward_machines.HalfCheetahV3 import VelocityRM\n",
    "from rmrl.reward_machines.potential_functions import ValueIteration\n",
    "from rmrl.policies.rm_policy import RMPolicy\n",
    "from rmrl.nn.models import RMFeatureExtractorSB\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "\n",
    "MODELS_DIR = Path('./models')\n",
    "LOGS_DIR = Path('./logs')\n",
    "TB_DIR = LOGS_DIR / 'tensorboard'\n",
    "\n",
    "\n",
    "RS_GAMMA = 0.9\n",
    "MAX_ITERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ddpg_trained_model(name, policy, env, timesteps):\n",
    "    model = DDPG(policy=policy,\n",
    "                 env=env,\n",
    "                 verbose=1,\n",
    "                 tensorboard_log=TB_DIR / name)\n",
    "    # load model if exists\n",
    "    try:\n",
    "        print('loading pre-trained model')\n",
    "        return model.load(MODELS_DIR / name, model.env)\n",
    "    except FileNotFoundError:\n",
    "        print('pre-trained model not found. training model')\n",
    "        train_model(model, timesteps)\n",
    "        model.save(MODELS_DIR / name)\n",
    "        return model\n",
    "\n",
    "def train_model(model, timesteps):\n",
    "    try:\n",
    "        iter(timesteps)\n",
    "    except TypeError:\n",
    "        timesteps = [timesteps]\n",
    "\n",
    "    for i, ts in enumerate(timesteps, 1):\n",
    "        print(f'run number {i}. {ts} timesteps')\n",
    "        model.learn(total_timesteps=ts,\n",
    "                    tb_log_name=f'run{i}',  # number the run logs\n",
    "                    reset_num_timesteps=False)  # continue the same curve\n",
    "\n",
    "def animate_env(model, num_iters=MAX_ITERS):\n",
    "    env = model.env\n",
    "    try:\n",
    "        obs = env.reset()\n",
    "        for i in range(num_iters):\n",
    "            action, _state = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "    except KeyboardInterrupt:\n",
    "        print('Early stop by user')\n",
    "    finally:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original cheetah env (move forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'HalfCheetah-v3'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loading pre-trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[94488]: Class GLFWApplicationDelegate is implemented in both /Users/guyazran/.mujoco/mjpro150/bin/libglfw.3.dylib (0x12571d778) and /usr/local/Caskroom/miniconda/base/envs/rmrl2/lib/python3.9/site-packages/glfw/libglfw.3.dylib (0x1268857c0). One of the two will be used. Which one is undefined.\n",
      "objc[94488]: Class GLFWWindowDelegate is implemented in both /Users/guyazran/.mujoco/mjpro150/bin/libglfw.3.dylib (0x12571d700) and /usr/local/Caskroom/miniconda/base/envs/rmrl2/lib/python3.9/site-packages/glfw/libglfw.3.dylib (0x1268857e8). One of the two will be used. Which one is undefined.\n",
      "objc[94488]: Class GLFWContentView is implemented in both /Users/guyazran/.mujoco/mjpro150/bin/libglfw.3.dylib (0x12571d7a0) and /usr/local/Caskroom/miniconda/base/envs/rmrl2/lib/python3.9/site-packages/glfw/libglfw.3.dylib (0x126885838). One of the two will be used. Which one is undefined.\n",
      "objc[94488]: Class GLFWWindow is implemented in both /Users/guyazran/.mujoco/mjpro150/bin/libglfw.3.dylib (0x12571d818) and /usr/local/Caskroom/miniconda/base/envs/rmrl2/lib/python3.9/site-packages/glfw/libglfw.3.dylib (0x1268858b0). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "model_fw = get_ddpg_trained_model('ddpg_cheetah_fw',\n",
    "                                  'MlpPolicy',\n",
    "                                  'HalfCheetah-v3',\n",
    "                                  timesteps=[1e4] * 4)  # 4 rounds of 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ab7221da585a1d21\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ab7221da585a1d21\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/tensorboard/ddpg_cheetah_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_env(model_fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed velocity (5.0) cheetah env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal velocity is: 5.0\n",
      "fixed goal\n"
     ]
    }
   ],
   "source": [
    "fixed_vel_env = velocity_env(initial_goal_vel=5.0, change_task_on_reset=False)\n",
    "fixed_vel_env.reset()\n",
    "print(f'goal velocity is: {fixed_vel_env.task}')\n",
    "print('goal changes on reset' if fixed_vel_env.change_task_on_reset else 'fixed goal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "loading pre-trained model\n",
      "pre-trained model not found. training model\n",
      "run number 1. 100000.0 timesteps\n",
      "Logging to logs/tensorboard/ddpg_cheetah_vel5/run1_0\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.51e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 32        |\n",
      "|    total timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 23.1      |\n",
      "|    critic_loss     | 0.803     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 101       |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 79.1      |\n",
      "|    critic_loss     | 1.04      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 97        |\n",
      "|    time_elapsed    | 122       |\n",
      "|    total timesteps | 12000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 129       |\n",
      "|    critic_loss     | 1.8       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.32e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 95        |\n",
      "|    time_elapsed    | 167       |\n",
      "|    total timesteps | 16000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 186       |\n",
      "|    critic_loss     | 2.22      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 15000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.32e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 93        |\n",
      "|    time_elapsed    | 213       |\n",
      "|    total timesteps | 20000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 214       |\n",
      "|    critic_loss     | 2.96      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 19000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.28e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 93        |\n",
      "|    time_elapsed    | 257       |\n",
      "|    total timesteps | 24000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 255       |\n",
      "|    critic_loss     | 3.19      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.25e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 92        |\n",
      "|    time_elapsed    | 303       |\n",
      "|    total timesteps | 28000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 290       |\n",
      "|    critic_loss     | 2.92      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 27000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.25e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 91        |\n",
      "|    time_elapsed    | 348       |\n",
      "|    total timesteps | 32000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 316       |\n",
      "|    critic_loss     | 3.15      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 31000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.24e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 91        |\n",
      "|    time_elapsed    | 394       |\n",
      "|    total timesteps | 36000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 335       |\n",
      "|    critic_loss     | 3.29      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 35000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 90        |\n",
      "|    time_elapsed    | 440       |\n",
      "|    total timesteps | 40000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 355       |\n",
      "|    critic_loss     | 2.59      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 39000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.18e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 88        |\n",
      "|    time_elapsed    | 495       |\n",
      "|    total timesteps | 44000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 375       |\n",
      "|    critic_loss     | 2.53      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 43000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 88        |\n",
      "|    time_elapsed    | 544       |\n",
      "|    total timesteps | 48000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 387       |\n",
      "|    critic_loss     | 2.69      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 47000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.17e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 88        |\n",
      "|    time_elapsed    | 590       |\n",
      "|    total timesteps | 52000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 398       |\n",
      "|    critic_loss     | 2.57      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 51000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.14e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 87        |\n",
      "|    time_elapsed    | 636       |\n",
      "|    total timesteps | 56000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 408       |\n",
      "|    critic_loss     | 2.77      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 55000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -5.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 88       |\n",
      "|    time_elapsed    | 681      |\n",
      "|    total timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 413      |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -5.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 86       |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 415      |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 63000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -5.05e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 794       |\n",
      "|    total timesteps | 68000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 418       |\n",
      "|    critic_loss     | 4.47      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 67000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.99e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 84        |\n",
      "|    time_elapsed    | 848       |\n",
      "|    total timesteps | 72000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 416       |\n",
      "|    critic_loss     | 5         |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 71000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.93e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 905       |\n",
      "|    total timesteps | 76000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 410       |\n",
      "|    critic_loss     | 5.61      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 75000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.92e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 958       |\n",
      "|    total timesteps | 80000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 400       |\n",
      "|    critic_loss     | 6.15      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 79000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.88e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 83        |\n",
      "|    time_elapsed    | 1011      |\n",
      "|    total timesteps | 84000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 390       |\n",
      "|    critic_loss     | 7.06      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 83000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.84e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 82        |\n",
      "|    time_elapsed    | 1062      |\n",
      "|    total timesteps | 88000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 379       |\n",
      "|    critic_loss     | 7.36      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 87000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -4.8e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 1112     |\n",
      "|    total timesteps | 92000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 367      |\n",
      "|    critic_loss     | 7.84     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 91000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.75e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 82        |\n",
      "|    time_elapsed    | 1160      |\n",
      "|    total timesteps | 96000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 353       |\n",
      "|    critic_loss     | 8.56      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 95000     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -4.7e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 341      |\n",
      "|    critic_loss     | 8.62     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99000    |\n",
      "---------------------------------\n",
      "run number 2. 100000.0 timesteps\n",
      "Logging to logs/tensorboard/ddpg_cheetah_vel5/run2_0\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.63e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 104       |\n",
      "|    fps             | 2440      |\n",
      "|    time_elapsed    | 42        |\n",
      "|    total timesteps | 104000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 328       |\n",
      "|    critic_loss     | 9.38      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 103000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.56e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 108       |\n",
      "|    fps             | 1119      |\n",
      "|    time_elapsed    | 96        |\n",
      "|    total timesteps | 108000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 317       |\n",
      "|    critic_loss     | 10.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 107000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 112       |\n",
      "|    fps             | 755       |\n",
      "|    time_elapsed    | 148       |\n",
      "|    total timesteps | 112000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 303       |\n",
      "|    critic_loss     | 9.4       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 111000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.37e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 116       |\n",
      "|    fps             | 571       |\n",
      "|    time_elapsed    | 203       |\n",
      "|    total timesteps | 116000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 292       |\n",
      "|    critic_loss     | 10.6      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 115000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.29e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 120       |\n",
      "|    fps             | 473       |\n",
      "|    time_elapsed    | 253       |\n",
      "|    total timesteps | 120000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 280       |\n",
      "|    critic_loss     | 10.8      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 119000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.26e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 124       |\n",
      "|    fps             | 393       |\n",
      "|    time_elapsed    | 315       |\n",
      "|    total timesteps | 124000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 275       |\n",
      "|    critic_loss     | 13.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 123000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -4.2e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 345      |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total timesteps | 128000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 268      |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 127000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -4.1e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 308      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total timesteps | 132000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 259      |\n",
      "|    critic_loss     | 17       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 131000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -4.02e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 136       |\n",
      "|    fps             | 282       |\n",
      "|    time_elapsed    | 481       |\n",
      "|    total timesteps | 136000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 249       |\n",
      "|    critic_loss     | 16.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 135000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.95e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 140       |\n",
      "|    fps             | 262       |\n",
      "|    time_elapsed    | 534       |\n",
      "|    total timesteps | 140000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 241       |\n",
      "|    critic_loss     | 17.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 139000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.85e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 144       |\n",
      "|    fps             | 244       |\n",
      "|    time_elapsed    | 589       |\n",
      "|    total timesteps | 144000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 231       |\n",
      "|    critic_loss     | 16.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 143000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.77e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 148       |\n",
      "|    fps             | 230       |\n",
      "|    time_elapsed    | 642       |\n",
      "|    total timesteps | 148000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 224       |\n",
      "|    critic_loss     | 16.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 147000    |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -3.7e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 218      |\n",
      "|    time_elapsed    | 696      |\n",
      "|    total timesteps | 152000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 218      |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 151000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.63e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 156       |\n",
      "|    fps             | 206       |\n",
      "|    time_elapsed    | 756       |\n",
      "|    total timesteps | 156000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 212       |\n",
      "|    critic_loss     | 18.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 155000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.56e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 160       |\n",
      "|    fps             | 196       |\n",
      "|    time_elapsed    | 814       |\n",
      "|    total timesteps | 160000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 207       |\n",
      "|    critic_loss     | 19.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 159000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.44e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 164       |\n",
      "|    fps             | 188       |\n",
      "|    time_elapsed    | 870       |\n",
      "|    total timesteps | 164000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 202       |\n",
      "|    critic_loss     | 20.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 163000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 168       |\n",
      "|    fps             | 181       |\n",
      "|    time_elapsed    | 924       |\n",
      "|    total timesteps | 168000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 197       |\n",
      "|    critic_loss     | 18        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 167000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.32e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 172       |\n",
      "|    fps             | 175       |\n",
      "|    time_elapsed    | 978       |\n",
      "|    total timesteps | 172000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 195       |\n",
      "|    critic_loss     | 19.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 171000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.24e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 176       |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 1031      |\n",
      "|    total timesteps | 176000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 191       |\n",
      "|    critic_loss     | 19.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 175000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.12e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 180       |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 1093      |\n",
      "|    total timesteps | 180000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 186       |\n",
      "|    critic_loss     | 16.3      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 179000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -3.03e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 184       |\n",
      "|    fps             | 158       |\n",
      "|    time_elapsed    | 1160      |\n",
      "|    total timesteps | 184000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 182       |\n",
      "|    critic_loss     | 15.8      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 183000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -2.95e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 188       |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 1230      |\n",
      "|    total timesteps | 188000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 178       |\n",
      "|    critic_loss     | 16        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 187000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -2.86e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 192       |\n",
      "|    fps             | 148       |\n",
      "|    time_elapsed    | 1289      |\n",
      "|    total timesteps | 192000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 173       |\n",
      "|    critic_loss     | 18        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 191000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -2.79e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 196       |\n",
      "|    fps             | 144       |\n",
      "|    time_elapsed    | 1354      |\n",
      "|    total timesteps | 196000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 169       |\n",
      "|    critic_loss     | 20.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 195000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | -2.71e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 200       |\n",
      "|    fps             | 141       |\n",
      "|    time_elapsed    | 1409      |\n",
      "|    total timesteps | 200000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 166       |\n",
      "|    critic_loss     | 17.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 199000    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_vel5 = get_ddpg_trained_model('ddpg_cheetah_vel5',\n",
    "                                    'MlpPolicy',\n",
    "                                    fixed_vel_env,\n",
    "                                    timesteps=[1e5] * 2)  # 2 rounds of 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8e0df8d3aefe89a9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8e0df8d3aefe89a9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/tensorboard/ddpg_cheetah_vel5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "animate_env(model_vel5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RM cheetah env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create RMs in env wrapper\n",
    "potential_fn = ValueIteration()\n",
    "def rm_fn(task_env: MultiTaskWrapper):\n",
    "    rm = VelocityRM(task_env.task)\n",
    "    rm.reshape_rewards(potential_fn(rm, RS_GAMMA), gamma=RS_GAMMA)\n",
    "    return [rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_vel_env_rs = RMEnvWrapper(fixed_vel_env, rm_fn,\n",
    "                                rm_observations=False,\n",
    "                                change_rms_on_reset=False)  # fixed task. no need to reset RM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick demo of the internal env reward machine and reward shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'goal velocity: {fixed_vel_env_rs.task}')\n",
    "fixed_vel_env_rs.rms[0].draw()\n",
    "fixed_vel_env_rs.rms[0].delta(8, ['75%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = fixed_vel_env_rs.rms[0]\n",
    "rm.reset_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0, rm.G[0])\n",
    "print()\n",
    "print(1, rm.G[1])\n",
    "print()\n",
    "print(7, rm.G[7])\n",
    "print()\n",
    "print(8, rm.G[8])\n",
    "print()\n",
    "print(9, rm.G[9])\n",
    "print()\n",
    "print(10, rm.G[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pots = potential_fn(rm, RS_GAMMA)\n",
    "pots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.reshape_rewards(pots, RS_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0, rm.G[0])\n",
    "print()\n",
    "print(1, rm.G[1])\n",
    "print()\n",
    "print(7, rm.G[7])\n",
    "print()\n",
    "print(8, rm.G[8])\n",
    "print()\n",
    "print(9, rm.G[9])\n",
    "print()\n",
    "print(10, rm.G[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vel5_rs = get_ddpg_trained_model('ddpg_cheetah_vel5_rs',\n",
    "                                       'MlpPolicy',\n",
    "                                       fixed_vel_env_rs,\n",
    "                                       timesteps=[1e5] * 2)  # 2 rounds of 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/tensorboard/ddpg_cheetah_vel5_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_env(fixed_vel_env_rs, model_vel_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_vel_env_rs_graph_input = RMEnvWrapper(fixed_vel_env, rm_fn,\n",
    "                                            rm_observations=True,\n",
    "                                            change_rms_on_reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO support policy_kwargs in \"get_ddpg_trained_model\" function\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=RMFeatureExtractorSB\n",
    ")\n",
    "\n",
    "model_vel_rs_gnn = DDPG('MultiInputPolicy',\n",
    "                        fixed_vel_env_rs_graph_input,\n",
    "                        verbose=1,\n",
    "                        tensorboard_log=\"./ddpg_cheetah_vel5.0_rs_gnn_tensorboard/\",\n",
    "                        policy_kwargs=policy_kwargs,\n",
    "                        batch_size=1)\n",
    "model_vel_rs_gnn.learn(total_timesteps=100_000, tb_log_name=\"first_run\", reset_num_timesteps=False)\n",
    "model_vel_rs_gnn.learn(total_timesteps=100_000, tb_log_name=\"second_run\", reset_num_timesteps=False)\n",
    "model_vel_rs_gnn.learn(total_timesteps=100_000, tb_log_name=\"third_run\", reset_num_timesteps=False)\n",
    "model_vel_rs_gnn.learn(total_timesteps=100_000, tb_log_name=\"fourth_run\", reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./ddpg_cheetah_vel5.0_rs_gnn_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_env(fixed_vel_env_rs_graph_input, model_vel_rs_gnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}