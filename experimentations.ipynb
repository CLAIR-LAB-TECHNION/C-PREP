{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35b66da-eb3c-448b-8457-d63b64d633e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9200224d-b33b-46fd-a587-93ae1df8ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from rmrl.experiments.configurations import *\n",
    "from rmrl.experiments.with_transfer import WithTransferExperiment\n",
    "from rmrl.experiments.runner import ExperimentsRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73c1259-a154-4032-b9c7-aa770442928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_exp(with_geco):\n",
    "    return WithTransferExperiment(\n",
    "        cfg=ExperimentConfiguration(\n",
    "            env=SupportedEnvironments.GRID_NAVIGATION,\n",
    "            cspace=ContextSpaces.FIXED_ENTITIES,\n",
    "            alg=Algos.PPO,\n",
    "            mods=[Mods.AS, Mods.RS] + ([Mods.GECO] if with_geco else []),\n",
    "            rm_kwargs={\n",
    "                'grid_resolution': (3, 4)\n",
    "            },\n",
    "            model_kwargs={},\n",
    "            alg_kwargs={\n",
    "                # 'learning_starts': 0,\n",
    "                # 'exploration_fraction': 0.5,\n",
    "                # 'n_steps': 128\n",
    "            },\n",
    "            seed=42\n",
    "        ),\n",
    "        total_timesteps=2e5,\n",
    "        dump_dir=('no ' if not with_geco else 'with ') + 'geco',\n",
    "        verbose=1,\n",
    "        log_interval=1,\n",
    "        # eval_freq=100,\n",
    "        # min_evals=500\n",
    "        n_eval_episodes=100,\n",
    "    )\n",
    "\n",
    "no_geco_exp = get_simple_exp(with_geco=False)\n",
    "with_geco_exp = get_simple_exp(with_geco=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc89849a-f54e-431b-a46e-23c95935bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[context_pair] = ExperimentsRunner.load_or_sample_contexts(exp=no_geco_exp,\n",
    "                                                           num_samples=1,\n",
    "                                                           sample_seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a38a8-7955-4ad9-a07a-56ab3b4c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training agent for task 9d53ea5c05475f97582559063ccedcdc5b0f7293be26cfd372a0e061b3b6446b\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to no geco/WithTransferExperiment/env-grid_nav/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(3,4)))/alg_kwargs-()/model_kwargs-()/seed-42/logs/tensorboard/9d53ea5c05475f97582559063ccedcdc5b0f7293be26cfd372a0e061b3b6446b_1\n",
      "Eval num_timesteps=1000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117781125 |\n",
      "|    clip_fraction        | 0.162        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0257       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0143      |\n",
      "|    value_loss           | 0.0603       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | -200       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01267228 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.57      |\n",
      "|    explained_variance   | 0.415      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00524    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00987   |\n",
      "|    value_loss           | 0.0897     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005872807 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 196      |\n",
      "|    ep_rew_mean      | -13.5    |\n",
      "|    ep_true_rew_mean | -194     |\n",
      "| time/               |          |\n",
      "|    fps              | 18       |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 436      |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010418005 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008148165 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 0.321       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010372154 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00695    |\n",
      "|    value_loss           | 0.381       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010396652 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.168       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    value_loss           | 0.452       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 192      |\n",
      "|    ep_rew_mean      | -12.8    |\n",
      "|    ep_true_rew_mean | -187     |\n",
      "| time/               |          |\n",
      "|    fps              | 18       |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 875      |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008146573 |\n",
      "|    clip_fraction        | 0.0873      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 0.428       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | -200       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00943714 |\n",
      "|    clip_fraction        | 0.0625     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.357      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00354   |\n",
      "|    value_loss           | 0.498      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009977619 |\n",
      "|    clip_fraction        | 0.08        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    value_loss           | 0.489       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009675706 |\n",
      "|    clip_fraction        | 0.0974      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.225       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    value_loss           | 0.538       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 183      |\n",
      "|    ep_rew_mean      | -11.9    |\n",
      "|    ep_true_rew_mean | -173     |\n",
      "| time/               |          |\n",
      "|    fps              | 19       |\n",
      "|    iterations       | 12       |\n",
      "|    time_elapsed     | 1292     |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008118153 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.233       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    value_loss           | 0.538       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066590724 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.315        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000961    |\n",
      "|    value_loss           | 0.554        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012728346 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.231       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0076     |\n",
      "|    value_loss           | 0.582       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-194.02 +/- 41.86\n",
      "Episode length: 196.02 +/- 27.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 196          |\n",
      "|    mean_reward          | -194         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107551785 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.304        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00986     |\n",
      "|    value_loss           | 0.592        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 163      |\n",
      "|    ep_rew_mean      | -10.3    |\n",
      "|    ep_true_rew_mean | -142     |\n",
      "| time/               |          |\n",
      "|    fps              | 19       |\n",
      "|    iterations       | 16       |\n",
      "|    time_elapsed     | 1706     |\n",
      "|    total_timesteps  | 32768    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-194.02 +/- 41.86\n",
      "Episode length: 196.02 +/- 27.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 196         |\n",
      "|    mean_reward          | -194        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011908649 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 0.533       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 198          |\n",
      "|    mean_reward          | -197         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071353395 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.279        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.33         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    value_loss           | 0.62         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008404003 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0971      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    value_loss           | 0.574       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-191.03 +/- 51.01\n",
      "Episode length: 194.03 +/- 33.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 194         |\n",
      "|    mean_reward          | -191        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010321014 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.392       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 0.577       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 166      |\n",
      "|    ep_rew_mean      | -10.3    |\n",
      "|    ep_true_rew_mean | -147     |\n",
      "| time/               |          |\n",
      "|    fps              | 19       |\n",
      "|    iterations       | 20       |\n",
      "|    time_elapsed     | 2113     |\n",
      "|    total_timesteps  | 40960    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012622929 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.497       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 0.605       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | -197     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-194.02 +/- 41.86\n",
      "Episode length: 196.02 +/- 27.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-194.02 +/- 41.86\n",
      "Episode length: 196.02 +/- 27.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 196          |\n",
      "|    mean_reward          | -194         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053013624 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.269        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.173        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 0.57         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-194.02 +/- 41.86\n",
      "Episode length: 196.02 +/- 27.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-188.05 +/- 58.54\n",
      "Episode length: 192.05 +/- 38.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 192         |\n",
      "|    mean_reward          | -188        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011064677 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.367       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 0.572       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=47000, episode_reward=-194.03 +/- 41.79\n",
      "Episode length: 196.03 +/- 27.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-170.44 +/- 88.68\n",
      "Episode length: 180.44 +/- 58.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 180         |\n",
      "|    mean_reward          | -170        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008754894 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.171       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00574    |\n",
      "|    value_loss           | 0.543       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=-176.27 +/- 80.47\n",
      "Episode length: 184.27 +/- 53.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 184      |\n",
      "|    mean_reward     | -176     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 166      |\n",
      "|    ep_rew_mean      | -10.2    |\n",
      "|    ep_true_rew_mean | -147     |\n",
      "| time/               |          |\n",
      "|    fps              | 19       |\n",
      "|    iterations       | 24       |\n",
      "|    time_elapsed     | 2562     |\n",
      "|    total_timesteps  | 49152    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-141.05 +/- 117.90\n",
      "Episode length: 161.05 +/- 77.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 161         |\n",
      "|    mean_reward          | -141        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011040082 |\n",
      "|    clip_fraction        | 0.0962      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.208       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 0.597       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=-149.68 +/- 111.19\n",
      "Episode length: 166.68 +/- 73.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 167      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-138.03 +/- 120.20\n",
      "Episode length: 159.03 +/- 79.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 159         |\n",
      "|    mean_reward          | -138        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012345966 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.374       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    value_loss           | 0.632       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=53000, episode_reward=-117.52 +/- 132.27\n",
      "Episode length: 145.52 +/- 87.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 146      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54000, episode_reward=-197.01 +/- 29.75\n",
      "Episode length: 198.01 +/- 19.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | -197        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010662874 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 0.567       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-150.00 +/- 110.48\n",
      "Episode length: 167.00 +/- 72.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 167         |\n",
      "|    mean_reward          | -150        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004739073 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.37        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.000438   |\n",
      "|    value_loss           | 0.577       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-105.71 +/- 137.46\n",
      "Episode length: 137.71 +/- 90.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 138      |\n",
      "|    mean_reward     | -106     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 131      |\n",
      "|    ep_rew_mean      | -8.03    |\n",
      "|    ep_true_rew_mean | -91.6    |\n",
      "| time/               |          |\n",
      "|    fps              | 19       |\n",
      "|    iterations       | 28       |\n",
      "|    time_elapsed     | 2919     |\n",
      "|    total_timesteps  | 57344    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-121.21 +/- 129.56\n",
      "Episode length: 148.21 +/- 85.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 148         |\n",
      "|    mean_reward          | -121        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012263501 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.29        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00823    |\n",
      "|    value_loss           | 0.552       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-100.62 +/- 138.47\n",
      "Episode length: 134.62 +/- 91.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 135      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-59.59 +/- 146.16\n",
      "Episode length: 107.59 +/- 96.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 108          |\n",
      "|    mean_reward          | -59.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076272115 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.201        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00471     |\n",
      "|    value_loss           | 0.603        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=61000, episode_reward=-50.50 +/- 146.56\n",
      "Episode length: 101.50 +/- 96.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 102      |\n",
      "|    mean_reward     | -50.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=62000, episode_reward=-52.99 +/- 147.03\n",
      "Episode length: 102.99 +/- 97.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 103         |\n",
      "|    mean_reward          | -53         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004697324 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.35        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 0.531       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-70.81 +/- 145.76\n",
      "Episode length: 114.81 +/- 96.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 115      |\n",
      "|    mean_reward     | -70.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-32.66 +/- 145.37\n",
      "Episode length: 89.66 +/- 95.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 89.7       |\n",
      "|    mean_reward          | -32.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 64000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00675567 |\n",
      "|    clip_fraction        | 0.0657     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.629      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.245      |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00334   |\n",
      "|    value_loss           | 0.606      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=-62.33 +/- 146.21\n",
      "Episode length: 109.33 +/- 96.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 109      |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 98.7     |\n",
      "|    ep_rew_mean      | -6.05    |\n",
      "|    ep_true_rew_mean | -42.7    |\n",
      "| time/               |          |\n",
      "|    fps              | 20       |\n",
      "|    iterations       | 32       |\n",
      "|    time_elapsed     | 3185     |\n",
      "|    total_timesteps  | 65536    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-82.33 +/- 144.13\n",
      "Episode length: 122.33 +/- 95.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 122         |\n",
      "|    mean_reward          | -82.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007423483 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.317       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 0.578       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-76.90 +/- 144.67\n",
      "Episode length: 118.90 +/- 95.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 119      |\n",
      "|    mean_reward     | -76.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-65.25 +/- 146.02\n",
      "Episode length: 111.25 +/- 96.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 111          |\n",
      "|    mean_reward          | -65.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053262096 |\n",
      "|    clip_fraction        | 0.0869       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.292        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00597     |\n",
      "|    value_loss           | 0.502        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-39.31 +/- 145.37\n",
      "Episode length: 94.31 +/- 95.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 94.3     |\n",
      "|    mean_reward     | -39.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-47.91 +/- 146.15\n",
      "Episode length: 99.91 +/- 96.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 99.9        |\n",
      "|    mean_reward          | -47.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008999558 |\n",
      "|    clip_fraction        | 0.0944      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.366       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00862    |\n",
      "|    value_loss           | 0.576       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-38.78 +/- 145.84\n",
      "Episode length: 93.78 +/- 96.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 93.8     |\n",
      "|    mean_reward     | -38.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-21.93 +/- 142.41\n",
      "Episode length: 82.93 +/- 93.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 82.9         |\n",
      "|    mean_reward          | -21.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074830344 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.707        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.525        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00537     |\n",
      "|    value_loss           | 0.581        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=-30.33 +/- 144.41\n",
      "Episode length: 88.33 +/- 95.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 88.3     |\n",
      "|    mean_reward     | -30.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 66.9     |\n",
      "|    ep_rew_mean      | -4.05    |\n",
      "|    ep_true_rew_mean | 6.13     |\n",
      "| time/               |          |\n",
      "|    fps              | 21       |\n",
      "|    iterations       | 36       |\n",
      "|    time_elapsed     | 3429     |\n",
      "|    total_timesteps  | 73728    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-47.63 +/- 146.42\n",
      "Episode length: 99.63 +/- 96.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 99.6        |\n",
      "|    mean_reward          | -47.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008743351 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.269       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 0.567       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-38.75 +/- 145.88\n",
      "Episode length: 93.75 +/- 96.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 93.8     |\n",
      "|    mean_reward     | -38.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-15.39 +/- 141.50\n",
      "Episode length: 78.39 +/- 93.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 78.4        |\n",
      "|    mean_reward          | -15.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006240327 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.259       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    value_loss           | 0.511       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=-35.91 +/- 145.47\n",
      "Episode length: 91.91 +/- 95.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 91.9     |\n",
      "|    mean_reward     | -35.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-12.30 +/- 140.80\n",
      "Episode length: 76.30 +/- 92.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 76.3         |\n",
      "|    mean_reward          | -12.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060608946 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.906       |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.182        |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 0.451        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=79000, episode_reward=-41.82 +/- 146.02\n",
      "Episode length: 95.82 +/- 96.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 95.8     |\n",
      "|    mean_reward     | -41.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-4.24 +/- 137.43\n",
      "Episode length: 71.24 +/- 90.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 71.2         |\n",
      "|    mean_reward          | -4.24        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067788153 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00405     |\n",
      "|    value_loss           | 0.514        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=13.81 +/- 130.07\n",
      "Episode length: 59.19 +/- 85.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 59.2     |\n",
      "|    mean_reward     | 13.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 61.3     |\n",
      "|    ep_rew_mean      | -3.64    |\n",
      "|    ep_true_rew_mean | 13.7     |\n",
      "| time/               |          |\n",
      "|    fps              | 22       |\n",
      "|    iterations       | 40       |\n",
      "|    time_elapsed     | 3644     |\n",
      "|    total_timesteps  | 81920    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-35.97 +/- 145.42\n",
      "Episode length: 91.97 +/- 95.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 92           |\n",
      "|    mean_reward          | -36          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 82000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090149455 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.892       |\n",
      "|    explained_variance   | 0.599        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.184        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    value_loss           | 0.5          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-27.27 +/- 144.01\n",
      "Episode length: 86.27 +/- 94.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 86.3     |\n",
      "|    mean_reward     | -27.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-9.68 +/- 139.69\n",
      "Episode length: 74.68 +/- 92.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 74.7         |\n",
      "|    mean_reward          | -9.68        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077226553 |\n",
      "|    clip_fraction        | 0.0794       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.819       |\n",
      "|    explained_variance   | 0.617        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.208        |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 0.513        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-36.04 +/- 145.36\n",
      "Episode length: 92.04 +/- 95.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 92       |\n",
      "|    mean_reward     | -36      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-18.66 +/- 141.99\n",
      "Episode length: 80.66 +/- 93.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 80.7     |\n",
      "|    mean_reward     | -18.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-7.19 +/- 138.44\n",
      "Episode length: 73.19 +/- 91.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 73.2         |\n",
      "|    mean_reward          | -7.19        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 87000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069415495 |\n",
      "|    clip_fraction        | 0.0917       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.16         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00616     |\n",
      "|    value_loss           | 0.493        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-21.43 +/- 142.81\n",
      "Episode length: 82.43 +/- 94.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 82.4     |\n",
      "|    mean_reward     | -21.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-4.46 +/- 137.28\n",
      "Episode length: 71.46 +/- 90.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 71.5         |\n",
      "|    mean_reward          | -4.46        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 89000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055536507 |\n",
      "|    clip_fraction        | 0.0633       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.746       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.29         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 0.476        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=8.08 +/- 133.03\n",
      "Episode length: 62.92 +/- 87.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 62.9     |\n",
      "|    mean_reward     | 8.08     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 48.2     |\n",
      "|    ep_rew_mean      | -2.82    |\n",
      "|    ep_true_rew_mean | 33.8     |\n",
      "| time/               |          |\n",
      "|    fps              | 23       |\n",
      "|    iterations       | 44       |\n",
      "|    time_elapsed     | 3864     |\n",
      "|    total_timesteps  | 90112    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-24.89 +/- 143.02\n",
      "Episode length: 84.89 +/- 94.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 84.9         |\n",
      "|    mean_reward          | -24.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 91000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070150592 |\n",
      "|    clip_fraction        | 0.0791       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.788       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.283        |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    value_loss           | 0.543        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=1.81 +/- 135.32\n",
      "Episode length: 67.19 +/- 89.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 67.2     |\n",
      "|    mean_reward     | 1.81     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "training agent for task 8b334c0c3b6aa1fca5abe5e0273f27444aa7f867c041d844dbcc0a8fd718be63\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to no geco/WithTransferExperiment/env-grid_nav/cspace-fixed_entities/alg-PPO/mods-(AS,RS)/rm_kwargs-((grid_resolution-(3,4)))/alg_kwargs-()/model_kwargs-()/seed-42/logs/tensorboard/8b334c0c3b6aa1fca5abe5e0273f27444aa7f867c041d844dbcc0a8fd718be63_1\n",
      "Eval num_timesteps=1000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | -200       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01332796 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | -0.114     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0221     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 0.0662     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013416102 |\n",
      "|    clip_fraction        | 0.0811      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008826109 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0682      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00433    |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 181      |\n",
      "|    ep_rew_mean      | -14.5    |\n",
      "|    ep_true_rew_mean | -163     |\n",
      "| time/               |          |\n",
      "|    fps              | 18       |\n",
      "|    iterations       | 4        |\n",
      "|    time_elapsed     | 436      |\n",
      "|    total_timesteps  | 8192     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | -200        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264369 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.0537      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.189       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 0.361       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | -200       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00606724 |\n",
      "|    clip_fraction        | 0.0672     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.00556    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.244      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.00619   |\n",
      "|    value_loss           | 0.609      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-167.39 +/- 92.76\n",
      "Episode length: 178.39 +/- 61.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 178         |\n",
      "|    mean_reward          | -167        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011437893 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.0602      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.349       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00676    |\n",
      "|    value_loss           | 0.72        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-185.23 +/- 64.38\n",
      "Episode length: 190.23 +/- 42.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -185     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | -200         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076820944 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.0215      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.504        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00644     |\n",
      "|    value_loss           | 0.704        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -200     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 165      |\n",
      "|    ep_rew_mean      | -13.1    |\n",
      "|    ep_true_rew_mean | -142     |\n",
      "| time/               |          |\n",
      "|    fps              | 18       |\n",
      "|    iterations       | 8        |\n",
      "|    time_elapsed     | 878      |\n",
      "|    total_timesteps  | 16384    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-135.00 +/- 122.39\n",
      "Episode length: 157.00 +/- 80.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 157          |\n",
      "|    mean_reward          | -135         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073180487 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0504       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.33         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    value_loss           | 0.821        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-137.80 +/- 120.64\n",
      "Episode length: 158.80 +/- 79.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 159      |\n",
      "|    mean_reward     | -138     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-134.78 +/- 122.81\n",
      "Episode length: 156.78 +/- 81.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 157         |\n",
      "|    mean_reward          | -135        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005305891 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.0771      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.496       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 0.867       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-146.91 +/- 113.32\n",
      "Episode length: 164.91 +/- 74.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 165      |\n",
      "|    mean_reward     | -147     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-182.18 +/- 70.53\n",
      "Episode length: 188.18 +/- 46.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 188          |\n",
      "|    mean_reward          | -182         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062776757 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.067        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.542        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    value_loss           | 0.979        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "no_geco_exp.run(*context_pair)\n",
    "\n",
    "end = time.time()\n",
    "print(f'execution time {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00977ee-a1dd-46e2-b9fc-1ed751f7a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "with_geco_exp.run(*context_pair)\n",
    "\n",
    "end = time.time()\n",
    "print(f'execution time {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0da3e-a0f2-4c88-a385-1bb86dc15be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e0326-90d1-4da0-ac38-48a005ec0725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
